{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ZScoreTransformer\n",
    "  attr_reader :means, :stdevs\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new\n",
    "    @stdevs = Hash.new\n",
    "    @feature_names = feature_names    \n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "       \n",
    "    examples = dataset[\"data\"] \n",
    "\n",
    "    j = 0\n",
    "    data = Array.new(@feature_names.length) { Array.new(examples.length) }\n",
    "    while j < @feature_names.length do\n",
    "          i = 0\n",
    "          while i < examples.length do\n",
    "            data[j][i]=(examples[i][\"features\"][@feature_names[j]])\n",
    "\n",
    "            i +=1\n",
    "          end\n",
    "          \n",
    "          m = mean data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "          s = stdev data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "\n",
    "          @means[@feature_names[j]] = m\n",
    "          @stdevs[@feature_names[j]] = s\n",
    "\n",
    "          j +=1\n",
    "    end\n",
    "    return @means,@stdevs\n",
    "    # END YOUR CODE\n",
    "  end\n",
    "  \n",
    "    def apply example_batch\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    i = 0\n",
    "    feature =  (self.means).keys\n",
    "\n",
    "\n",
    "    while i< feature.length do    \n",
    "\n",
    "        if (self.means[feature[i]]).is_a? Numeric and   (self.stdevs[feature[i]]).is_a?Numeric and    (self.stdevs[feature[i]]) !=0\n",
    "          j = 0\n",
    "          while j<example_batch.length do\n",
    "              if example_batch[j][\"features\"][feature[i]].is_a?Numeric\n",
    "                example_batch[j][\"features\"][feature[i]] = (example_batch[j][\"features\"][feature[i]] - self.means[feature[i]])/self.stdevs[feature[i]]\n",
    "              end\n",
    "                j+=1\n",
    "          end\n",
    "        end\n",
    "        i +=1\n",
    "    end\n",
    "\n",
    "\n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MeanImputation\n",
    "  attr_reader :means\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new\n",
    "    @feature_names = feature_names\n",
    "  end\n",
    "  \n",
    "  def train dataset    \n",
    "    # BEGIN YOUR CODE\n",
    "    examples = dataset[\"data\"] \n",
    "\n",
    "    j = 0\n",
    "    data = Array.new(@feature_names.length) { Array.new(examples.length) }\n",
    "    while j < @feature_names.length do\n",
    "          i = 0\n",
    "          while i < examples.length do\n",
    "            data[j][i]=(examples[i][\"features\"][@feature_names[j]])\n",
    "\n",
    "            i +=1\n",
    "          end\n",
    "          \n",
    "          m = mean data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "\n",
    "\n",
    "          @means[@feature_names[j]] = m\n",
    "\n",
    "\n",
    "          j +=1\n",
    "          \n",
    "    \n",
    "    end\n",
    "\n",
    "    return @means\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    example = Hash.new()\n",
    "    \n",
    "    while i< example_batch.length do    \n",
    "        \n",
    "        j = 0\n",
    "        name  = example_batch[i][\"features\"].keys\n",
    "        while j < @feature_names.length do   \n",
    "\n",
    "          if name.include?@feature_names[j]\n",
    "             if example_batch[i][\"features\"][@feature_names[j]].is_a?Numeric\n",
    "               example_batch[i][\"features\"][@feature_names[j]] = example_batch[i][\"features\"][@feature_names[j]]\n",
    "             else\n",
    "               example_batch[i][\"features\"][@feature_names[j]] = self.means[@feature_names[j]]\n",
    "             end\n",
    "          else \n",
    "             example_batch[i][\"features\"].store(@feature_names[j],self.means[@feature_names[j]])\n",
    "\n",
    "          end\n",
    "        j+=1\n",
    "        end\n",
    "\n",
    "        i +=1\n",
    "    end\n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AgeRangeAsVector\n",
    "  def initialize; end\n",
    "  def train dataset; end\n",
    "  def apply(example_batch)\n",
    "    min_age = 0\n",
    "    max_age = 100\n",
    "    feature_name = \"days_birth\"\n",
    "    pattern = \"age_range_%d\"\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "     \n",
    "    while i< example_batch.length do        \n",
    "          if example_batch[i][\"features\"].keys.include?feature_name\n",
    "                bin = -example_batch[i][\"features\"][feature_name]/(365*5)  *5\n",
    "\n",
    "                example_batch[i][\"features\"].delete(feature_name)\n",
    "\n",
    "                binned_age = min_age\n",
    "\n",
    "                while binned_age < max_age+1 do\n",
    "\n",
    "                      new_feature_name = pattern% binned_age\n",
    "\n",
    "                      if bin > binned_age or bin < (binned_age-5)    \n",
    "                        if binned_age == max_age\n",
    "                          example_batch[i][\"features\"].store(new_feature_name,1)\n",
    "                        elsif binned_age == max_age\n",
    "                          example_batch[i][\"features\"].store(new_feature_name,1)\n",
    "                        else\n",
    "                        example_batch[i][\"features\"].store(new_feature_name,nil)\n",
    "                        end\n",
    "                      else \n",
    "                        example_batch[i][\"features\"].store(new_feature_name,1)\n",
    "                      end\n",
    "\n",
    "                      binned_age+=5\n",
    "                end\n",
    "          else\n",
    "          end\n",
    "    i+=1\n",
    "    end \n",
    "\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TargetAveraging\n",
    "  attr_reader :means\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new {|h,k| h[k] = Hash.new}\n",
    "    @feature_names = feature_names\n",
    "    @pattern = \"avg_%s\"\n",
    "  end\n",
    "  \n",
    "  def train dataset    \n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    while i< @feature_names.length do\n",
    "\n",
    "          a = dataset.clone()[\"data\"].select{|e|  !e[\"features\"][@feature_names[i]].nil?}\n",
    "          if a.empty?\n",
    "          else\n",
    "                b = a.uniq! {|e|  e[\"features\"][@feature_names[i]] }\n",
    "                name = b.map { |e|  e[\"features\"][@feature_names[i]]  }           ######find unique value\n",
    "\n",
    "                num =Array.new(name.length,0)\n",
    "                lab_num = Array.new(name.length,0)\n",
    "                j = 0\n",
    "                while j< name.length do\n",
    "                      a = dataset.clone()[\"data\"].select{|e|  !e[\"features\"][@feature_names[i]].nil?}\n",
    "                      k = 0\n",
    "                      while k< dataset[\"data\"].length do\n",
    "                            if dataset[\"data\"][k][\"features\"][@feature_names[i]] == name[j]\n",
    "                                  num[j]+=1\n",
    "                                  lab_num[j] += dataset[\"data\"][k][\"label\"]\n",
    "                            end\n",
    "                            k+=1\n",
    "                      end\n",
    "\n",
    "                      @means[@feature_names[i]].store(name[j],lab_num[j].to_f/num[j])\n",
    "                      j+=1\n",
    "\n",
    "                end\n",
    "          end\n",
    "          i+=1\n",
    "    end\n",
    "  \n",
    "    return @means\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "    def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "    @pattern = \"avg_%s\"\n",
    "    j = 0\n",
    "    mean = self.means\n",
    "    fea = mean.keys\n",
    "\n",
    "    i = 0\n",
    "    while i< fea.length do\n",
    "      name = mean[fea[i]].keys\n",
    "      j= 0\n",
    "      \n",
    "      while j<name.length do\n",
    "        \n",
    "        k = 0        \n",
    "        while k < example_batch.length do\n",
    "\n",
    "          if !(example_batch[k][\"features\"][fea[i]]).nil?\n",
    "            value = example_batch[k][\"features\"][fea[i]]\n",
    "            example_batch[k][\"features\"].delete(fea[i])\n",
    "\n",
    "            example_batch[k][\"features\"].store(@pattern%fea[i],mean[fea[i]][value])\n",
    "          end\n",
    "          k+=1\n",
    "        end\n",
    "        \n",
    "        j+=1\n",
    "      end\n",
    "      i+=1\n",
    "    end\n",
    "\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OneHotEncoding\n",
    "  def initialize feature_names\n",
    "    @feature_names = feature_names\n",
    "    @pattern = \"%s=%s\"\n",
    "  end\n",
    "  \n",
    "  def train dataset; end\n",
    "  \n",
    "  def apply(example_batch)\n",
    "    \n",
    "    # BEGIN YOUR CODE\n",
    "    \n",
    "    j = 0\n",
    "    while j<example_batch.length do\n",
    "      \n",
    "          name = example_batch[j][\"features\"].keys\n",
    "          i = 0\n",
    "          while i< @feature_names.length do\n",
    "              if name.include?@feature_names[i] and !(example_batch[j][\"features\"][@feature_names[i]]).nil?\n",
    "\n",
    "                name_new = @pattern % [@feature_names[i], example_batch[j][\"features\"][@feature_names[i]]]\n",
    "                example_batch[j][\"features\"].delete(@feature_names[i])\n",
    "                example_batch[j][\"features\"].store(name_new,1.0)\n",
    "              end\n",
    "              i+=1\n",
    "          end\n",
    "          j+=1\n",
    "    end\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    \n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogTransform\n",
    "  def initialize feature_names\n",
    "    @feature_names = feature_names\n",
    "    @pattern = \"log_%s\"\n",
    "  end\n",
    "  \n",
    "  def train dataset; end\n",
    "  \n",
    "  def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    while i < @feature_names.length do\n",
    "      j = 0\n",
    "     \n",
    "      while j<example_batch.length do\n",
    "   \n",
    "        if (example_batch[j][\"features\"].keys).include? @feature_names[i] and  example_batch[j][\"features\"][@feature_names[i]] > 0\n",
    "                data = example_batch[j][\"features\"][@feature_names[i]]\n",
    "                example_batch[j][\"features\"].delete(@feature_names[i])\n",
    "                example_batch[j][\"features\"].store(@pattern % @feature_names[i], Math.log(data))\n",
    "        end\n",
    "        j+=1\n",
    "      end\n",
    "      \n",
    "      \n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class L2Normalize\n",
    "  def train dataset; end\n",
    "  def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "      j = 0\n",
    "      while j < example_batch.length do\n",
    "\n",
    "          name = example_batch[j][\"features\"].keys\n",
    "          i = 0\n",
    "          normal_factor = norm example_batch[j][\"features\"]\n",
    "\n",
    "          while i<name.length do\n",
    "            if example_batch[j][\"features\"][name[i]].is_a? Numeric and normal_factor!=0\n",
    "                example_batch[j][\"features\"][name[i]] = example_batch[j][\"features\"][name[i]]/normal_factor\n",
    "            else\n",
    "                example_batch[j][\"features\"][name[i]] = example_batch[j][\"features\"][name[i]]\n",
    "            end\n",
    "            i+=1\n",
    "          end\n",
    "\n",
    "\n",
    "          j+=1\n",
    "      end    \n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureTransformPipeline\n",
    "  def initialize *transformers\n",
    "    @transformers = transformers\n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "    # BEGIN YOUR CODE \n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i< @transformers.length do\n",
    "        @transformers[i].train dataset \n",
    "\n",
    "        j = 0\n",
    "        while j<dataset[\"data\"].length do\n",
    "  \n",
    "          c = dataset[\"data\"][j]\n",
    "          dataset[\"data\"][i] = (@transformers[i].apply  [c])[0]\n",
    "\n",
    "          j+=1\n",
    "        end\n",
    "      i+=1\n",
    "    end\n",
    "       \n",
    "    return dataset\n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def apply example_batch \n",
    "    return @transformers.inject(example_batch) do |u, transform|\n",
    "      u = transform.apply example_batch\n",
    "\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2394cfcfad4ddbc5545c064a75d42b23",
     "grade": false,
     "grade_id": "cell-4eb3580dec510638",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Final Project Tests\n",
    "\n",
    "In [Part 3](./part-3.ipynb) of the final project, you will create your own features and try different combinations of models. This notebook sets up the foundation for the final project. Most of the code you need for the final project has already been implemented by you across multiple assignments. It has been scattered and we have copied and pasted where necessary. \n",
    "\n",
    "In this assignment, you will assemble a small library of the ```Learner```s, ```FeatureTransformer```s and ```Metric```s needed for the final project. The tests here do not cover everything we did in the course or everything you need for the final project, but it is a good start. Use this notebook to test your implementations. Once you are done, you can copy the appropriate code into the part-3 notebooks.\n",
    "\n",
    "### Validation\n",
    "If your Validate button shows an Interrupt error, that means your code takes longer than 1 minute to execute. This can happen. If you really want to do validation, use the command line following these steps. \n",
    "\n",
    "1. Go to the folder view and open a terminal.\n",
    "1. Start a ```bash``` shell.\n",
    "1. Run ```nbgrader validate final-project-3.1/tests-3.1.ipynb``` or any other path to a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09171d07ff9cb8306a7e2ae385423a03",
     "grade": false,
     "grade_id": "cell-fe2566e435b8f92b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       ":load_spambase_dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './final_project_lib.rb'\n",
    "require './metrics.rb'\n",
    "def load_german_credit_dataset; JSON.parse(File.read('german-credit.json')); end\n",
    "def load_spambase_dataset; JSON.parse(File.read('spambase.json')); end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b65ab5311e15fcbbf5fe9ade4309bc5b",
     "grade": false,
     "grade_id": "cell-bf4956f79c741a39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Metrics\n",
    "\n",
    "Copy / refactor **your** implementations of the following methods in the cell below located in this directory. Some are already in the file ```metrics.rb``` for you. Note that you should copy the entire method / class to the cell below.\n",
    "\n",
    "* class AUCMetric \n",
    "* cross_validate\n",
    "* mean\n",
    "* stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":cross_validate"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_validate dataset, folds, &block\n",
    "  examples = dataset[\"data\"]\n",
    "  # BEGIN YOUR CODE\n",
    "  batch = examples.length/folds\n",
    "  folds.times do |fold|\n",
    "\n",
    "\n",
    "      test_data = dataset.clone\n",
    "      test_data[\"data\"] = test_data[\"data\"][fold*batch,batch]  ##CV training examples\n",
    "\n",
    "      train_data = dataset.clone\n",
    "      train_data[\"data\"] = train_data[\"data\"] - test_data[\"data\"]  ##CV testing examples\n",
    "\n",
    "    ## Call the callback like this:\n",
    "    yield train_data, test_data, fold\n",
    "  end\n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":score_binary_classification_model"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_binary_classification_model(data, weights, model)\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  scores =Array.new(data.length) { Array.new(2)}\n",
    "  i = 0\n",
    "  while i< data.length do\n",
    "    scores[i][0] = model.predict(data[i],weights)\n",
    "    scores[i][1] = data[i][\"label\"]\n",
    "    i+=1\n",
    "  end\n",
    "  \n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return scores\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44a55f0cb817c05d6a06fcd75320ac4d",
     "grade": false,
     "grade_id": "cell-49d4fccc0ebf4574",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":evaluate"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "\n",
    "class TransformingLearner\n",
    "  include Learner\n",
    "  attr_accessor :name\n",
    "  def initialize transformer, learner\n",
    "    @parameters = learner.parameters\n",
    "    @parameters[\"learner\"] = learner.parameters[\"name\"] || learner.class.name\n",
    "    @transformer = transformer\n",
    "    @learner = learner\n",
    "    @name = self.class.name\n",
    "  end\n",
    "  def train dataset\n",
    "    @transformer.train dataset\n",
    "    transformed_examples = @transformer.apply dataset[\"data\"]\n",
    "    train_dataset = dataset.clone\n",
    "    train_dataset[\"data\"] = transformed_examples\n",
    "    @learner.train train_dataset\n",
    "  end\n",
    "  \n",
    "  def predict example\n",
    "    transformed_example = @transformer.apply [example]\n",
    "    @learner.predict transformed_example.first\n",
    "  end\n",
    "  \n",
    "  def evaluate dataset\n",
    "    transformed_dataset = dataset.clone\n",
    "    transformed_dataset[\"data\"] = @transformer.apply dataset[\"data\"]\n",
    "    @learner.evaluate transformed_dataset\n",
    "  end\n",
    "end\n",
    "\n",
    "class CopyingTransformingLearner\n",
    "  include Learner\n",
    "  attr_accessor :name\n",
    "  def initialize transformer, learner\n",
    "    @parameters = learner.parameters\n",
    "    @parameters[\"learner\"] = learner.parameters[\"name\"] || learner.class.name\n",
    "    @transformer = transformer\n",
    "    @learner = learner\n",
    "    @name = self.class.name\n",
    "  end\n",
    "\n",
    "  def clone_example example\n",
    "    e = example.clone\n",
    "    e[\"features\"] = example[\"features\"].clone\n",
    "    return e\n",
    "  end\n",
    "    \n",
    "  def clone_dataset dataset\n",
    "    cloned_dataset = dataset.clone\n",
    "    cloned_dataset[\"features\"] = dataset[\"features\"].clone\n",
    "    cloned_dataset[\"data\"] = dataset[\"data\"].map {|e| clone_example(e)}\n",
    "    return cloned_dataset\n",
    "  end\n",
    "    \n",
    "  def train dataset\n",
    "    @transformer.train clone_dataset(dataset)\n",
    "      \n",
    "    train_dataset = clone_dataset(dataset)\n",
    "    transformed_examples = @transformer.apply train_dataset[\"data\"]\n",
    "    train_dataset[\"data\"] = transformed_examples\n",
    "    @learner.train train_dataset\n",
    "  end\n",
    "  \n",
    "  def predict example\n",
    "    transformed_example = @transformer.apply [clone_example(example)]\n",
    "    @learner.predict transformed_example.first\n",
    "  end\n",
    "  \n",
    "  def evaluate dataset\n",
    "    transformed_dataset = clone_dataset dataset\n",
    "    transformed_dataset[\"data\"] = @transformer.apply dataset[\"data\"]\n",
    "    @learner.evaluate transformed_dataset\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#END YOUR CODe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":mean"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean x\n",
    "  # BEGIN YOUR CODE\n",
    "  x = x.compact\n",
    "  x.inject(0.0){|sum,i| sum + i }/x.length()  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":stdev"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stdev x\n",
    "  # BEGIN YOUR CODE\n",
    "  x = x.compact\n",
    "  mu = mean x\n",
    "  su = x.inject(0.0){|sum,i| sum + (i-mu)**2 }/(x.length()-1)\n",
    "  Math.sqrt(su) \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":dot"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implement the error function given a weight vector, w\n",
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "    x_name = x.keys\n",
    "    w_name = w.keys\n",
    "\n",
    "    i = 0\n",
    "    dot = 0.0\n",
    "    if x_name.length == 0 \n",
    "      dot =  0.0\n",
    "    else \n",
    "        while i<x_name.length do\n",
    "\n",
    "          if w_name.include?x_name[i]\n",
    "            index = w_name.index { |x| [x_name[i]].include?(x) }\n",
    "            if x[x_name[i]].is_a? Numeric  and w[w_name[index]].is_a? Numeric\n",
    "              dot += x[x_name[i]]*w[w_name[index]]\n",
    "            else \n",
    "              dot +=0.0\n",
    "            end\n",
    "          else \n",
    "            dot += 0.0\n",
    "          end\n",
    "          i+=1\n",
    "        end  \n",
    "    end\n",
    "  return dot\n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":norm"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return Math.sqrt(dot w, w)\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":confusion_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(scores, t)\n",
    "  matrix = Hash.new {|h,predicted_class| h[predicted_class] = Hash.new {|h,true_class| h[true_class] = 0.0}}\n",
    "  # BEGIN YOUR CODE\n",
    "  matrix[\"N\"][\"N\"] = 0\n",
    "  matrix[\"N\"][\"P\"] = 0\n",
    "  matrix[\"P\"][\"N\"] = 0\n",
    "  matrix[\"P\"][\"P\"] = 0\n",
    "  i = 0\n",
    "  while i<scores.length do\n",
    "    if scores[i][1]>0 and scores[i][0]>=t\n",
    "      matrix[\"P\"][\"P\"] += 1\n",
    "    elsif scores[i][1]>0 and scores[i][0]<t\n",
    "      matrix[\"N\"][\"P\"] += 1\n",
    "    elsif scores[i][1]<=0 and scores[i][0]>=t\n",
    "      matrix[\"P\"][\"N\"] += 1\n",
    "    elsif scores[i][1]<=0 and scores[i][0]<t\n",
    "      matrix[\"N\"][\"N\"] += 1\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return matrix\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ZScoreTransformer\n",
    "  attr_reader :means, :stdevs\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new\n",
    "    @stdevs = Hash.new\n",
    "    @feature_names = feature_names    \n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "       \n",
    "    examples = dataset[\"data\"] \n",
    "\n",
    "    j = 0\n",
    "    data = Array.new(@feature_names.length) { Array.new(examples.length) }\n",
    "    while j < @feature_names.length do\n",
    "          i = 0\n",
    "          while i < examples.length do\n",
    "            data[j][i]=(examples[i][\"features\"][@feature_names[j]])\n",
    "\n",
    "            i +=1\n",
    "          end\n",
    "          \n",
    "          m = mean data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "          s = stdev data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "\n",
    "          @means[@feature_names[j]] = m\n",
    "          @stdevs[@feature_names[j]] = s\n",
    "\n",
    "          j +=1\n",
    "    end\n",
    "    return @means,@stdevs\n",
    "    # END YOUR CODE\n",
    "  end\n",
    "  \n",
    "    def apply example_batch\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    i = 0\n",
    "    feature =  (self.means).keys\n",
    "\n",
    " \n",
    "    while i< feature.length do    \n",
    "\n",
    "        if (self.means[feature[i]]).is_a? Numeric and   (self.stdevs[feature[i]]).is_a?Numeric and    (self.stdevs[feature[i]]) !=0\n",
    "          j = 0\n",
    "          while j<example_batch.length do\n",
    "              if example_batch[j][\"features\"][feature[i]].is_a?Numeric\n",
    "                example_batch[j][\"features\"][feature[i]] = (example_batch[j][\"features\"][feature[i]] - self.means[feature[i]])/self.stdevs[feature[i]]\n",
    "              end\n",
    "                j+=1\n",
    "          end\n",
    "        end\n",
    "        i +=1\n",
    "    end\n",
    "          \n",
    "\n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb4442ddbe8a7242d4cf24bf790ccf9c",
     "grade": false,
     "grade_id": "cell-706d8ecd21388de7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Models\n",
    "Convert your regularized logistic regression implementation into a ```LogisticRegressionLearner```, stored in ```\"linear_models.rb\"```. You should reuse the ```StochasticGradientDescent``` and ```LogisticRegressionModelL2``` classes from previous assignments. \n",
    "\n",
    "The ```Learner``` interface (implemented as a ruby mixin) exposes common methods that all classifiers will use. The ```Learner``` is defined as follows:\n",
    "```\n",
    "module Learner  \n",
    "  attr_reader :parameters\n",
    "  def name\n",
    "      self.class.name\n",
    "  end\n",
    "  def train train_dataset    \n",
    "  end\n",
    "  def predict example\n",
    "  end\n",
    "  def evaluate eval_dataset\n",
    "  end\n",
    "end\n",
    "```\n",
    "where ```parameters``` contain, say the weights or other hyperparameters.\n",
    "\n",
    "Implement the ```LogisticRegressionLearner``` to like this:\n",
    "\n",
    "```ruby\n",
    "class LogisticRegressionLearner\n",
    "  attr_reader :parameters\n",
    "  attr_reader :weights  \n",
    "  include Learner  \n",
    "  \n",
    "  def initialize regularization: 0.0, learning_rate: 0.01, batch_size: 20, epochs: 1\n",
    "    @parameters = {\"regularization\" => regularization, \n",
    "      \"learning_rate\" => learning_rate, \n",
    "      \"epochs\" => epochs, \"batch_size\" => batch_size}\n",
    "  end\n",
    "    \n",
    "  def train dataset\n",
    "      ###\n",
    "  end\n",
    "    \n",
    "  def predict example\n",
    "      ###\n",
    "  end\n",
    "    \n",
    "  def evaluate dataset\n",
    "      ###\n",
    "  end\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be5956a73feab76f604c14331a44198b",
     "grade": false,
     "grade_id": "cell-d7de1e840762a48b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "class StochasticGradientDescent\n",
    "  attr_reader :weights\n",
    "  attr_reader :objective\n",
    "  def initialize obj, w_0, lr \n",
    "    @objective = obj\n",
    "    @weights = w_0\n",
    "    @n = 1.0\n",
    "    @lr = lr\n",
    "  end\n",
    "  def update x\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    lr  = @lr.to_f / Math.sqrt(@n)\n",
    "    @n +=1\n",
    "\n",
    "    if x.empty?\n",
    "    else\n",
    "      feature = x[0][\"features\"].keys\n",
    "    \n",
    "    end\n",
    "  \n",
    "    if (@weights.keys)==[0] and  x.length>0\n",
    "      i=0\n",
    "      while i<feature.length do\n",
    "        @weights.store(feature[i],@weights[0])\n",
    "        i+=1\n",
    "      end\n",
    "      \n",
    "    end\n",
    "    \n",
    "    @weights.delete(0)\n",
    "    \n",
    "    i = 0\n",
    "    gradient = @objective.grad x,@weights\n",
    "\n",
    "    while i < @weights.length do\n",
    "      if @weights[@weights.keys[i]].is_a?Numeric and gradient[@weights.keys[i]].is_a?Numeric\n",
    "        @weights[@weights.keys[i]]  = @weights[@weights.keys[i]] - lr*gradient[@weights.keys[i]]        \n",
    "      end\n",
    "      i+=1\n",
    "    end    \n",
    "\n",
    "   return @objective.adjust @weights\n",
    "\n",
    "  \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegressionModelL2\n",
    "  def initialize reg_param\n",
    "    @reg_param = reg_param\n",
    "  end\n",
    "\n",
    "  def predict row, w\n",
    "    x = row[\"features\"]    \n",
    "    1.0 / (1 + Math.exp(-dot(w, x)))\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].nan? or w[k].infinite?}\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].abs > 1e5 }\n",
    "  end\n",
    "  \n",
    "  def func data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    adjust w\n",
    "    i = 0\n",
    "    sum  = 0.0\n",
    "\n",
    "    while i< data.length do\n",
    "      y=(data[i][\"label\"])\n",
    "      yhat=dot(w, data[i][\"features\"])\n",
    "      sum += Math.log(Math.exp(-y*yhat)+1.0)\n",
    "      i+=1\n",
    "    end\n",
    "\n",
    "    \n",
    "    return sum/data.length + 0.5 * @reg_param *(norm(w)**2)\n",
    "  end \n",
    "   def grad data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    \n",
    "    g = Hash.new()\n",
    "    key_list =[]\n",
    "    data.each do |e|\n",
    "        key_list.push((e[\"features\"].keys).compact)\n",
    "    end\n",
    "    name = key_list.flatten.compact.uniq{|x| x  }\n",
    "\n",
    "    sum  = Array.new(name.length)\n",
    "    \n",
    "\n",
    "    adjust w\n",
    "    \n",
    "    if (w.keys).empty?\n",
    "      i = 0\n",
    "      while i<name.length do\n",
    "        w.store(name[i],w[0])\n",
    "        i+=1\n",
    "      end\n",
    "      w.delete(0)\n",
    "    end\n",
    "    \n",
    "    norm  = norm w\n",
    "\n",
    "    i = 0\n",
    "    while i< w.length do\n",
    "      \n",
    "      sum = 0.0\n",
    "      j = 0\n",
    "      while j<data.length\n",
    "        yhat =dot  data[j][\"features\"], w\n",
    "        ylabel = data[j][\"label\"]\n",
    "        if data[j][\"features\"][name[i]].is_a? Numeric\n",
    "          sum += (1.0/(1.0+Math.exp(ylabel*yhat)))*(-ylabel *data[j][\"features\"][name[i]] )\n",
    "        end\n",
    "        j+=1\n",
    "      end\n",
    "      sum = (sum/data.length) + @reg_param * w[name[i]]\n",
    "      g.store(name[i],sum)\n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return g\n",
    "  end\n",
    "  def adjust w\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].nan? or w[k].infinite?}\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].abs > 1e5 }\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":evaluate"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "class LogisticRegressionLearner\n",
    "  attr_reader :parameters\n",
    "  attr_reader :weights  \n",
    "  include Learner  \n",
    "\n",
    "  def initialize regularization: 0.0, learning_rate: 0.01, batch_size: 20, epochs: 1\n",
    "    @parameters = {\"regularization\" => regularization, \n",
    "      \"learning_rate\" => learning_rate, \n",
    "      \"epochs\" => epochs, \"batch_size\" => batch_size}\n",
    "  end\n",
    "\n",
    "  def train dataset\n",
    "\n",
    "\n",
    "    reg = @parameters[\"regularization\"]\n",
    "    batch_size = @parameters[\"batch_size\"]\n",
    "    num_epoch = @parameters[\"epochs\"]\n",
    "    lr = @parameters[\"learning_rate\"]\n",
    "\n",
    "    w = @weights\n",
    "\n",
    "\n",
    "    if w.nil?\n",
    "      w = Hash.new {|h,k| h[k] =0.0}\n",
    "    end\n",
    "\n",
    "   losses = []\n",
    "\n",
    "   num_epoch.times do\n",
    "\n",
    "        obj = LogisticRegressionModelL2.new(reg)\n",
    "        l = obj.func(dataset[\"data\"][0,batch_size],w)\n",
    "  \n",
    "        losses.append(l)\n",
    "        sgd = StochasticGradientDescent.new(obj, w, lr)\n",
    "        sgd.update(dataset[\"data\"][0,batch_size])\n",
    "\n",
    "        w = sgd.weights\n",
    "\n",
    "  \n",
    "  end\n",
    "  @weights = w\n",
    "  \n",
    "  return w\n",
    "   \n",
    "  end\n",
    "\n",
    "\n",
    "  def predict example \n",
    "    \n",
    "    x = example[\"features\"]    \n",
    "   \n",
    "   return dot(@weights, x)\n",
    "  end\n",
    "\n",
    "\n",
    "  def evaluate dataset\n",
    "   \n",
    "    examples = dataset[\"data\"]\n",
    "    \n",
    "    i = 0\n",
    "    score = Array.new(examples.length) { Array.new(2) }\n",
    "    while i<examples.length do      \n",
    "      x = examples[i]   \n",
    "      y = self.predict(x)   \n",
    "      score[i][0] = y\n",
    "      score[i][1] = x[\"label\"]\n",
    "      i+=1\n",
    "    end\n",
    "\n",
    "  return score\n",
    "  end\n",
    "\n",
    "\n",
    "end\n",
    "#END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":num_positives"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_positives scores\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  i = 0\n",
    "  sum = 0\n",
    "  while i< scores.length do\n",
    "    if scores[i][1]>0\n",
    "      sum += 1\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "  return sum\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":num_negatives"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_negatives scores\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  return scores.length - num_positives(scores)\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class AUCMetric \n",
    "  include Metric  \n",
    "\n",
    "  def roc_curve(scores)\n",
    "  fp_rates = [0.0]\n",
    "  tp_rates = [0.0]\n",
    "  auc = 0.0\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "\n",
    "  t_list = scores.sort_by{|e| [-e[0]]}\n",
    " \n",
    "  i = 0\n",
    "  total_pos = num_positives scores\n",
    "  total_neg = num_negatives scores\n",
    "  p = 0.0\n",
    "  n = 0.0\n",
    "\n",
    "  t_list.each do |e|\n",
    "\n",
    "    if e[1] == 1\n",
    "      p +=1.0   \n",
    "      fp_rates.append(fp_rates[-1])      \n",
    "      tp_rates.append(p/total_pos)\n",
    "    else\n",
    "      n +=1.0 \n",
    "      fp_rates.append(n/total_neg)      \n",
    "      tp_rates.append(tp_rates[-1])\n",
    "    end\n",
    "\n",
    "  end\n",
    "\n",
    "  i = 1\n",
    "  while i < (tp_rates.length) do\n",
    "      auc+= 0.5*(fp_rates[i] -fp_rates[i-1])*(tp_rates[i] + tp_rates[i-1]  )\n",
    "    i+=1\n",
    "  end\n",
    "  \n",
    "\n",
    "  #END YOUR CODE\n",
    "    return [fp_rates, tp_rates, auc]\n",
    "  end\n",
    "\n",
    "\n",
    "\n",
    "#   def calc_auc_only scores\n",
    "#     fp, tp, auc = roc_curve scores\n",
    "#     return auc\n",
    "#   end\n",
    "  def apply scores\n",
    "    fp, tp, auc = roc_curve scores\n",
    "    return auc\n",
    "  end\n",
    "\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c67c9759a945f589894ee0b0bf8ab8d0",
     "grade": false,
     "grade_id": "cell-cc6d92a466da0d44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Transformers\n",
    "Refactor / copy your ```FeatureTransformer``` implementations to the cell below. Extra transformers has been implemented for you in a file ```transformers.rb``` to demonstrate how to apply feature transforms before training learners. There is also a wrapped learner that can transform data and then train any learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e091b71efe6eaf28d91d83578a1599",
     "grade": false,
     "grade_id": "cell-a75c8fea3e028d75",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":feature_transformer"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "def feature_transformer\n",
    "  FeatureTransformPipeline.new(\n",
    "    #ext_source\n",
    "    ZScoreTransformer.new(%w(ext_source_1 ext_source_2 ext_source_3)),\n",
    "    MeanImputation.new(%w(ext_source_1 ext_source_2 ext_source_3)),\n",
    "    \n",
    "    #Treat amt_income_total and amt_credit as log normal\n",
    "    LogTransform.new(%w(amt_income_total amt_credit)),\n",
    "    ZScoreTransformer.new(%w(log_amt_income_total log_amt_credit)),\n",
    "      \n",
    "    #Imputation for commonarea_avg\n",
    "    MeanImputation.new(%w(commonarea_avg)),      #### log_amt_income_total log_amt_credit ?\n",
    "      \n",
    "    #One-hot encoded features\n",
    "    AgeRangeAsVector.new,      \n",
    "    OneHotEncoding.new(%w(name_family_status code_gender)),\n",
    "      \n",
    "    #Target Averages\n",
    "    TargetAveraging.new(%w(name_income_type flag_own_car flag_own_realty\n",
    "      name_family_status organization_type name_housing_type name_education_type)),      \n",
    "    L2Normalize.new\n",
    "  )\n",
    "end\n",
    "\n",
    "\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":FeatureTransformPipeline"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FeatureTransformPipeline\n",
    "  def initialize *transformers\n",
    "    @transformers = transformers\n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "    # BEGIN YOUR CODE \n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i< @transformers.length do\n",
    "        @transformers[i].train dataset \n",
    "\n",
    "        j = 0\n",
    "        while j<dataset[\"data\"].length do\n",
    "  \n",
    "          c = dataset[\"data\"][j]\n",
    "          dataset[\"data\"][i] = (@transformers[i].apply  [c])[0]\n",
    "\n",
    "          j+=1\n",
    "        end\n",
    "      i+=1\n",
    "    end\n",
    "       \n",
    "    return dataset\n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def apply example_batch \n",
    "    return @transformers.inject(example_batch) do |u, transform|\n",
    "      u = transform.apply example_batch\n",
    "\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  spambase = load_spambase_dataset()  \n",
    "#   linear = LogisticRegressionLearner.new(regularization: 0.01, learning_rate: 0.001, batch_size: 20, epochs: 1)\n",
    "  \n",
    "#   ## Provide your own name to a TransformingLearner\n",
    "#   zlearner = CopyingTransformingLearner.new(ZScoreTransformer.new(spambase[\"features\"]), linear)  \n",
    "#   zlearner.name = \"ZScore_Logistic\"\n",
    "  \n",
    "#   learners = [zlearner]\n",
    "#   puts \"debug\",learners,\"debug\"\n",
    "#   df, learner_summary = parameter_search learners, spambase\n",
    "#   best_model_stats = learner_summary[\"ZScore_Logistic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dd40175cc8661e71a47a452c402dae5",
     "grade": true,
     "grade_id": "cell-0f2399ea2fd0967e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV: CopyingTransformingLearner, parameters: {\"regularization\"=>0.01, \"learning_rate\"=>0.001, \"epochs\"=>1, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}\n",
      "0.624034272645382\n",
      "{\n",
      "  \"ZScore_Logistic\": {\n",
      "    \"learner\": \"ZScore_Logistic\",\n",
      "    \"parameters\": {\n",
      "      \"regularization\": 0.01,\n",
      "      \"learning_rate\": 0.001,\n",
      "      \"epochs\": 1,\n",
      "      \"batch_size\": 20,\n",
      "      \"learner\": \"LogisticRegressionLearner\"\n",
      "    },\n",
      "    \"folds\": 5,\n",
      "    \"mean_train_metric\": 0.621668199631282,\n",
      "    \"stdev_train_metric\": 0.024901567194550795,\n",
      "    \"mean_test_metric\": 0.624034272645382,\n",
      "    \"stdev_test_metric\": 0.027451654221655422\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> Daru::DataFrame(1x7) </b>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  \n",
       "    <tr>\n",
       "      <th></th>\n",
       "      \n",
       "        <th>learner</th>\n",
       "      \n",
       "        <th>parameters</th>\n",
       "      \n",
       "        <th>folds</th>\n",
       "      \n",
       "        <th>mean_train_metric</th>\n",
       "      \n",
       "        <th>stdev_train_metric</th>\n",
       "      \n",
       "        <th>mean_test_metric</th>\n",
       "      \n",
       "        <th>stdev_test_metric</th>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "</thead>\n",
       "  <tbody>\n",
       "  \n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      \n",
       "        <td>ZScore_Logistic</td>\n",
       "      \n",
       "        <td>{\"regularization\"=>0.01, \"learning_rate\"=>0.001, \"epochs\"=>1, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}</td>\n",
       "      \n",
       "        <td>5</td>\n",
       "      \n",
       "        <td>0.621668199631282</td>\n",
       "      \n",
       "        <td>0.024901567194550795</td>\n",
       "      \n",
       "        <td>0.624034272645382</td>\n",
       "      \n",
       "        <td>0.027451654221655422</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "\n",
       "  \n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "#<Daru::DataFrame(1x7)>\n",
       "                               learner         parameters              folds  mean_train_metric stdev_train_metric   mean_test_metric  stdev_test_metric\n",
       "                  0    ZScore_Logistic {\"regularization\"=                  5  0.621668199631282 0.0249015671945507  0.624034272645382 0.0274516542216554"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_0f2399()\n",
    "  spambase = load_spambase_dataset()  \n",
    "  linear = LogisticRegressionLearner.new(regularization: 0.01, learning_rate: 0.001, batch_size: 20, epochs: 1)\n",
    "  \n",
    "  ## Provide your own name to a TransformingLearner\n",
    "  zlearner = CopyingTransformingLearner.new(ZScoreTransformer.new(spambase[\"features\"]), linear)  \n",
    "  zlearner.name = \"ZScore_Logistic\"\n",
    "  \n",
    "  learners = [zlearner]\n",
    "  \n",
    "  df, learner_summary = parameter_search learners, spambase\n",
    "  best_model_stats = learner_summary[\"ZScore_Logistic\"]\n",
    "  assert_true best_model_stats[\"mean_test_metric\"] > 0.6, \"Must return AUC > 0.6\"\n",
    "  \n",
    "  df\n",
    "end\n",
    "test_0f2399()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff145d8c9b32e48311fa0fec232cb678",
     "grade": true,
     "grade_id": "cell-ef8eedcb3edebf85",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV: CopyingTransformingLearner, parameters: {\"regularization\"=>0.0001, \"learning_rate\"=>0.01, \"epochs\"=>50, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}\n",
      "0.6719494060200436\n",
      "{\n",
      "  \"OneHot_ZScore_Log_Logistic\": {\n",
      "    \"learner\": \"OneHot_ZScore_Log_Logistic\",\n",
      "    \"parameters\": {\n",
      "      \"regularization\": 0.0001,\n",
      "      \"learning_rate\": 0.01,\n",
      "      \"epochs\": 50,\n",
      "      \"batch_size\": 20,\n",
      "      \"learner\": \"LogisticRegressionLearner\"\n",
      "    },\n",
      "    \"folds\": 5,\n",
      "    \"mean_train_metric\": 0.6875603552194638,\n",
      "    \"stdev_train_metric\": 0.08545420079756087,\n",
      "    \"mean_test_metric\": 0.6719494060200436,\n",
      "    \"stdev_test_metric\": 0.1068412459299978\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def test_ef8eed()\n",
    "  german_credit = load_german_credit_dataset()  \n",
    "  \n",
    "  linear = LogisticRegressionLearner.new(regularization: 0.0001, learning_rate: 0.01, batch_size: 20, epochs: 50)    \n",
    "  transformer = FeatureTransformPipeline.new(\n",
    "    OneHotEncoding.new(%w(checking_account credit_history purpose savings job_tenure)),\n",
    "    OneHotEncoding.new(%w(personal_status_gender other_debtors property other_installments housing)),\n",
    "    OneHotEncoding.new(%w(job has_telephone is_foreign_worker)),\n",
    "    ZScoreTransformer.new(%w(loan_duration installment_to_salary residence_tenure age)),\n",
    "    LogTransform.new(%w(credit_amount))\n",
    "  )\n",
    "  \n",
    "  zlearner = CopyingTransformingLearner.new(transformer, linear)  \n",
    "  zlearner.name = \"OneHot_ZScore_Log_Logistic\"\n",
    "  learners = [zlearner]\n",
    "  df, best_model = parameter_search learners, german_credit\n",
    "\n",
    "  best_model_stats = best_model[zlearner.name]\n",
    "  assert_true best_model_stats[\"mean_test_metric\"] > 0.65, \"Must return AUC > 0.65\"\n",
    "end\n",
    "test_ef8eed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27af47ce08723dd96583e005da00dcac",
     "grade": false,
     "grade_id": "cell-f9a425d32a9d4401",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Putting it together\n",
    "\n",
    "Now, we will test different type models on the same dataset and compare. This is similar to what will happen in the final project, so use this test as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7672a720d9d577f47163e68314aa2815",
     "grade": true,
     "grade_id": "cell-a0551aba6e312271",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV: CopyingTransformingLearner, parameters: {\"regularization\"=>0.0001, \"learning_rate\"=>0.01, \"epochs\"=>50, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}\n",
      "0.6719494060200436\n",
      "5-fold CV: CopyingTransformingLearner, parameters: {\"regularization\"=>0.1, \"learning_rate\"=>0.01, \"epochs\"=>50, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}\n",
      "0.6715707131371996\n",
      "{\n",
      "  \"logistic-regression-0.0001\": {\n",
      "    \"learner\": \"logistic-regression-0.0001\",\n",
      "    \"parameters\": {\n",
      "      \"regularization\": 0.0001,\n",
      "      \"learning_rate\": 0.01,\n",
      "      \"epochs\": 50,\n",
      "      \"batch_size\": 20,\n",
      "      \"learner\": \"LogisticRegressionLearner\"\n",
      "    },\n",
      "    \"folds\": 5,\n",
      "    \"mean_train_metric\": 0.6875603552194638,\n",
      "    \"stdev_train_metric\": 0.08545420079756087,\n",
      "    \"mean_test_metric\": 0.6719494060200436,\n",
      "    \"stdev_test_metric\": 0.1068412459299978\n",
      "  },\n",
      "  \"logistic-regression-0.1\": {\n",
      "    \"learner\": \"logistic-regression-0.1\",\n",
      "    \"parameters\": {\n",
      "      \"regularization\": 0.1,\n",
      "      \"learning_rate\": 0.01,\n",
      "      \"epochs\": 50,\n",
      "      \"batch_size\": 20,\n",
      "      \"learner\": \"LogisticRegressionLearner\"\n",
      "    },\n",
      "    \"folds\": 5,\n",
      "    \"mean_train_metric\": 0.6863714492550004,\n",
      "    \"stdev_train_metric\": 0.08512325619852246,\n",
      "    \"mean_test_metric\": 0.6715707131371996,\n",
      "    \"stdev_test_metric\": 0.10715379554132377\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> Daru::DataFrame(2x7) </b>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  \n",
       "    <tr>\n",
       "      <th></th>\n",
       "      \n",
       "        <th>learner</th>\n",
       "      \n",
       "        <th>parameters</th>\n",
       "      \n",
       "        <th>folds</th>\n",
       "      \n",
       "        <th>mean_train_metric</th>\n",
       "      \n",
       "        <th>stdev_train_metric</th>\n",
       "      \n",
       "        <th>mean_test_metric</th>\n",
       "      \n",
       "        <th>stdev_test_metric</th>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "</thead>\n",
       "  <tbody>\n",
       "  \n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      \n",
       "        <td>logistic-regression-0.0001</td>\n",
       "      \n",
       "        <td>{\"regularization\"=>0.0001, \"learning_rate\"=>0.01, \"epochs\"=>50, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}</td>\n",
       "      \n",
       "        <td>5</td>\n",
       "      \n",
       "        <td>0.6875603552194638</td>\n",
       "      \n",
       "        <td>0.08545420079756087</td>\n",
       "      \n",
       "        <td>0.6719494060200436</td>\n",
       "      \n",
       "        <td>0.1068412459299978</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      \n",
       "        <td>logistic-regression-0.1</td>\n",
       "      \n",
       "        <td>{\"regularization\"=>0.1, \"learning_rate\"=>0.01, \"epochs\"=>50, \"batch_size\"=>20, \"learner\"=>\"LogisticRegressionLearner\"}</td>\n",
       "      \n",
       "        <td>5</td>\n",
       "      \n",
       "        <td>0.6863714492550004</td>\n",
       "      \n",
       "        <td>0.08512325619852246</td>\n",
       "      \n",
       "        <td>0.6715707131371996</td>\n",
       "      \n",
       "        <td>0.10715379554132377</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "\n",
       "  \n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "#<Daru::DataFrame(2x7)>\n",
       "                               learner         parameters              folds  mean_train_metric stdev_train_metric   mean_test_metric  stdev_test_metric\n",
       "                  0 logistic-regressio {\"regularization\"=                  5 0.6875603552194638 0.0854542007975608 0.6719494060200436 0.1068412459299978\n",
       "                  1 logistic-regressio {\"regularization\"=                  5 0.6863714492550004 0.0851232561985224 0.6715707131371996 0.1071537955413237"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_a0551a()\n",
    "  german_credit = load_german_credit_dataset()\n",
    "\n",
    "  linear1 = LogisticRegressionLearner.new(regularization: 0.0001, learning_rate: 0.01, batch_size: 20, epochs: 50)    \n",
    "  transformer = FeatureTransformPipeline.new(\n",
    "    OneHotEncoding.new(%w(checking_account credit_history purpose savings job_tenure)),\n",
    "    OneHotEncoding.new(%w(personal_status_gender other_debtors property other_installments housing)),\n",
    "    OneHotEncoding.new(%w(job has_telephone is_foreign_worker)),\n",
    "    ZScoreTransformer.new(%w(loan_duration installment_to_salary residence_tenure age)),\n",
    "    LogTransform.new(%w(credit_amount))\n",
    "  )\n",
    "  learner1 = CopyingTransformingLearner.new(transformer, linear1)\n",
    "  learner1.name = \"logistic-regression-0.0001\"\n",
    "  \n",
    "  linear2 = LogisticRegressionLearner.new(regularization: 0.1, learning_rate: 0.01, batch_size: 20, epochs: 50)    \n",
    "  learner2 = CopyingTransformingLearner.new(transformer, linear2)\n",
    "  learner2.name = \"logistic-regression-0.1\"\n",
    "  \n",
    "  learners = [learner1, learner2] \n",
    "  df, model_stats = parameter_search learners, german_credit\n",
    "  \n",
    "  assert_true model_stats[\"logistic-regression-0.0001\"][\"mean_test_metric\"] > 0.65, \"Logistic Regression 1 > 0.65\"\n",
    "\n",
    "  auc1 = model_stats[\"logistic-regression-0.0001\"][\"mean_test_metric\"]\n",
    "  auc2 = model_stats[\"logistic-regression-0.1\"][\"mean_test_metric\"]\n",
    "  \n",
    "  assert_true auc1 > auc2, \"Linear Model with transforms beats Decision Tree out of the box\"  \n",
    "  df\n",
    "end\n",
    "test_a0551a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
