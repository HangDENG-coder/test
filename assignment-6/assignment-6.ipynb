{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e6ab857abc3cfe14bb37f38c65213f8",
     "grade": false,
     "grade_id": "cell-2b73ace9624518ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6: Kernels\n",
    "\n",
    "In this assignment, we will explore kernels and the kernelized Support Vector Machine. We will not be implementing the Support Vector Machine optimization algorithm, but you are welcome to do so. on your own. Instead, we will use the [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)--an industry-standard algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a524df4604bfcdefdd1bab0c5f30d206",
     "grade": false,
     "grade_id": "cell-92bb6ddd4870eefd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29c53022ea3f5804838abb0e16c79d22",
     "grade": false,
     "grade_id": "cell-f32d96bd8d255ddb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.1 (0 Points)\n",
    "\n",
    "Paste your ```dot```, ```norm```, ```mean```, and ```stdev``` methods in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8cc02688c41fc1f1a6596cce1f9bde3",
     "grade": false,
     "grade_id": "cell-af7a4deadaa69ebf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":stdev"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "   x_name = x.keys\n",
    "    w_name = w.keys\n",
    "\n",
    "    i = 0\n",
    "    dot = 0.0\n",
    "    if x_name.length == 0 and x.length == 0\n",
    "      dot =  0.0\n",
    "    else \n",
    "        while i<x_name.length do\n",
    "\n",
    "          if w_name.include?x_name[i]\n",
    "            index = w_name.index { |x| [x_name[i]].include?(x) }\n",
    "            if x[x_name[i]].is_a? Numeric  and w[w_name[index]].is_a? Numeric\n",
    "              dot += x[x_name[i]]*w[w_name[index]]\n",
    "            else \n",
    "              dot +=0.0\n",
    "            end\n",
    "          else \n",
    "            dot += 0.0\n",
    "          end\n",
    "          i+=1\n",
    "        end  \n",
    "    end\n",
    "  return dot\n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return Math.sqrt(dot w, w)\n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "def mean x\n",
    "  # BEGIN YOUR CODE\n",
    "  x.inject(0.0){|sum,i| sum + i }/x.length()  \n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "def stdev x\n",
    "  # BEGIN YOUR CODE\n",
    "  mu = mean x\n",
    "  su = x.inject(0.0){|sum,i| sum + (i-mu)**2 }/(x.length()-1)\n",
    "  Math.sqrt(su) \n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65812c80a8272cce537093aa62af428a",
     "grade": false,
     "grade_id": "cell-a3a301d3c794f30e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.2 (5 Points)\n",
    "\n",
    "Implement the ```calculate_margin``` function below which determines the geometric margin. Recall that the margin requires calculating the norm of the weight vector--without the bias. Recall that the margin is just one number, so the output of the function below is just one value.\n",
    "\n",
    "The following image shows the optimal margin for a sample dataset. This corresponds to one of the weight vectors below, you will figure out which one.\n",
    "\n",
    "![margin-separable.png](margin-separable.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c454e13a5bd78739a707117b625a4e3",
     "grade": false,
     "grade_id": "cell-b5f4f4cfd3514fd8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":calculate_margin"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_margin dataset, w\n",
    "  # BEGIN YOUR CODE\n",
    "  i = 0\n",
    "  margin_distance = []\n",
    "  while i< dataset[\"data\"].length do\n",
    "    x = dataset[\"data\"][i][\"features\"]\n",
    "    margin_distance.append((dot(x,w)+w[\"bias\"])*dataset[\"data\"][i][\"label\"])\n",
    "    i+=1\n",
    "  end\n",
    "  if (w.keys).include?(\"bias\")\n",
    "    w.delete(\"bias\")\n",
    "    weights_norm = norm(w)\n",
    "  else\n",
    "    weights_norm = norm(w)\n",
    "  end\n",
    "  return margin_distance.min/weights_norm\n",
    "\n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8422104e70c7a2d0bcf85e9d865b0f19",
     "grade": true,
     "grade_id": "cell-3c5e678aa4ab53c9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin of W2\n",
      "-1.5575225394931038\n",
      "Margin of W3\n",
      "-5.2391068771758285\n",
      "Margin of W4\n",
      "-1.7676466746001211\n",
      "Margin of W5\n",
      "0.2809754813868902\n"
     ]
    }
   ],
   "source": [
    "def test_3c5e67()\n",
    "  dataset = two_gaussians_sep_dataset()\n",
    "\n",
    "  w1 = {\"x1\" => 0.341517248849071, \"x2\" => 0.22698102802980452, \"bias\" => -1.4860798684346137}\n",
    "  w2 = {\"x1\" => 1.9615964448725158, \"x2\"=>1.322858357015272, \"bias\"=>-9.134050645364391}\n",
    "  w3 = {\"x1\" => 2.002639618673358, \"x2\" => 1.4715091416394062, \"bias\" => 0.018309357180013794}\n",
    "  w4 = {\"x1\" => 4.13242464849763, \"x2\" => 2.50762341344167, \"bias\" => -18.6590519796601}\n",
    "  w5 = {\"x1\" => 1.0633739433059786, \"x2\"=>1.2870656701510783, \"bias\"=>-7.544598345465133}\n",
    "  puts \"Margin of W2\", calculate_margin(dataset, w2)\n",
    "  puts \"Margin of W3\", calculate_margin(dataset, w3)\n",
    "  puts \"Margin of W4\", calculate_margin(dataset, w4)\n",
    "  puts \"Margin of W5\", calculate_margin(dataset, w5)\n",
    "  assert_in_delta -1.824358578224836, calculate_margin(dataset, w1), 1e-3\n",
    "\n",
    "end \n",
    "test_3c5e67()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25bb916fb7e6357aea8331fc634d6528",
     "grade": false,
     "grade_id": "cell-5ae29ca31100178e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.3 (5 Points)\n",
    "Which of the above weight vectors corresponds to the margin in the figure, which shows the maximum margin for the ```two_gaussians_sep_dataset```?\n",
    "\n",
    "Your answer should be as follows. It is case sensitive.\n",
    "\n",
    "```ruby\n",
    "def answer_e785ee()\n",
    "    return \"W8\" \n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60b67733cb315539651533ec7d19bcd4",
     "grade": false,
     "grade_id": "cell-e785ee2f39168160",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":answer_e785ee"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_e785ee()\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  \n",
    "  return \"W5\"\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "760dfae8a35cec6279cc36e342c36eca",
     "grade": true,
     "grade_id": "cell-f04be25f2ef7445d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_f04be2()\n",
    "  assert_true answer_e785ee().is_a?(String)\n",
    "end\n",
    "test_f04be2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0e48e61929182f47603e6ff16407294",
     "grade": false,
     "grade_id": "cell-da798f2334f08f4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.4 (10 Points)\n",
    "Now calculate the margin for different dataset which is not linearly separable. Which of the weight vectors below corresponds to its maximum margin--the one shown in the figure below?\n",
    "\n",
    "![margin-non-separable](./margin-non-separable.png)\n",
    "\n",
    "Your answer should be as follows. It is case sensitive.\n",
    "\n",
    "```ruby\n",
    "def answer_d218ea()\n",
    "    return \"W8\" \n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf32f57942b52ee6acb4a58d96e2f011",
     "grade": true,
     "grade_id": "cell-fe184032144478fd",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2\n",
      "-0.6951456368580061\n",
      "W3\n",
      "-4.596769219555084\n",
      "W4\n",
      "-0.7344371684609744\n",
      "W5\n",
      "-1.6663984698598633\n"
     ]
    }
   ],
   "source": [
    "def test_3c5e67()\n",
    "  dataset = two_gaussians_dataset()\n",
    "\n",
    "  w1 = {\"x1\" => 0.341517248849071, \"x2\" => 0.22698102802980452, \"bias\" => -1.4860798684346137}\n",
    "  w2 = {\"x1\" => 1.9615964448725158, \"x2\"=>1.322858357015272, \"bias\"=>-9.134050645364391}\n",
    "  w3 = {\"x1\" => 2.002639618673358, \"x2\" => 1.4715091416394062, \"bias\" => 0.018309357180013794}\n",
    "  w4 = {\"x1\" => 4.13242464849763, \"x2\" => 2.50762341344167, \"bias\" => -18.6590519796601}\n",
    "  w5 = {\"x1\" => 1.0633739433059786, \"x2\"=>1.2870656701510783, \"bias\"=>-7.544598345465133}\n",
    "  puts \"W2\", calculate_margin(dataset, w2)\n",
    "  puts \"W3\", calculate_margin(dataset, w3)\n",
    "  puts \"W4\", calculate_margin(dataset, w4)\n",
    "  puts \"W5\", calculate_margin(dataset, w5)\n",
    "  assert_in_delta -0.9260633144926164, calculate_margin(dataset, w1), 1e-3\n",
    "end \n",
    "test_3c5e67()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f589514da85900c43455576f0d81c7b",
     "grade": false,
     "grade_id": "cell-d218ea42ed4fa093",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":answer_d218ea"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_d218ea()\n",
    "  # BEGIN YOUR CODE\n",
    "  return \"W2\"\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5130ae4e3c03f72424e6ca66d0eea4bd",
     "grade": true,
     "grade_id": "cell-87f7a28f6466cdd6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_87f7a2()\n",
    "  assert_true answer_d218ea().is_a?(String)\n",
    "end\n",
    "test_87f7a2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f04330da00e6c3570b387065cf973bb",
     "grade": false,
     "grade_id": "cell-f2919bb6152165bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.4 (5 Points)\n",
    "\n",
    "We have seen two maximum margins in the questions above. Some are positive and some are negative. Why? Shouldn't the maximum margin always be positive?\n",
    "\n",
    "* **A**: The maximum margin is always positive, but in question 1.3, none of them was the maximum margin.\n",
    "* **B**: The maximum margin is only positive when the data is non-linearly separable such as Question 1.4.\n",
    "* **C**: The maximum margin is always positive and my implementation is probably just wrong.\n",
    "* **D**: The maximum margin is only positive when the data is linearly separable such as Question 1.3.\n",
    "\n",
    "If your answer to the question above it \"Z\", change the cell below as follows:\n",
    "```ruby\n",
    "def answer_b0ae4b()\n",
    "    return \"Z\" \n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "206be1802fde0b673942d2a43f6ae1df",
     "grade": false,
     "grade_id": "cell-b0ae4b9e9c4173a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":answer_b0ae4b"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_b0ae4b()\n",
    "  # BEGIN YOUR CODE\n",
    "  return \"D\"\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "781dd666ecf88950e7c06f9473bfa082",
     "grade": true,
     "grade_id": "cell-ae8da0cdafdd1351",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_ae8da0()\n",
    "  assert_true answer_b0ae4b().is_a?(String)\n",
    "end\n",
    "test_ae8da0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f1d1e325478d9dc2934f1124b07395f",
     "grade": false,
     "grade_id": "cell-402bcd35a2824c99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.5 (5 Points)\n",
    "\n",
    "Calculate the support vectors. Recall that the support vectors are those examples which lie either within the margin or exactly on the margin. Given a dataset and a weight vector, return an array of examples which are the support vectors. Do not reorder the examples from the dataset.\n",
    "\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Given the vector: ```{\"x1\" => 1.0633739433059786, \"x2\"=>1.2870656701510783, \"bias\"=>-7.544598345465133}```, return the vectors in an array: \n",
    "\n",
    "```ruby\n",
    "[\n",
    "    {\"features\"=>{\"x1\"=>6.47918178231868, \"x2\"=>1.28546395369992}, \"label\"=>1}\n",
    "    #...\n",
    "]   \n",
    "```\n",
    "\n",
    "Note:\n",
    "* The LibSVM package has a  tolerance of 0.001, so you may need to allow for this in calculating the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa13d23a97523ae9f32dea543ce5564d",
     "grade": false,
     "grade_id": "cell-7c188ad2d939c749",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":find_support_vectors"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_support_vectors dataset, w\n",
    "  # BEGIN YOUR CODE\n",
    "  support = []\n",
    "  i = 0\n",
    "  while i< dataset[\"data\"].length do\n",
    "    x =  (dot( dataset[\"data\"][i][\"features\"],w) + w[\"bias\"])*(dataset[\"data\"][i][\"label\"])\n",
    "    if x < 1+0.001\n",
    "      support.append(dataset[\"data\"][i])\n",
    "    else\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "  return support\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6a17158c8d633b95f28417391392b94",
     "grade": true,
     "grade_id": "cell-e69175486193c6df",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"features\"=>{\"x1\"=>6.47918178231868, \"x2\"=>1.28546395369992}, \"label\"=>1}\n",
      "{\"features\"=>{\"x1\"=>5.42385035965064, \"x2\"=>0.844550871352983}, \"label\"=>-1}\n",
      "{\"features\"=>{\"x1\"=>1.04738472853165, \"x2\"=>4.21929173416568}, \"label\"=>-1}\n",
      "{\"features\"=>{\"x1\"=>7.01297057699241, \"x2\"=>-0.708704476157131}, \"label\"=>-1}\n",
      "{\"features\"=>{\"x1\"=>5.07257528976597, \"x2\"=>2.03536512305274}, \"label\"=>1}\n"
     ]
    }
   ],
   "source": [
    "def test_e69175()\n",
    "  dataset = two_gaussians_sep_dataset()\n",
    "  w = {\"x1\" => 1.0633739433059786, \"x2\"=>1.2870656701510783, \"bias\"=>-7.544598345465133}\n",
    "  \n",
    "  support_vectors = find_support_vectors dataset, w\n",
    "  support_vectors.each {|sv| puts sv}\n",
    "  assert_equal 5, support_vectors.size\n",
    "  \n",
    "  sv_0 = support_vectors.first[\"features\"]\n",
    "  assert_in_delta 6.479, sv_0[\"x1\"], 1e-3\n",
    "  assert_in_delta 1.285, sv_0[\"x2\"], 1e-3\n",
    "end \n",
    "test_e69175()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e2c0b8187fbadcc582c9fcfc90de85a",
     "grade": false,
     "grade_id": "cell-92476e18e0b4015b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question 2\n",
    "\n",
    "Next, we will implement the ```LinearKernel```. Although we are using an existing package to train the Support Vector Machine, you will implement the predict function.\n",
    "\n",
    "\n",
    "## Question 2.1 (5 Points)\n",
    "\n",
    "Implement the linear kernel. Recall the formula for the linear kernel is as follows:\n",
    "\n",
    "# $K_L(x_i, x_j) = \\left < x_i, x_j \\right >$\n",
    "\n",
    "where $\\left < \\cdot \\right >$ is the inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ec1a1c155d092f1390a8b6cacccac93",
     "grade": false,
     "grade_id": "cell-cfcf37f4ef151b69",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":func"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearKernel\n",
    "  def update_parameter parameter\n",
    "    parameter.kernel_type = Libsvm::KernelType::LINEAR\n",
    "  end  \n",
    "  \n",
    "  def func x_i, x_j\n",
    "    # BEGIN YOUR CODE\n",
    "    return dot(x_i[\"features\"],x_j[\"features\"])\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18c83dafb102057d31bf4c76c5f66a83",
     "grade": true,
     "grade_id": "cell-ac7e575ba644ce04",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_ac7e57()\n",
    "  lin = LinearKernel.new\n",
    "  examples =  [\n",
    "    {\"features\" => {\"a\" => 2, \"b\" => 3}, \"label\" => 1}, \n",
    "    {\"features\" => {\"b\" => 7, \"c\" => 1}, \"label\" => 7},\n",
    "    {\"features\" => {\"a\" => 2, \"b\" => 3}, \"label\" => 2}\n",
    "  ]\n",
    "  \n",
    "  z = {\"features\" => {\"a\" => -5, \"b\" => 2}} \n",
    "  \n",
    "  assert_equal -4, lin.func(z, examples[0]), \"1\"\n",
    "  assert_equal -4, lin.func(examples[0], z), \"2\"  \n",
    "  assert_equal 14.0, lin.func(z, examples[1]), \"3\"  \n",
    "  assert_equal 14.0, lin.func(examples[1], z), \"4\"    \n",
    "  assert_equal 21, lin.func(examples[0], examples[1]), \"5\"  \n",
    "end\n",
    "\n",
    "test_ac7e57()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (5 Points)\n",
    "\n",
    "The following is the ```train``` method for the ```SupportVectorMachineLearner```, which has been provided for you. The learner calls the LibSVM package and saves the model. Recall that for a Kernelized SVM, the model consists of the support vectors, the bias term, and the $\\alpha$ values. \n",
    "\n",
    "With the Linear kernel, we can reconstruct the weight vector from the kernelized model. Implement ```to_linear_weights``` to provide the weight vector. Note that these weights correspond to one of the weights  in Question 1.3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "559228f394cce3e06c547051b574290a",
     "grade": false,
     "grade_id": "cell-59bfb6192e38b4c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":train"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'libsvm'\n",
    "\n",
    "class SupportVectorMachineLearner\n",
    "  include Learner\n",
    "  attr_reader :model\n",
    "  def initialize complexity: 1.0, kernel: LinearKernel.new\n",
    "    @parameters = {\"complexity\" => complexity, \"kernel\" => kernel}\n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "    feature_to_id, id_to_feature = create_feature_maps dataset\n",
    "    libsvm_examples, libsvm_labels = dataset_to_libsvm dataset, feature_to_id\n",
    "\n",
    "    problem = Libsvm::Problem.new\n",
    "    parameter = Libsvm::SvmParameter.new\n",
    "\n",
    "    parameter.cache_size = 1\n",
    "    parameters[\"kernel\"].update_parameter(parameter)\n",
    "    parameter.eps = 0.001\n",
    "    parameter.c = @parameters[\"complexity\"]\n",
    "\n",
    "    problem.set_examples(libsvm_labels, libsvm_examples)\n",
    "\n",
    "    filename = \"libsvm-#{rand(1e9).to_i.to_s(36)}.model\"\n",
    "    begin\n",
    "      Libsvm::Model.train(problem, parameter).save(filename)      \n",
    "      @model = load_support_vectors(filename, id_to_feature)\n",
    "    ensure\n",
    "      File.delete filename if File.exists? filename\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3de36af1d665a6ddbfd07c73a9c2b780",
     "grade": false,
     "grade_id": "cell-379007f4e373273b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vectors\n",
      "{\"label\"=>1, \"features\"=>{\"x1\"=>6.4791818, \"x2\"=>1.285464}, \"alpha\"=>0.8143410058746133}\n",
      "{\"label\"=>1, \"features\"=>{\"x1\"=>5.0725753, \"x2\"=>2.0353651}, \"alpha\"=>1.0}\n",
      "{\"label\"=>-1, \"features\"=>{\"x1\"=>5.4238504, \"x2\"=>0.84455087}, \"alpha\"=>1.0}\n",
      "{\"label\"=>-1, \"features\"=>{\"x1\"=>1.0473847, \"x2\"=>4.2192917}, \"alpha\"=>0.31000059049063017}\n",
      "{\"label\"=>-1, \"features\"=>{\"x1\"=>7.0129706, \"x2\"=>-0.70870448}, \"alpha\"=>0.5043404153839827}\n",
      "Bias\n",
      "-7.544598345465133\n"
     ]
    }
   ],
   "source": [
    "def example_train_libsvm()\n",
    "  dataset = two_gaussians_sep_dataset()\n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new\n",
    "  \n",
    "  svm_learner.train dataset\n",
    "  \n",
    "  model = svm_learner.model\n",
    "  puts \"Support vectors\"\n",
    "  model[\"data\"].each {|x| puts x}\n",
    "  puts \"Bias\", model[\"bias\"]\n",
    "end\n",
    "example_train_libsvm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "928eda0ead5f28d0369c0eacc2566666",
     "grade": false,
     "grade_id": "cell-63f3b5a8c9429cc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":to_linear_weights"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_linear_weights model_linear\n",
    "  weights = Hash.new {|h,k| h[k] = 0.0} \n",
    "  weights[\"bias\"] = model_linear[\"bias\"]\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  data =  model_linear\n",
    "  name = data[\"features\"]\n",
    "  wei = Array.new(name.length, 0)\n",
    "\n",
    "  j = 0\n",
    "  while j<name.length do\n",
    "      i = 0\n",
    "      while i< data[\"data\"].length do\n",
    "        wei[j] += data[\"data\"][i][\"alpha\"]*data[\"data\"][i][\"features\"][name[j]]*data[\"data\"][i][\"label\"]\n",
    "        i+=1\n",
    "      end\n",
    "      weights.store(name[j],wei[j])\n",
    "      j+=1\n",
    "  end\n",
    "    \n",
    "\n",
    "  \n",
    "  #END YOUR CODE\n",
    "  \n",
    "  return weights\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "992ef6df92eb7ac1feb1f39f5656015e",
     "grade": true,
     "grade_id": "cell-c2e0012efe7281a4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"bias\"=>-7.544598345465133, \"x1\"=>1.0633739433059786, \"x2\"=>1.2870656701510783}\n"
     ]
    }
   ],
   "source": [
    "def test_c2e001()\n",
    "  dataset = two_gaussians_sep_dataset()\n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new  \n",
    "  svm_learner.train dataset  \n",
    "  model = svm_learner.model\n",
    "  \n",
    "  weights = to_linear_weights model  \n",
    "  puts weights\n",
    "\n",
    "  assert_in_delta 1.0634, weights[\"x1\"], 1e-3, \"x1\"\n",
    "  assert_in_delta 1.2871, weights[\"x2\"], 1e-3, \"x2\"\n",
    "end\n",
    "test_c2e001()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "527a33e3c0efaeb1d9bac818f954e18c",
     "grade": true,
     "grade_id": "cell-88346a1f530ab8ef",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"bias\"=>-9.134050645364391, \"x1\"=>1.9615964448725158, \"x2\"=>1.322858357015272}\n"
     ]
    }
   ],
   "source": [
    "def test_88346a()\n",
    "  dataset = two_gaussians_dataset()\n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new  \n",
    "  svm_learner.train dataset  \n",
    "  model = svm_learner.model\n",
    "  \n",
    "  weights = to_linear_weights model  \n",
    "  puts weights\n",
    "  \n",
    "  assert_in_delta 1.9616, weights[\"x1\"], 1e-3, \"x1\"\n",
    "  assert_in_delta 1.3229, weights[\"x2\"], 1e-3, \"x2\"\n",
    "end\n",
    "test_88346a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c86daaca896794fd487dcb295763ca1e",
     "grade": false,
     "grade_id": "cell-de8e2722a4b2c5da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2.2 (5 Points)\n",
    "\n",
    "Verify that the bias is calculated correctly. Each example in the support vector dataset has a value called ```alpha```. Recall that for each example $i$ there $0 < \\alpha_i < C$, we can calculate the bias directly. We will follow the convention used in LibSVM and return the average bias calculated based on this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82a935d67c9e4ba012b6deeb73d776ae",
     "grade": false,
     "grade_id": "cell-8b942099974706be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":estimate_bias"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_bias model_linear, c\n",
    "  est_bias = 0.0\n",
    "  \n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  w = to_linear_weights model_linear\n",
    "  b = []\n",
    "\n",
    "\n",
    "  i =0\n",
    "\n",
    "  while i< model_linear[\"data\"].length do\n",
    "    if model_linear[\"data\"][i][\"alpha\"] < c\n",
    "      b.append (model_linear[\"data\"][i][\"label\"] -dot(model_linear[\"data\"][i][\"features\"],w))\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "\n",
    "  est_bias =  mean( b )\n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return est_bias\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "582c6d6133a99aa3eea62fadc1f1115a",
     "grade": true,
     "grade_id": "cell-e2475abd5720571f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_e2475a()\n",
    "  dataset = two_gaussians_dataset()\n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new  \n",
    "  svm_learner.train dataset  \n",
    "  model = svm_learner.model\n",
    "  true_bias = model.delete \"bias\"\n",
    "  \n",
    "  estimated_bias = estimate_bias model, 1.0\n",
    "\n",
    "  assert_in_delta -9.1340, true_bias, 1e-3\n",
    "  assert_in_delta -9.1340, estimated_bias, 1e-3\n",
    "end\n",
    "test_e2475a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89cbab50831c89754c7b94c2c64d7457",
     "grade": true,
     "grade_id": "cell-78c3cc496b1a7344",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_78c3cc()\n",
    "  dataset = two_gaussians_sep_dataset()\n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new  \n",
    "  svm_learner.train dataset  \n",
    "  model = svm_learner.model\n",
    "  true_bias = model.delete \"bias\"\n",
    "  \n",
    "  estimated_bias = estimate_bias model, 1.0  \n",
    "  assert_in_delta -7.5446, true_bias, 1e-3\n",
    "  assert_in_delta -7.5446, estimated_bias, 1e-3\n",
    "end\n",
    "test_78c3cc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b264cc4c3e252c4dc7523fe57831a14",
     "grade": false,
     "grade_id": "cell-ae6495e16961f290",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2.3 (5 Points)\n",
    "\n",
    "After training the Support Vector Machine, let's verify that the trainer returns the same support vectors you calculated earlier. The verification is as follows:\n",
    "\n",
    "1. Train a model with the LibSVM libray to return support vectors, $S$\n",
    "1. Call your ```to_linear_weights``` to return a weight vector, $w$\n",
    "1. Call your ```find_support_vectors``` with $w$ to find the support vectors, $\\tilde{S}$\n",
    "1. Verify that $S \\approx \\tilde{S}$\n",
    "\n",
    "Obviously, you used the support vectors $S$ to calculate $w$ but the code earlier used the weight vector $w$ to calculate the support vectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9043c57cf9536c2aab553e9a189706dc",
     "grade": true,
     "grade_id": "cell-b2c94afb10b3b906",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_3c5e67()\n",
    "  dataset = two_gaussians_sep_dataset()  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new  \n",
    "  svm_learner.train dataset  \n",
    "  model = svm_learner.model\n",
    "  sorted_learned_support_vectors = model[\"data\"].sort_by {|sv| [sv[\"label\"], sv[\"x1\"]]}\n",
    "  \n",
    "  #Calculate the weights\n",
    "  weights = to_linear_weights model\n",
    "  \n",
    "  #Calculate the support vectors based on these weights\n",
    "  support_vectors = find_support_vectors dataset, weights\n",
    "  sorted_manual_support_vectors = support_vectors.sort_by {|sv| [sv[\"label\"], sv[\"x1\"]]}\n",
    "  \n",
    "  #Verify that they are the same\n",
    "  assert_equal 5, sorted_learned_support_vectors.size, \"T1\"\n",
    "  assert_equal 5, sorted_manual_support_vectors.size, \"T2\"\n",
    "  checked = 0\n",
    "  5.times do |i|\n",
    "    sv_learned = sorted_learned_support_vectors[i]\n",
    "    sv_manual = sorted_manual_support_vectors[i]\n",
    "    %w(x1 x2).each do |k|\n",
    "      assert_in_delta sv_learned[k], sv_manual[k], 1e-3, \"SV[#{i}][#{k}]\"\n",
    "      checked += 1\n",
    "    end\n",
    "  end\n",
    "  assert_equal 10, checked\n",
    "end \n",
    "test_3c5e67()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8e127497dbc267fa5895b2871c14c7a",
     "grade": false,
     "grade_id": "cell-dbbad89ea19af645",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2.4 (5 Points) \n",
    "\n",
    "Implement the ```predict``` function for the learner, which takes an example and uses the support vectors to predict the margin score for the example. Recall that the margin score is output of the SVM rather than the posterior $P(c\\mid x)$, which is not directly obtainable from an SVM. Make this predict function work for any kernel, not just the linear one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38a4c5cc8cd1163a07417043279018ff",
     "grade": false,
     "grade_id": "cell-5f5a021bd8e37692",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":predict"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SupportVectorMachineLearner   \n",
    "  def evaluate eval_dataset\n",
    "    examples = eval_dataset[\"data\"]\n",
    "    examples.map do |example|\n",
    "      score = predict(example)\n",
    "      label = example[\"label\"] > 0 ? 1 : 0\n",
    "      [score, label]\n",
    "    end\n",
    "  end\n",
    " \n",
    "  def predict example\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "      dataset = self.model\n",
    "\n",
    "      w = to_linear_weights dataset\n",
    "      x = example[\"features\"]\n",
    "    \n",
    "      sum  = 0.0\n",
    "    i = 0\n",
    "\n",
    "    while i< dataset[\"data\"].length do\n",
    " \n",
    "      sum+= @parameters[\"kernel\"].func(dataset[\"data\"][i],example) * dataset[\"data\"][i][\"alpha\"] * dataset[\"data\"][i][\"label\"]\n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    \n",
    "      margin_distance =sum+w[\"bias\"]\n",
    "    return margin_distance\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c13690e825b97f063a47973f6afd82fa",
     "grade": true,
     "grade_id": "cell-598d4a87e77894c4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.558039335277201\n",
      "-9.67134623207709\n",
      "0.9988808193584333\n"
     ]
    }
   ],
   "source": [
    "def test_598d4a()\n",
    "  dataset = two_gaussians_sep_dataset()  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new  \n",
    "  svm_learner.train dataset  \n",
    "  model = svm_learner.model\n",
    "\n",
    "  example1 = {\"features\" => {\"x1\" => 6.0, \"x2\" => 6.0}}  \n",
    "  score1 = svm_learner.predict(example1)\n",
    "  \n",
    "  puts score1  \n",
    "  assert_true(score1 > 0, \"example1 is positive\")\n",
    "  \n",
    "  example2 = {\"features\" => {\"x1\" => -2.0, \"x2\" => 0.0}}  \n",
    "  score2 = svm_learner.predict(example2)\n",
    "  puts score2\n",
    "  assert_true(score2 < 0, \"example2 is negative\")\n",
    "  \n",
    "  example3 = {\"features\" => {\"x1\" => 6.479, \"x2\" => 1.285}}  \n",
    "  score3 = svm_learner.predict(example3)\n",
    "  puts score3\n",
    "  assert_true(score3 > 0, \"example3 is a positive support vector\")  \n",
    "  assert_in_delta(1.0, score3, 1e-2)\n",
    "end\n",
    "test_598d4a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfe7cd3cfe96e430349cabdd99f96c9f",
     "grade": false,
     "grade_id": "cell-15de396269ce907a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2.5 (10 Points)\n",
    "\n",
    "Paste your ```AUCMetric``` implementation below, including ```roc_curve``` and ```apply``` which may have been called ```calc_auc_only``` in another assignment. We will use this to draw an ROC curve for some of our datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":confusion_matrix"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(scores, t)\n",
    "  matrix = Hash.new {|h,predicted_class| h[predicted_class] = Hash.new {|h,true_class| h[true_class] = 0.0}}\n",
    "  # BEGIN YOUR CODE\n",
    "  matrix[\"N\"][\"N\"] = 0\n",
    "  matrix[\"N\"][\"P\"] = 0\n",
    "  matrix[\"P\"][\"N\"] = 0\n",
    "  matrix[\"P\"][\"P\"] = 0\n",
    "  i = 0\n",
    "  while i<scores.length do\n",
    "    if scores[i][1]>0 and scores[i][0]>=t\n",
    "      matrix[\"P\"][\"P\"] += 1\n",
    "    elsif scores[i][1]>0 and scores[i][0]<t\n",
    "      matrix[\"N\"][\"P\"] += 1\n",
    "    elsif scores[i][1]<=0 and scores[i][0]>=t\n",
    "      matrix[\"P\"][\"N\"] += 1\n",
    "    elsif scores[i][1]<=0 and scores[i][0]<t\n",
    "      matrix[\"N\"][\"N\"] += 1\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":false_positive_rate"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def false_positive_rate(matrix, total_pos, total_neg)\n",
    "  # BEGIN YOUR CODE\n",
    "  if total_neg== 0 and matrix[\"P\"][\"N\"] == 0\n",
    "    rate = 1.0\n",
    "  elsif total_neg>0\n",
    "    rate =  matrix[\"P\"][\"N\"].to_f/total_neg\n",
    "  end\n",
    "  return  rate\n",
    "  \n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":true_positive_rate"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def true_positive_rate(matrix, total_pos, total_neg)\n",
    "  # BEGIN YOUR CODE\n",
    "  if total_pos== 0 and matrix[\"P\"][\"P\"] == 0\n",
    "    rate = 1.0\n",
    "  elsif total_pos>0\n",
    "    rate =  matrix[\"P\"][\"P\"].to_f/total_pos\n",
    "  end\n",
    "  return  rate \n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b39d648f8f80200e327fc8a4538f68e9",
     "grade": false,
     "grade_id": "cell-8eb34d1dd40e3ebc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":calc_auc_only"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AUCMetric \n",
    "  include Metric\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  \n",
    "  def roc_curve(scores)\n",
    "  fp_rates = [0.0]\n",
    "  tp_rates = [0.0]\n",
    "  auc = 0.0\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  \n",
    "  t_list = (scores.clone).transpose[0].sort!{|x, y| y <=> x}\n",
    "  score = (scores.clone).sort!{|x, y| y <=> x}\n",
    "  i=0\n",
    "  matrix = confusion_matrix(score, t_list[i])\n",
    "    total_pos = matrix[\"P\"][\"P\"] + matrix[\"N\"][\"P\"] \n",
    "    total_neg = matrix[\"P\"][\"N\"] + matrix[\"N\"][\"N\"]\n",
    "\n",
    "  while i< scores.length do\n",
    "    matrix = confusion_matrix(score, t_list[i])\n",
    "#     total_pos = matrix[\"P\"][\"P\"] + matrix[\"N\"][\"P\"] \n",
    "#     total_neg = matrix[\"P\"][\"N\"] + matrix[\"N\"][\"N\"]\n",
    "    fp_rates.append(false_positive_rate(matrix, total_pos, total_neg))\n",
    "    tp_rates.append(true_positive_rate(matrix, total_pos, total_neg))\n",
    "    auc+=(tp_rates[-1]+tp_rates[-2])*(fp_rates[-1]-fp_rates[-2])*0.5\n",
    "\n",
    "    i+=1\n",
    " \n",
    "  end\n",
    "  \n",
    "  #END YOUR CODE\n",
    "    return [fp_rates, tp_rates, auc]\n",
    "  end\n",
    "\n",
    "\n",
    "\n",
    "  def calc_auc_only scores\n",
    "    fp, tp, auc = roc_curve scores\n",
    "    auc\n",
    "  end\n",
    "\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AUCMetric \n",
    "#   include Metric\n",
    "#   # BEGIN YOUR CODE\n",
    "\n",
    "# # BEGIN YOUR CODE\n",
    "# def roc_curve(scores)\n",
    "#   fp_rates = [0.0]\n",
    "#   tp_rates = [0.0]\n",
    "#   auc = 0.0\n",
    "#   # BEGIN YOUR CODE\n",
    "#   total_pos = num_positive scores\n",
    "#   total_neg = num_negative scores\n",
    "#   p = 0.0\n",
    "#   n = 0.0\n",
    "#   score = (scores.clone).sort!{|x, y| y <=> x}\n",
    "\n",
    "#   while i< scores.length do\n",
    "#     if scores[i] == 1\n",
    "#       p +=1.0\n",
    "#       tp_rates.append(p/total_pos)\n",
    "#       fp_eates.append(fp_eates[-1])\n",
    "#     else\n",
    "#       n +=1.0\n",
    "#       tp_rates.append(fp_eates[-1])\n",
    "#       fp_eates.append(n/total_pos)\n",
    "#     end\n",
    "#     auc+=(tp_rates[-1]+tp_rates[-2])*(fp_rates[-1]-fp_rates[-2])*0.5\n",
    "#     i+=1 \n",
    "#   end\n",
    "\n",
    "  \n",
    "# #   #END YOUR CODE\n",
    "#   return [fp_rates, tp_rates, auc]\n",
    "# end\n",
    "\n",
    "# def calc_auc_only scores\n",
    "#   fp, tp, auc = roc_curve scores\n",
    "#   auc\n",
    "# end\n",
    "# #END YOUR CODE\n",
    "\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a77e06b4cb6d44204627c1678134063",
     "grade": true,
     "grade_id": "cell-13718ddc2ca32ecf",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model has 2 support vectors\n",
      "{\"labels\"=>[-1, 1], \"bias\"=>-3.603741848930266, \"features\"=>[\"x1\", \"x2\"], \"data\"=>[{\"label\"=>1, \"features\"=>{\"x1\"=>4.3748602, \"x2\"=>4.981015}, \"alpha\"=>0.24369264490672954}, {\"label\"=>-1, \"features\"=>{\"x1\"=>2.2732122, \"x2\"=>3.0341878}, \"alpha\"=>0.24369264490672954}]}\n",
      "AUC on test set is 0.9995676767676768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-f3346d82-b1d3-43b4-a463-49e5fab0c455'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"6ff06019-96d1-4706-84d9-a84cd62b6497\"}],\"options\":{\"x_label\":\"X\",\"y_label\":\"Y\",\"zoom\":true,\"width\":700,\"xrange\":[0.0,1.0],\"yrange\":[0.0,1.0]}}],\"data\":{\"6ff06019-96d1-4706-84d9-a84cd62b6497\":[{\"x\":0.0,\"y\":0.0},{\"x\":0.0,\"y\":0.002},{\"x\":0.0,\"y\":0.004},{\"x\":0.0,\"y\":0.006},{\"x\":0.0,\"y\":0.008},{\"x\":0.0,\"y\":0.01},{\"x\":0.0,\"y\":0.012},{\"x\":0.0,\"y\":0.014},{\"x\":0.0,\"y\":0.016},{\"x\":0.0,\"y\":0.018},{\"x\":0.0,\"y\":0.02},{\"x\":0.0,\"y\":0.022},{\"x\":0.0,\"y\":0.024},{\"x\":0.0,\"y\":0.026},{\"x\":0.0,\"y\":0.028},{\"x\":0.0,\"y\":0.03},{\"x\":0.0,\"y\":0.032},{\"x\":0.0,\"y\":0.034},{\"x\":0.0,\"y\":0.036},{\"x\":0.0,\"y\":0.038},{\"x\":0.0,\"y\":0.04},{\"x\":0.0,\"y\":0.042},{\"x\":0.0,\"y\":0.044},{\"x\":0.0,\"y\":0.046},{\"x\":0.0,\"y\":0.048},{\"x\":0.0,\"y\":0.05},{\"x\":0.0,\"y\":0.052},{\"x\":0.0,\"y\":0.054},{\"x\":0.0,\"y\":0.056},{\"x\":0.0,\"y\":0.058},{\"x\":0.0,\"y\":0.06},{\"x\":0.0,\"y\":0.062},{\"x\":0.0,\"y\":0.064},{\"x\":0.0,\"y\":0.066},{\"x\":0.0,\"y\":0.068},{\"x\":0.0,\"y\":0.07},{\"x\":0.0,\"y\":0.072},{\"x\":0.0,\"y\":0.074},{\"x\":0.0,\"y\":0.076},{\"x\":0.0,\"y\":0.078},{\"x\":0.0,\"y\":0.08},{\"x\":0.0,\"y\":0.082},{\"x\":0.0,\"y\":0.084},{\"x\":0.0,\"y\":0.086},{\"x\":0.0,\"y\":0.088},{\"x\":0.0,\"y\":0.09},{\"x\":0.0,\"y\":0.092},{\"x\":0.0,\"y\":0.094},{\"x\":0.0,\"y\":0.096},{\"x\":0.0,\"y\":0.098},{\"x\":0.0,\"y\":0.1},{\"x\":0.0,\"y\":0.102},{\"x\":0.0,\"y\":0.104},{\"x\":0.0,\"y\":0.106},{\"x\":0.0,\"y\":0.108},{\"x\":0.0,\"y\":0.11},{\"x\":0.0,\"y\":0.112},{\"x\":0.0,\"y\":0.114},{\"x\":0.0,\"y\":0.116},{\"x\":0.0,\"y\":0.118},{\"x\":0.0,\"y\":0.12},{\"x\":0.0,\"y\":0.122},{\"x\":0.0,\"y\":0.124},{\"x\":0.0,\"y\":0.126},{\"x\":0.0,\"y\":0.128},{\"x\":0.0,\"y\":0.13},{\"x\":0.0,\"y\":0.132},{\"x\":0.0,\"y\":0.134},{\"x\":0.0,\"y\":0.136},{\"x\":0.0,\"y\":0.138},{\"x\":0.0,\"y\":0.14},{\"x\":0.0,\"y\":0.142},{\"x\":0.0,\"y\":0.144},{\"x\":0.0,\"y\":0.146},{\"x\":0.0,\"y\":0.148},{\"x\":0.0,\"y\":0.15},{\"x\":0.0,\"y\":0.152},{\"x\":0.0,\"y\":0.154},{\"x\":0.0,\"y\":0.156},{\"x\":0.0,\"y\":0.158},{\"x\":0.0,\"y\":0.16},{\"x\":0.0,\"y\":0.162},{\"x\":0.0,\"y\":0.164},{\"x\":0.0,\"y\":0.166},{\"x\":0.0,\"y\":0.168},{\"x\":0.0,\"y\":0.17},{\"x\":0.0,\"y\":0.172},{\"x\":0.0,\"y\":0.174},{\"x\":0.0,\"y\":0.176},{\"x\":0.0,\"y\":0.178},{\"x\":0.0,\"y\":0.18},{\"x\":0.0,\"y\":0.182},{\"x\":0.0,\"y\":0.184},{\"x\":0.0,\"y\":0.186},{\"x\":0.0,\"y\":0.188},{\"x\":0.0,\"y\":0.19},{\"x\":0.0,\"y\":0.192},{\"x\":0.0,\"y\":0.194},{\"x\":0.0,\"y\":0.196},{\"x\":0.0,\"y\":0.198},{\"x\":0.0,\"y\":0.2},{\"x\":0.0,\"y\":0.202},{\"x\":0.0,\"y\":0.204},{\"x\":0.0,\"y\":0.206},{\"x\":0.0,\"y\":0.208},{\"x\":0.0,\"y\":0.21},{\"x\":0.0,\"y\":0.212},{\"x\":0.0,\"y\":0.214},{\"x\":0.0,\"y\":0.216},{\"x\":0.0,\"y\":0.218},{\"x\":0.0,\"y\":0.22},{\"x\":0.0,\"y\":0.222},{\"x\":0.0,\"y\":0.224},{\"x\":0.0,\"y\":0.226},{\"x\":0.0,\"y\":0.228},{\"x\":0.0,\"y\":0.23},{\"x\":0.0,\"y\":0.232},{\"x\":0.0,\"y\":0.234},{\"x\":0.0,\"y\":0.236},{\"x\":0.0,\"y\":0.238},{\"x\":0.0,\"y\":0.24},{\"x\":0.0,\"y\":0.242},{\"x\":0.0,\"y\":0.244},{\"x\":0.0,\"y\":0.246},{\"x\":0.0,\"y\":0.248},{\"x\":0.0,\"y\":0.25},{\"x\":0.0,\"y\":0.252},{\"x\":0.0,\"y\":0.254},{\"x\":0.0,\"y\":0.256},{\"x\":0.0,\"y\":0.258},{\"x\":0.0,\"y\":0.26},{\"x\":0.0,\"y\":0.262},{\"x\":0.0,\"y\":0.264},{\"x\":0.0,\"y\":0.266},{\"x\":0.0,\"y\":0.268},{\"x\":0.0,\"y\":0.27},{\"x\":0.0,\"y\":0.272},{\"x\":0.0,\"y\":0.274},{\"x\":0.0,\"y\":0.276},{\"x\":0.0,\"y\":0.278},{\"x\":0.0,\"y\":0.28},{\"x\":0.0,\"y\":0.282},{\"x\":0.0,\"y\":0.284},{\"x\":0.0,\"y\":0.286},{\"x\":0.0,\"y\":0.288},{\"x\":0.0,\"y\":0.29},{\"x\":0.0,\"y\":0.292},{\"x\":0.0,\"y\":0.294},{\"x\":0.0,\"y\":0.296},{\"x\":0.0,\"y\":0.298},{\"x\":0.0,\"y\":0.3},{\"x\":0.0,\"y\":0.302},{\"x\":0.0,\"y\":0.304},{\"x\":0.0,\"y\":0.306},{\"x\":0.0,\"y\":0.308},{\"x\":0.0,\"y\":0.31},{\"x\":0.0,\"y\":0.312},{\"x\":0.0,\"y\":0.314},{\"x\":0.0,\"y\":0.316},{\"x\":0.0,\"y\":0.318},{\"x\":0.0,\"y\":0.32},{\"x\":0.0,\"y\":0.322},{\"x\":0.0,\"y\":0.324},{\"x\":0.0,\"y\":0.326},{\"x\":0.0,\"y\":0.328},{\"x\":0.0,\"y\":0.33},{\"x\":0.0,\"y\":0.332},{\"x\":0.0,\"y\":0.334},{\"x\":0.0,\"y\":0.336},{\"x\":0.0,\"y\":0.338},{\"x\":0.0,\"y\":0.34},{\"x\":0.0,\"y\":0.342},{\"x\":0.0,\"y\":0.344},{\"x\":0.0,\"y\":0.346},{\"x\":0.0,\"y\":0.348},{\"x\":0.0,\"y\":0.35},{\"x\":0.0,\"y\":0.352},{\"x\":0.0,\"y\":0.354},{\"x\":0.0,\"y\":0.356},{\"x\":0.0,\"y\":0.358},{\"x\":0.0,\"y\":0.36},{\"x\":0.0,\"y\":0.362},{\"x\":0.0,\"y\":0.364},{\"x\":0.0,\"y\":0.366},{\"x\":0.0,\"y\":0.368},{\"x\":0.0,\"y\":0.37},{\"x\":0.0,\"y\":0.372},{\"x\":0.0,\"y\":0.374},{\"x\":0.0,\"y\":0.376},{\"x\":0.0,\"y\":0.378},{\"x\":0.0,\"y\":0.38},{\"x\":0.0,\"y\":0.382},{\"x\":0.0,\"y\":0.384},{\"x\":0.0,\"y\":0.386},{\"x\":0.0,\"y\":0.388},{\"x\":0.0,\"y\":0.39},{\"x\":0.0,\"y\":0.392},{\"x\":0.0,\"y\":0.394},{\"x\":0.0,\"y\":0.396},{\"x\":0.0,\"y\":0.398},{\"x\":0.0,\"y\":0.4},{\"x\":0.0,\"y\":0.402},{\"x\":0.0,\"y\":0.404},{\"x\":0.0,\"y\":0.406},{\"x\":0.0,\"y\":0.408},{\"x\":0.0,\"y\":0.41},{\"x\":0.0,\"y\":0.412},{\"x\":0.0,\"y\":0.414},{\"x\":0.0,\"y\":0.416},{\"x\":0.0,\"y\":0.418},{\"x\":0.0,\"y\":0.42},{\"x\":0.0,\"y\":0.422},{\"x\":0.0,\"y\":0.424},{\"x\":0.0,\"y\":0.426},{\"x\":0.0,\"y\":0.428},{\"x\":0.0,\"y\":0.43},{\"x\":0.0,\"y\":0.432},{\"x\":0.0,\"y\":0.434},{\"x\":0.0,\"y\":0.436},{\"x\":0.0,\"y\":0.438},{\"x\":0.0,\"y\":0.44},{\"x\":0.0,\"y\":0.442},{\"x\":0.0,\"y\":0.444},{\"x\":0.0,\"y\":0.446},{\"x\":0.0,\"y\":0.448},{\"x\":0.0,\"y\":0.45},{\"x\":0.0,\"y\":0.452},{\"x\":0.0,\"y\":0.454},{\"x\":0.0,\"y\":0.456},{\"x\":0.0,\"y\":0.458},{\"x\":0.0,\"y\":0.46},{\"x\":0.0,\"y\":0.462},{\"x\":0.0,\"y\":0.464},{\"x\":0.0,\"y\":0.466},{\"x\":0.0,\"y\":0.468},{\"x\":0.0,\"y\":0.47},{\"x\":0.0,\"y\":0.472},{\"x\":0.0,\"y\":0.474},{\"x\":0.0,\"y\":0.476},{\"x\":0.0,\"y\":0.478},{\"x\":0.0,\"y\":0.48},{\"x\":0.0,\"y\":0.482},{\"x\":0.0,\"y\":0.484},{\"x\":0.0,\"y\":0.486},{\"x\":0.0,\"y\":0.488},{\"x\":0.0,\"y\":0.49},{\"x\":0.0,\"y\":0.492},{\"x\":0.0,\"y\":0.494},{\"x\":0.0,\"y\":0.496},{\"x\":0.0,\"y\":0.498},{\"x\":0.0,\"y\":0.5},{\"x\":0.0,\"y\":0.502},{\"x\":0.0,\"y\":0.504},{\"x\":0.0,\"y\":0.506},{\"x\":0.0,\"y\":0.508},{\"x\":0.0,\"y\":0.51},{\"x\":0.0,\"y\":0.512},{\"x\":0.0,\"y\":0.514},{\"x\":0.0,\"y\":0.516},{\"x\":0.0,\"y\":0.518},{\"x\":0.0,\"y\":0.52},{\"x\":0.0,\"y\":0.522},{\"x\":0.0,\"y\":0.524},{\"x\":0.0,\"y\":0.526},{\"x\":0.0,\"y\":0.528},{\"x\":0.0,\"y\":0.53},{\"x\":0.0,\"y\":0.532},{\"x\":0.0,\"y\":0.534},{\"x\":0.0,\"y\":0.536},{\"x\":0.0,\"y\":0.538},{\"x\":0.0,\"y\":0.54},{\"x\":0.0,\"y\":0.542},{\"x\":0.0,\"y\":0.544},{\"x\":0.0,\"y\":0.546},{\"x\":0.0,\"y\":0.548},{\"x\":0.0,\"y\":0.55},{\"x\":0.0,\"y\":0.552},{\"x\":0.0,\"y\":0.554},{\"x\":0.0,\"y\":0.556},{\"x\":0.0,\"y\":0.558},{\"x\":0.0,\"y\":0.56},{\"x\":0.0,\"y\":0.562},{\"x\":0.0,\"y\":0.564},{\"x\":0.0,\"y\":0.566},{\"x\":0.0,\"y\":0.568},{\"x\":0.0,\"y\":0.57},{\"x\":0.0,\"y\":0.572},{\"x\":0.0,\"y\":0.574},{\"x\":0.0,\"y\":0.576},{\"x\":0.0,\"y\":0.578},{\"x\":0.0,\"y\":0.58},{\"x\":0.0,\"y\":0.582},{\"x\":0.0,\"y\":0.584},{\"x\":0.0,\"y\":0.586},{\"x\":0.0,\"y\":0.588},{\"x\":0.0,\"y\":0.59},{\"x\":0.0,\"y\":0.592},{\"x\":0.0,\"y\":0.594},{\"x\":0.0,\"y\":0.596},{\"x\":0.0,\"y\":0.598},{\"x\":0.0,\"y\":0.6},{\"x\":0.0,\"y\":0.602},{\"x\":0.0,\"y\":0.604},{\"x\":0.0,\"y\":0.606},{\"x\":0.0,\"y\":0.608},{\"x\":0.0,\"y\":0.61},{\"x\":0.0,\"y\":0.612},{\"x\":0.0,\"y\":0.614},{\"x\":0.0,\"y\":0.616},{\"x\":0.0,\"y\":0.618},{\"x\":0.0,\"y\":0.62},{\"x\":0.0,\"y\":0.622},{\"x\":0.0,\"y\":0.624},{\"x\":0.0,\"y\":0.626},{\"x\":0.0,\"y\":0.628},{\"x\":0.0,\"y\":0.63},{\"x\":0.0,\"y\":0.632},{\"x\":0.0,\"y\":0.634},{\"x\":0.0,\"y\":0.636},{\"x\":0.0,\"y\":0.638},{\"x\":0.0,\"y\":0.64},{\"x\":0.0,\"y\":0.642},{\"x\":0.0,\"y\":0.644},{\"x\":0.0,\"y\":0.646},{\"x\":0.0,\"y\":0.648},{\"x\":0.0,\"y\":0.65},{\"x\":0.0,\"y\":0.652},{\"x\":0.0,\"y\":0.654},{\"x\":0.0,\"y\":0.656},{\"x\":0.0,\"y\":0.658},{\"x\":0.0,\"y\":0.66},{\"x\":0.0,\"y\":0.662},{\"x\":0.0,\"y\":0.664},{\"x\":0.0,\"y\":0.666},{\"x\":0.0,\"y\":0.668},{\"x\":0.0,\"y\":0.67},{\"x\":0.0,\"y\":0.672},{\"x\":0.0,\"y\":0.674},{\"x\":0.0,\"y\":0.676},{\"x\":0.0,\"y\":0.678},{\"x\":0.0,\"y\":0.68},{\"x\":0.0,\"y\":0.682},{\"x\":0.0,\"y\":0.684},{\"x\":0.0,\"y\":0.686},{\"x\":0.0,\"y\":0.688},{\"x\":0.0,\"y\":0.69},{\"x\":0.0,\"y\":0.692},{\"x\":0.0,\"y\":0.694},{\"x\":0.0,\"y\":0.696},{\"x\":0.0,\"y\":0.698},{\"x\":0.0,\"y\":0.7},{\"x\":0.0,\"y\":0.702},{\"x\":0.0,\"y\":0.704},{\"x\":0.0,\"y\":0.706},{\"x\":0.0,\"y\":0.708},{\"x\":0.0,\"y\":0.71},{\"x\":0.0,\"y\":0.712},{\"x\":0.0,\"y\":0.714},{\"x\":0.0,\"y\":0.716},{\"x\":0.0,\"y\":0.718},{\"x\":0.0,\"y\":0.72},{\"x\":0.0,\"y\":0.722},{\"x\":0.0,\"y\":0.724},{\"x\":0.0,\"y\":0.726},{\"x\":0.0,\"y\":0.728},{\"x\":0.0,\"y\":0.73},{\"x\":0.0,\"y\":0.732},{\"x\":0.0,\"y\":0.734},{\"x\":0.0,\"y\":0.736},{\"x\":0.0,\"y\":0.738},{\"x\":0.0,\"y\":0.74},{\"x\":0.0,\"y\":0.742},{\"x\":0.0,\"y\":0.744},{\"x\":0.0,\"y\":0.746},{\"x\":0.0,\"y\":0.748},{\"x\":0.0,\"y\":0.75},{\"x\":0.0,\"y\":0.752},{\"x\":0.0,\"y\":0.754},{\"x\":0.0,\"y\":0.756},{\"x\":0.0,\"y\":0.758},{\"x\":0.0,\"y\":0.76},{\"x\":0.0,\"y\":0.762},{\"x\":0.0,\"y\":0.764},{\"x\":0.0,\"y\":0.766},{\"x\":0.0,\"y\":0.768},{\"x\":0.0,\"y\":0.77},{\"x\":0.0,\"y\":0.772},{\"x\":0.0,\"y\":0.774},{\"x\":0.0,\"y\":0.776},{\"x\":0.0,\"y\":0.778},{\"x\":0.0,\"y\":0.78},{\"x\":0.0,\"y\":0.782},{\"x\":0.0,\"y\":0.784},{\"x\":0.0,\"y\":0.786},{\"x\":0.0,\"y\":0.788},{\"x\":0.0,\"y\":0.79},{\"x\":0.0,\"y\":0.792},{\"x\":0.0,\"y\":0.794},{\"x\":0.0,\"y\":0.796},{\"x\":0.0,\"y\":0.798},{\"x\":0.0,\"y\":0.8},{\"x\":0.0,\"y\":0.802},{\"x\":0.0,\"y\":0.804},{\"x\":0.0,\"y\":0.806},{\"x\":0.0,\"y\":0.808},{\"x\":0.0,\"y\":0.81},{\"x\":0.0,\"y\":0.812},{\"x\":0.0,\"y\":0.814},{\"x\":0.0,\"y\":0.816},{\"x\":0.0,\"y\":0.818},{\"x\":0.0,\"y\":0.82},{\"x\":0.0,\"y\":0.822},{\"x\":0.0,\"y\":0.824},{\"x\":0.0,\"y\":0.826},{\"x\":0.0,\"y\":0.828},{\"x\":0.0,\"y\":0.83},{\"x\":0.0,\"y\":0.832},{\"x\":0.0,\"y\":0.834},{\"x\":0.0,\"y\":0.836},{\"x\":0.0,\"y\":0.838},{\"x\":0.0,\"y\":0.84},{\"x\":0.0,\"y\":0.842},{\"x\":0.0,\"y\":0.844},{\"x\":0.0,\"y\":0.846},{\"x\":0.0,\"y\":0.848},{\"x\":0.0,\"y\":0.85},{\"x\":0.0,\"y\":0.852},{\"x\":0.0,\"y\":0.854},{\"x\":0.0,\"y\":0.856},{\"x\":0.0,\"y\":0.858},{\"x\":0.0,\"y\":0.86},{\"x\":0.0,\"y\":0.862},{\"x\":0.0,\"y\":0.864},{\"x\":0.0,\"y\":0.866},{\"x\":0.0,\"y\":0.868},{\"x\":0.0,\"y\":0.87},{\"x\":0.0,\"y\":0.872},{\"x\":0.0,\"y\":0.874},{\"x\":0.0,\"y\":0.876},{\"x\":0.0,\"y\":0.878},{\"x\":0.0,\"y\":0.88},{\"x\":0.0,\"y\":0.882},{\"x\":0.0,\"y\":0.884},{\"x\":0.0,\"y\":0.886},{\"x\":0.0,\"y\":0.888},{\"x\":0.0,\"y\":0.89},{\"x\":0.0,\"y\":0.892},{\"x\":0.0,\"y\":0.894},{\"x\":0.0,\"y\":0.896},{\"x\":0.0,\"y\":0.898},{\"x\":0.0,\"y\":0.9},{\"x\":0.0,\"y\":0.902},{\"x\":0.0,\"y\":0.904},{\"x\":0.0,\"y\":0.906},{\"x\":0.0,\"y\":0.908},{\"x\":0.0,\"y\":0.91},{\"x\":0.0,\"y\":0.912},{\"x\":0.0,\"y\":0.914},{\"x\":0.0,\"y\":0.916},{\"x\":0.0,\"y\":0.918},{\"x\":0.0,\"y\":0.92},{\"x\":0.0,\"y\":0.922},{\"x\":0.0,\"y\":0.924},{\"x\":0.0,\"y\":0.926},{\"x\":0.0,\"y\":0.928},{\"x\":0.0,\"y\":0.93},{\"x\":0.0,\"y\":0.932},{\"x\":0.0,\"y\":0.934},{\"x\":0.0,\"y\":0.936},{\"x\":0.0,\"y\":0.938},{\"x\":0.0,\"y\":0.94},{\"x\":0.0,\"y\":0.942},{\"x\":0.0,\"y\":0.944},{\"x\":0.00202020202020202,\"y\":0.944},{\"x\":0.00202020202020202,\"y\":0.946},{\"x\":0.00202020202020202,\"y\":0.948},{\"x\":0.00202020202020202,\"y\":0.95},{\"x\":0.00202020202020202,\"y\":0.952},{\"x\":0.00202020202020202,\"y\":0.954},{\"x\":0.00202020202020202,\"y\":0.956},{\"x\":0.00202020202020202,\"y\":0.958},{\"x\":0.00202020202020202,\"y\":0.96},{\"x\":0.00404040404040404,\"y\":0.96},{\"x\":0.00404040404040404,\"y\":0.962},{\"x\":0.00404040404040404,\"y\":0.964},{\"x\":0.00404040404040404,\"y\":0.966},{\"x\":0.00404040404040404,\"y\":0.968},{\"x\":0.006060606060606061,\"y\":0.968},{\"x\":0.006060606060606061,\"y\":0.97},{\"x\":0.006060606060606061,\"y\":0.972},{\"x\":0.006060606060606061,\"y\":0.974},{\"x\":0.006060606060606061,\"y\":0.976},{\"x\":0.006060606060606061,\"y\":0.978},{\"x\":0.006060606060606061,\"y\":0.98},{\"x\":0.006060606060606061,\"y\":0.982},{\"x\":0.006060606060606061,\"y\":0.984},{\"x\":0.006060606060606061,\"y\":0.986},{\"x\":0.006060606060606061,\"y\":0.988},{\"x\":0.006060606060606061,\"y\":0.99},{\"x\":0.00808080808080808,\"y\":0.99},{\"x\":0.010101010101010102,\"y\":0.99},{\"x\":0.010101010101010102,\"y\":0.992},{\"x\":0.012121212121212121,\"y\":0.992},{\"x\":0.014141414141414142,\"y\":0.992},{\"x\":0.01616161616161616,\"y\":0.992},{\"x\":0.01616161616161616,\"y\":0.994},{\"x\":0.01818181818181818,\"y\":0.994},{\"x\":0.020202020202020204,\"y\":0.994},{\"x\":0.022222222222222223,\"y\":0.994},{\"x\":0.024242424242424242,\"y\":0.994},{\"x\":0.026262626262626262,\"y\":0.994},{\"x\":0.026262626262626262,\"y\":0.996},{\"x\":0.028282828282828285,\"y\":0.996},{\"x\":0.030303030303030304,\"y\":0.996},{\"x\":0.03232323232323232,\"y\":0.996},{\"x\":0.03232323232323232,\"y\":0.998},{\"x\":0.03232323232323232,\"y\":1.0},{\"x\":0.03434343434343434,\"y\":1.0},{\"x\":0.03636363636363636,\"y\":1.0},{\"x\":0.03838383838383838,\"y\":1.0},{\"x\":0.04040404040404041,\"y\":1.0},{\"x\":0.04242424242424243,\"y\":1.0},{\"x\":0.044444444444444446,\"y\":1.0},{\"x\":0.046464646464646465,\"y\":1.0},{\"x\":0.048484848484848485,\"y\":1.0},{\"x\":0.050505050505050504,\"y\":1.0},{\"x\":0.052525252525252523,\"y\":1.0},{\"x\":0.05454545454545454,\"y\":1.0},{\"x\":0.05656565656565657,\"y\":1.0},{\"x\":0.05858585858585859,\"y\":1.0},{\"x\":0.06060606060606061,\"y\":1.0},{\"x\":0.06262626262626263,\"y\":1.0},{\"x\":0.06464646464646465,\"y\":1.0},{\"x\":0.06666666666666667,\"y\":1.0},{\"x\":0.06868686868686869,\"y\":1.0},{\"x\":0.0707070707070707,\"y\":1.0},{\"x\":0.07272727272727272,\"y\":1.0},{\"x\":0.07474747474747474,\"y\":1.0},{\"x\":0.07676767676767676,\"y\":1.0},{\"x\":0.07878787878787878,\"y\":1.0},{\"x\":0.08080808080808081,\"y\":1.0},{\"x\":0.08282828282828283,\"y\":1.0},{\"x\":0.08484848484848485,\"y\":1.0},{\"x\":0.08686868686868687,\"y\":1.0},{\"x\":0.08888888888888889,\"y\":1.0},{\"x\":0.09090909090909091,\"y\":1.0},{\"x\":0.09292929292929293,\"y\":1.0},{\"x\":0.09494949494949495,\"y\":1.0},{\"x\":0.09696969696969697,\"y\":1.0},{\"x\":0.09898989898989899,\"y\":1.0},{\"x\":0.10101010101010101,\"y\":1.0},{\"x\":0.10303030303030303,\"y\":1.0},{\"x\":0.10505050505050505,\"y\":1.0},{\"x\":0.10707070707070707,\"y\":1.0},{\"x\":0.10909090909090909,\"y\":1.0},{\"x\":0.1111111111111111,\"y\":1.0},{\"x\":0.11313131313131314,\"y\":1.0},{\"x\":0.11515151515151516,\"y\":1.0},{\"x\":0.11717171717171718,\"y\":1.0},{\"x\":0.1191919191919192,\"y\":1.0},{\"x\":0.12121212121212122,\"y\":1.0},{\"x\":0.12323232323232323,\"y\":1.0},{\"x\":0.12525252525252525,\"y\":1.0},{\"x\":0.12727272727272726,\"y\":1.0},{\"x\":0.1292929292929293,\"y\":1.0},{\"x\":0.13131313131313133,\"y\":1.0},{\"x\":0.13333333333333333,\"y\":1.0},{\"x\":0.13535353535353536,\"y\":1.0},{\"x\":0.13737373737373737,\"y\":1.0},{\"x\":0.1393939393939394,\"y\":1.0},{\"x\":0.1414141414141414,\"y\":1.0},{\"x\":0.14343434343434344,\"y\":1.0},{\"x\":0.14545454545454545,\"y\":1.0},{\"x\":0.14747474747474748,\"y\":1.0},{\"x\":0.1494949494949495,\"y\":1.0},{\"x\":0.15151515151515152,\"y\":1.0},{\"x\":0.15353535353535352,\"y\":1.0},{\"x\":0.15555555555555556,\"y\":1.0},{\"x\":0.15757575757575756,\"y\":1.0},{\"x\":0.1595959595959596,\"y\":1.0},{\"x\":0.16161616161616163,\"y\":1.0},{\"x\":0.16363636363636364,\"y\":1.0},{\"x\":0.16565656565656567,\"y\":1.0},{\"x\":0.16767676767676767,\"y\":1.0},{\"x\":0.1696969696969697,\"y\":1.0},{\"x\":0.1717171717171717,\"y\":1.0},{\"x\":0.17373737373737375,\"y\":1.0},{\"x\":0.17575757575757575,\"y\":1.0},{\"x\":0.17777777777777778,\"y\":1.0},{\"x\":0.1797979797979798,\"y\":1.0},{\"x\":0.18181818181818182,\"y\":1.0},{\"x\":0.18383838383838383,\"y\":1.0},{\"x\":0.18585858585858586,\"y\":1.0},{\"x\":0.18787878787878787,\"y\":1.0},{\"x\":0.1898989898989899,\"y\":1.0},{\"x\":0.1919191919191919,\"y\":1.0},{\"x\":0.19393939393939394,\"y\":1.0},{\"x\":0.19595959595959597,\"y\":1.0},{\"x\":0.19797979797979798,\"y\":1.0},{\"x\":0.2,\"y\":1.0},{\"x\":0.20202020202020202,\"y\":1.0},{\"x\":0.20404040404040405,\"y\":1.0},{\"x\":0.20606060606060606,\"y\":1.0},{\"x\":0.2080808080808081,\"y\":1.0},{\"x\":0.2101010101010101,\"y\":1.0},{\"x\":0.21212121212121213,\"y\":1.0},{\"x\":0.21414141414141413,\"y\":1.0},{\"x\":0.21616161616161617,\"y\":1.0},{\"x\":0.21818181818181817,\"y\":1.0},{\"x\":0.2202020202020202,\"y\":1.0},{\"x\":0.2222222222222222,\"y\":1.0},{\"x\":0.22424242424242424,\"y\":1.0},{\"x\":0.22626262626262628,\"y\":1.0},{\"x\":0.22828282828282828,\"y\":1.0},{\"x\":0.23030303030303031,\"y\":1.0},{\"x\":0.23232323232323232,\"y\":1.0},{\"x\":0.23434343434343435,\"y\":1.0},{\"x\":0.23636363636363636,\"y\":1.0},{\"x\":0.2383838383838384,\"y\":1.0},{\"x\":0.2404040404040404,\"y\":1.0},{\"x\":0.24242424242424243,\"y\":1.0},{\"x\":0.24444444444444444,\"y\":1.0},{\"x\":0.24646464646464647,\"y\":1.0},{\"x\":0.24848484848484848,\"y\":1.0},{\"x\":0.2505050505050505,\"y\":1.0},{\"x\":0.25252525252525254,\"y\":1.0},{\"x\":0.2545454545454545,\"y\":1.0},{\"x\":0.25656565656565655,\"y\":1.0},{\"x\":0.2585858585858586,\"y\":1.0},{\"x\":0.2606060606060606,\"y\":1.0},{\"x\":0.26262626262626265,\"y\":1.0},{\"x\":0.26464646464646463,\"y\":1.0},{\"x\":0.26666666666666666,\"y\":1.0},{\"x\":0.2686868686868687,\"y\":1.0},{\"x\":0.27070707070707073,\"y\":1.0},{\"x\":0.2727272727272727,\"y\":1.0},{\"x\":0.27474747474747474,\"y\":1.0},{\"x\":0.2767676767676768,\"y\":1.0},{\"x\":0.2787878787878788,\"y\":1.0},{\"x\":0.2808080808080808,\"y\":1.0},{\"x\":0.2828282828282828,\"y\":1.0},{\"x\":0.28484848484848485,\"y\":1.0},{\"x\":0.2868686868686869,\"y\":1.0},{\"x\":0.28888888888888886,\"y\":1.0},{\"x\":0.2909090909090909,\"y\":1.0},{\"x\":0.29292929292929293,\"y\":1.0},{\"x\":0.29494949494949496,\"y\":1.0},{\"x\":0.296969696969697,\"y\":1.0},{\"x\":0.298989898989899,\"y\":1.0},{\"x\":0.301010101010101,\"y\":1.0},{\"x\":0.30303030303030304,\"y\":1.0},{\"x\":0.30505050505050507,\"y\":1.0},{\"x\":0.30707070707070705,\"y\":1.0},{\"x\":0.3090909090909091,\"y\":1.0},{\"x\":0.3111111111111111,\"y\":1.0},{\"x\":0.31313131313131315,\"y\":1.0},{\"x\":0.3151515151515151,\"y\":1.0},{\"x\":0.31717171717171716,\"y\":1.0},{\"x\":0.3191919191919192,\"y\":1.0},{\"x\":0.3212121212121212,\"y\":1.0},{\"x\":0.32323232323232326,\"y\":1.0},{\"x\":0.32525252525252524,\"y\":1.0},{\"x\":0.32727272727272727,\"y\":1.0},{\"x\":0.3292929292929293,\"y\":1.0},{\"x\":0.33131313131313134,\"y\":1.0},{\"x\":0.3333333333333333,\"y\":1.0},{\"x\":0.33535353535353535,\"y\":1.0},{\"x\":0.3373737373737374,\"y\":1.0},{\"x\":0.3393939393939394,\"y\":1.0},{\"x\":0.3414141414141414,\"y\":1.0},{\"x\":0.3434343434343434,\"y\":1.0},{\"x\":0.34545454545454546,\"y\":1.0},{\"x\":0.3474747474747475,\"y\":1.0},{\"x\":0.34949494949494947,\"y\":1.0},{\"x\":0.3515151515151515,\"y\":1.0},{\"x\":0.35353535353535354,\"y\":1.0},{\"x\":0.35555555555555557,\"y\":1.0},{\"x\":0.3575757575757576,\"y\":1.0},{\"x\":0.3595959595959596,\"y\":1.0},{\"x\":0.3616161616161616,\"y\":1.0},{\"x\":0.36363636363636365,\"y\":1.0},{\"x\":0.3656565656565657,\"y\":1.0},{\"x\":0.36767676767676766,\"y\":1.0},{\"x\":0.3696969696969697,\"y\":1.0},{\"x\":0.3717171717171717,\"y\":1.0},{\"x\":0.37373737373737376,\"y\":1.0},{\"x\":0.37575757575757573,\"y\":1.0},{\"x\":0.37777777777777777,\"y\":1.0},{\"x\":0.3797979797979798,\"y\":1.0},{\"x\":0.38181818181818183,\"y\":1.0},{\"x\":0.3838383838383838,\"y\":1.0},{\"x\":0.38585858585858585,\"y\":1.0},{\"x\":0.3878787878787879,\"y\":1.0},{\"x\":0.3898989898989899,\"y\":1.0},{\"x\":0.39191919191919194,\"y\":1.0},{\"x\":0.3939393939393939,\"y\":1.0},{\"x\":0.39595959595959596,\"y\":1.0},{\"x\":0.397979797979798,\"y\":1.0},{\"x\":0.4,\"y\":1.0},{\"x\":0.402020202020202,\"y\":1.0},{\"x\":0.40404040404040403,\"y\":1.0},{\"x\":0.40606060606060607,\"y\":1.0},{\"x\":0.4080808080808081,\"y\":1.0},{\"x\":0.4101010101010101,\"y\":1.0},{\"x\":0.4121212121212121,\"y\":1.0},{\"x\":0.41414141414141414,\"y\":1.0},{\"x\":0.4161616161616162,\"y\":1.0},{\"x\":0.41818181818181815,\"y\":1.0},{\"x\":0.4202020202020202,\"y\":1.0},{\"x\":0.4222222222222222,\"y\":1.0},{\"x\":0.42424242424242425,\"y\":1.0},{\"x\":0.4262626262626263,\"y\":1.0},{\"x\":0.42828282828282827,\"y\":1.0},{\"x\":0.4303030303030303,\"y\":1.0},{\"x\":0.43232323232323233,\"y\":1.0},{\"x\":0.43434343434343436,\"y\":1.0},{\"x\":0.43636363636363634,\"y\":1.0},{\"x\":0.4383838383838384,\"y\":1.0},{\"x\":0.4404040404040404,\"y\":1.0},{\"x\":0.44242424242424244,\"y\":1.0},{\"x\":0.4444444444444444,\"y\":1.0},{\"x\":0.44646464646464645,\"y\":1.0},{\"x\":0.4484848484848485,\"y\":1.0},{\"x\":0.4505050505050505,\"y\":1.0},{\"x\":0.45252525252525255,\"y\":1.0},{\"x\":0.45454545454545453,\"y\":1.0},{\"x\":0.45656565656565656,\"y\":1.0},{\"x\":0.4585858585858586,\"y\":1.0},{\"x\":0.46060606060606063,\"y\":1.0},{\"x\":0.4626262626262626,\"y\":1.0},{\"x\":0.46464646464646464,\"y\":1.0},{\"x\":0.4666666666666667,\"y\":1.0},{\"x\":0.4686868686868687,\"y\":1.0},{\"x\":0.4707070707070707,\"y\":1.0},{\"x\":0.4727272727272727,\"y\":1.0},{\"x\":0.47474747474747475,\"y\":1.0},{\"x\":0.4767676767676768,\"y\":1.0},{\"x\":0.47878787878787876,\"y\":1.0},{\"x\":0.4808080808080808,\"y\":1.0},{\"x\":0.48282828282828283,\"y\":1.0},{\"x\":0.48484848484848486,\"y\":1.0},{\"x\":0.4868686868686869,\"y\":1.0},{\"x\":0.4888888888888889,\"y\":1.0},{\"x\":0.4909090909090909,\"y\":1.0},{\"x\":0.49292929292929294,\"y\":1.0},{\"x\":0.494949494949495,\"y\":1.0},{\"x\":0.49696969696969695,\"y\":1.0},{\"x\":0.498989898989899,\"y\":1.0},{\"x\":0.501010101010101,\"y\":1.0},{\"x\":0.503030303030303,\"y\":1.0},{\"x\":0.5050505050505051,\"y\":1.0},{\"x\":0.5070707070707071,\"y\":1.0},{\"x\":0.509090909090909,\"y\":1.0},{\"x\":0.5111111111111111,\"y\":1.0},{\"x\":0.5131313131313131,\"y\":1.0},{\"x\":0.5151515151515151,\"y\":1.0},{\"x\":0.5171717171717172,\"y\":1.0},{\"x\":0.5191919191919192,\"y\":1.0},{\"x\":0.5212121212121212,\"y\":1.0},{\"x\":0.5232323232323233,\"y\":1.0},{\"x\":0.5252525252525253,\"y\":1.0},{\"x\":0.5272727272727272,\"y\":1.0},{\"x\":0.5292929292929293,\"y\":1.0},{\"x\":0.5313131313131313,\"y\":1.0},{\"x\":0.5333333333333333,\"y\":1.0},{\"x\":0.5353535353535354,\"y\":1.0},{\"x\":0.5373737373737374,\"y\":1.0},{\"x\":0.5393939393939394,\"y\":1.0},{\"x\":0.5414141414141415,\"y\":1.0},{\"x\":0.5434343434343434,\"y\":1.0},{\"x\":0.5454545454545454,\"y\":1.0},{\"x\":0.5474747474747474,\"y\":1.0},{\"x\":0.5494949494949495,\"y\":1.0},{\"x\":0.5515151515151515,\"y\":1.0},{\"x\":0.5535353535353535,\"y\":1.0},{\"x\":0.5555555555555556,\"y\":1.0},{\"x\":0.5575757575757576,\"y\":1.0},{\"x\":0.5595959595959596,\"y\":1.0},{\"x\":0.5616161616161616,\"y\":1.0},{\"x\":0.5636363636363636,\"y\":1.0},{\"x\":0.5656565656565656,\"y\":1.0},{\"x\":0.5676767676767677,\"y\":1.0},{\"x\":0.5696969696969697,\"y\":1.0},{\"x\":0.5717171717171717,\"y\":1.0},{\"x\":0.5737373737373738,\"y\":1.0},{\"x\":0.5757575757575758,\"y\":1.0},{\"x\":0.5777777777777777,\"y\":1.0},{\"x\":0.5797979797979798,\"y\":1.0},{\"x\":0.5818181818181818,\"y\":1.0},{\"x\":0.5838383838383838,\"y\":1.0},{\"x\":0.5858585858585859,\"y\":1.0},{\"x\":0.5878787878787879,\"y\":1.0},{\"x\":0.5898989898989899,\"y\":1.0},{\"x\":0.591919191919192,\"y\":1.0},{\"x\":0.593939393939394,\"y\":1.0},{\"x\":0.5959595959595959,\"y\":1.0},{\"x\":0.597979797979798,\"y\":1.0},{\"x\":0.6,\"y\":1.0},{\"x\":0.602020202020202,\"y\":1.0},{\"x\":0.604040404040404,\"y\":1.0},{\"x\":0.6060606060606061,\"y\":1.0},{\"x\":0.6080808080808081,\"y\":1.0},{\"x\":0.6101010101010101,\"y\":1.0},{\"x\":0.6121212121212121,\"y\":1.0},{\"x\":0.6141414141414141,\"y\":1.0},{\"x\":0.6161616161616161,\"y\":1.0},{\"x\":0.6181818181818182,\"y\":1.0},{\"x\":0.6202020202020202,\"y\":1.0},{\"x\":0.6222222222222222,\"y\":1.0},{\"x\":0.6242424242424243,\"y\":1.0},{\"x\":0.6262626262626263,\"y\":1.0},{\"x\":0.6282828282828283,\"y\":1.0},{\"x\":0.6303030303030303,\"y\":1.0},{\"x\":0.6323232323232323,\"y\":1.0},{\"x\":0.6343434343434343,\"y\":1.0},{\"x\":0.6363636363636364,\"y\":1.0},{\"x\":0.6383838383838384,\"y\":1.0},{\"x\":0.6404040404040404,\"y\":1.0},{\"x\":0.6424242424242425,\"y\":1.0},{\"x\":0.6444444444444445,\"y\":1.0},{\"x\":0.6464646464646465,\"y\":1.0},{\"x\":0.6484848484848484,\"y\":1.0},{\"x\":0.6505050505050505,\"y\":1.0},{\"x\":0.6525252525252525,\"y\":1.0},{\"x\":0.6545454545454545,\"y\":1.0},{\"x\":0.6565656565656566,\"y\":1.0},{\"x\":0.6585858585858586,\"y\":1.0},{\"x\":0.6606060606060606,\"y\":1.0},{\"x\":0.6626262626262627,\"y\":1.0},{\"x\":0.6646464646464646,\"y\":1.0},{\"x\":0.6666666666666666,\"y\":1.0},{\"x\":0.6686868686868687,\"y\":1.0},{\"x\":0.6707070707070707,\"y\":1.0},{\"x\":0.6727272727272727,\"y\":1.0},{\"x\":0.6747474747474748,\"y\":1.0},{\"x\":0.6767676767676768,\"y\":1.0},{\"x\":0.6787878787878788,\"y\":1.0},{\"x\":0.6808080808080809,\"y\":1.0},{\"x\":0.6828282828282828,\"y\":1.0},{\"x\":0.6848484848484848,\"y\":1.0},{\"x\":0.6868686868686869,\"y\":1.0},{\"x\":0.6888888888888889,\"y\":1.0},{\"x\":0.6909090909090909,\"y\":1.0},{\"x\":0.692929292929293,\"y\":1.0},{\"x\":0.694949494949495,\"y\":1.0},{\"x\":0.696969696969697,\"y\":1.0},{\"x\":0.6989898989898989,\"y\":1.0},{\"x\":0.701010101010101,\"y\":1.0},{\"x\":0.703030303030303,\"y\":1.0},{\"x\":0.705050505050505,\"y\":1.0},{\"x\":0.7070707070707071,\"y\":1.0},{\"x\":0.7090909090909091,\"y\":1.0},{\"x\":0.7111111111111111,\"y\":1.0},{\"x\":0.7131313131313132,\"y\":1.0},{\"x\":0.7151515151515152,\"y\":1.0},{\"x\":0.7171717171717171,\"y\":1.0},{\"x\":0.7191919191919192,\"y\":1.0},{\"x\":0.7212121212121212,\"y\":1.0},{\"x\":0.7232323232323232,\"y\":1.0},{\"x\":0.7252525252525253,\"y\":1.0},{\"x\":0.7272727272727273,\"y\":1.0},{\"x\":0.7292929292929293,\"y\":1.0},{\"x\":0.7313131313131314,\"y\":1.0},{\"x\":0.7333333333333333,\"y\":1.0},{\"x\":0.7353535353535353,\"y\":1.0},{\"x\":0.7373737373737373,\"y\":1.0},{\"x\":0.7393939393939394,\"y\":1.0},{\"x\":0.7414141414141414,\"y\":1.0},{\"x\":0.7434343434343434,\"y\":1.0},{\"x\":0.7454545454545455,\"y\":1.0},{\"x\":0.7474747474747475,\"y\":1.0},{\"x\":0.7494949494949495,\"y\":1.0},{\"x\":0.7515151515151515,\"y\":1.0},{\"x\":0.7535353535353535,\"y\":1.0},{\"x\":0.7555555555555555,\"y\":1.0},{\"x\":0.7575757575757576,\"y\":1.0},{\"x\":0.7595959595959596,\"y\":1.0},{\"x\":0.7616161616161616,\"y\":1.0},{\"x\":0.7636363636363637,\"y\":1.0},{\"x\":0.7656565656565657,\"y\":1.0},{\"x\":0.7676767676767676,\"y\":1.0},{\"x\":0.7696969696969697,\"y\":1.0},{\"x\":0.7717171717171717,\"y\":1.0},{\"x\":0.7737373737373737,\"y\":1.0},{\"x\":0.7757575757575758,\"y\":1.0},{\"x\":0.7777777777777778,\"y\":1.0},{\"x\":0.7797979797979798,\"y\":1.0},{\"x\":0.7818181818181819,\"y\":1.0},{\"x\":0.7838383838383839,\"y\":1.0},{\"x\":0.7858585858585858,\"y\":1.0},{\"x\":0.7878787878787878,\"y\":1.0},{\"x\":0.7898989898989899,\"y\":1.0},{\"x\":0.7919191919191919,\"y\":1.0},{\"x\":0.793939393939394,\"y\":1.0},{\"x\":0.795959595959596,\"y\":1.0},{\"x\":0.797979797979798,\"y\":1.0},{\"x\":0.8,\"y\":1.0},{\"x\":0.802020202020202,\"y\":1.0},{\"x\":0.804040404040404,\"y\":1.0},{\"x\":0.806060606060606,\"y\":1.0},{\"x\":0.8080808080808081,\"y\":1.0},{\"x\":0.8101010101010101,\"y\":1.0},{\"x\":0.8121212121212121,\"y\":1.0},{\"x\":0.8141414141414142,\"y\":1.0},{\"x\":0.8161616161616162,\"y\":1.0},{\"x\":0.8181818181818182,\"y\":1.0},{\"x\":0.8202020202020202,\"y\":1.0},{\"x\":0.8222222222222222,\"y\":1.0},{\"x\":0.8242424242424242,\"y\":1.0},{\"x\":0.8262626262626263,\"y\":1.0},{\"x\":0.8282828282828283,\"y\":1.0},{\"x\":0.8303030303030303,\"y\":1.0},{\"x\":0.8323232323232324,\"y\":1.0},{\"x\":0.8343434343434344,\"y\":1.0},{\"x\":0.8363636363636363,\"y\":1.0},{\"x\":0.8383838383838383,\"y\":1.0},{\"x\":0.8404040404040404,\"y\":1.0},{\"x\":0.8424242424242424,\"y\":1.0},{\"x\":0.8444444444444444,\"y\":1.0},{\"x\":0.8464646464646465,\"y\":1.0},{\"x\":0.8484848484848485,\"y\":1.0},{\"x\":0.8505050505050505,\"y\":1.0},{\"x\":0.8525252525252526,\"y\":1.0},{\"x\":0.8545454545454545,\"y\":1.0},{\"x\":0.8565656565656565,\"y\":1.0},{\"x\":0.8585858585858586,\"y\":1.0},{\"x\":0.8606060606060606,\"y\":1.0},{\"x\":0.8626262626262626,\"y\":1.0},{\"x\":0.8646464646464647,\"y\":1.0},{\"x\":0.8666666666666667,\"y\":1.0},{\"x\":0.8686868686868687,\"y\":1.0},{\"x\":0.8707070707070707,\"y\":1.0},{\"x\":0.8727272727272727,\"y\":1.0},{\"x\":0.8747474747474747,\"y\":1.0},{\"x\":0.8767676767676768,\"y\":1.0},{\"x\":0.8787878787878788,\"y\":1.0},{\"x\":0.8808080808080808,\"y\":1.0},{\"x\":0.8828282828282829,\"y\":1.0},{\"x\":0.8848484848484849,\"y\":1.0},{\"x\":0.8868686868686869,\"y\":1.0},{\"x\":0.8888888888888888,\"y\":1.0},{\"x\":0.8909090909090909,\"y\":1.0},{\"x\":0.8929292929292929,\"y\":1.0},{\"x\":0.8949494949494949,\"y\":1.0},{\"x\":0.896969696969697,\"y\":1.0},{\"x\":0.898989898989899,\"y\":1.0},{\"x\":0.901010101010101,\"y\":1.0},{\"x\":0.9030303030303031,\"y\":1.0},{\"x\":0.9050505050505051,\"y\":1.0},{\"x\":0.907070707070707,\"y\":1.0},{\"x\":0.9090909090909091,\"y\":1.0},{\"x\":0.9111111111111111,\"y\":1.0},{\"x\":0.9131313131313131,\"y\":1.0},{\"x\":0.9151515151515152,\"y\":1.0},{\"x\":0.9171717171717172,\"y\":1.0},{\"x\":0.9191919191919192,\"y\":1.0},{\"x\":0.9212121212121213,\"y\":1.0},{\"x\":0.9232323232323232,\"y\":1.0},{\"x\":0.9252525252525252,\"y\":1.0},{\"x\":0.9272727272727272,\"y\":1.0},{\"x\":0.9292929292929293,\"y\":1.0},{\"x\":0.9313131313131313,\"y\":1.0},{\"x\":0.9333333333333333,\"y\":1.0},{\"x\":0.9353535353535354,\"y\":1.0},{\"x\":0.9373737373737374,\"y\":1.0},{\"x\":0.9393939393939394,\"y\":1.0},{\"x\":0.9414141414141414,\"y\":1.0},{\"x\":0.9434343434343434,\"y\":1.0},{\"x\":0.9454545454545454,\"y\":1.0},{\"x\":0.9474747474747475,\"y\":1.0},{\"x\":0.9494949494949495,\"y\":1.0},{\"x\":0.9515151515151515,\"y\":1.0},{\"x\":0.9535353535353536,\"y\":1.0},{\"x\":0.9555555555555556,\"y\":1.0},{\"x\":0.9575757575757575,\"y\":1.0},{\"x\":0.9595959595959596,\"y\":1.0},{\"x\":0.9616161616161616,\"y\":1.0},{\"x\":0.9636363636363636,\"y\":1.0},{\"x\":0.9656565656565657,\"y\":1.0},{\"x\":0.9676767676767677,\"y\":1.0},{\"x\":0.9696969696969697,\"y\":1.0},{\"x\":0.9717171717171718,\"y\":1.0},{\"x\":0.9737373737373738,\"y\":1.0},{\"x\":0.9757575757575757,\"y\":1.0},{\"x\":0.9777777777777777,\"y\":1.0},{\"x\":0.9797979797979798,\"y\":1.0},{\"x\":0.9818181818181818,\"y\":1.0},{\"x\":0.9838383838383838,\"y\":1.0},{\"x\":0.9858585858585859,\"y\":1.0},{\"x\":0.9878787878787879,\"y\":1.0},{\"x\":0.98989898989899,\"y\":1.0},{\"x\":0.9919191919191919,\"y\":1.0},{\"x\":0.9939393939393939,\"y\":1.0},{\"x\":0.9959595959595959,\"y\":1.0},{\"x\":0.997979797979798,\"y\":1.0},{\"x\":1.0,\"y\":1.0}]},\"extension\":[]}\n",
       "        var id_name = '#vis-f3346d82-b1d3-43b4-a463-49e5fab0c455';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x0000564350d1d550 @properties={:diagrams=>[#<Nyaplot::Diagram:0x00005643513f5d28 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"6ff06019-96d1-4706-84d9-a84cd62b6497\"}, @xrange=[0.0, 1.0], @yrange=[0.0, 1.0]>], :options=>{:x_label=>\"X\", :y_label=>\"Y\", :zoom=>true, :width=>700, :xrange=>[0.0, 1.0], :yrange=>[0.0, 1.0]}}>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_13718d()\n",
    "  dataset = two_gaussians_dataset()  \n",
    "  train_data = dataset.clone\n",
    "  train_data[\"data\"] = dataset[\"data\"][0,5]\n",
    "\n",
    "  test_data = dataset.clone\n",
    "  test_data[\"data\"] = dataset[\"data\"][5,995]\n",
    "  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new\n",
    "  svm_learner.train train_data\n",
    "  \n",
    "  linear_model = svm_learner.model\n",
    "  puts \"Linear model has #{linear_model[\"data\"].size} support vectors\"\n",
    "  puts linear_model\n",
    "  scores = svm_learner.evaluate test_data  \n",
    "  metric = AUCMetric.new\n",
    "  fpr, tpr, auc = metric.roc_curve scores\n",
    "  \n",
    "  puts \"AUC on test set is #{auc}\"\n",
    "  assert_true(auc > 0.99, \"AUC should be > 0.99\")\n",
    "  plot fpr, tpr\n",
    "end\n",
    "test_13718d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c90808df25ecf416901cd0ce8b342c9f",
     "grade": false,
     "grade_id": "cell-8e14f2b18a7f2a17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see how the classifier does on a nonlinear dataset.\n",
    "\n",
    "![circle_dataset.png](./circle_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abc2e89d5ca6483805f55761d332e78c",
     "grade": true,
     "grade_id": "cell-9d618ae467664dbe",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model has 435 support vectors\n",
      "AUC on test set is 0.469785575048733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-c853731a-027e-4544-903b-1bf37d944cbe'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"be5e71e9-db89-4745-a587-d3b2e00df85e\"}],\"options\":{\"x_label\":\"X\",\"y_label\":\"Y\",\"zoom\":true,\"width\":700,\"xrange\":[0.0,1.0],\"yrange\":[0.0,1.0]}}],\"data\":{\"be5e71e9-db89-4745-a587-d3b2e00df85e\":[{\"x\":0.0,\"y\":0.0},{\"x\":0.012345679012345678,\"y\":0.0},{\"x\":0.024691358024691357,\"y\":0.0},{\"x\":0.037037037037037035,\"y\":0.0},{\"x\":0.04938271604938271,\"y\":0.0},{\"x\":0.06172839506172839,\"y\":0.0},{\"x\":0.07407407407407407,\"y\":0.0},{\"x\":0.08641975308641975,\"y\":0.0},{\"x\":0.09876543209876543,\"y\":0.0},{\"x\":0.1111111111111111,\"y\":0.0},{\"x\":0.12345679012345678,\"y\":0.0},{\"x\":0.13580246913580246,\"y\":0.0},{\"x\":0.14814814814814814,\"y\":0.0},{\"x\":0.16049382716049382,\"y\":0.0},{\"x\":0.1728395061728395,\"y\":0.0},{\"x\":0.18518518518518517,\"y\":0.0},{\"x\":0.19753086419753085,\"y\":0.0},{\"x\":0.20987654320987653,\"y\":0.0},{\"x\":0.2222222222222222,\"y\":0.0},{\"x\":0.2345679012345679,\"y\":0.0},{\"x\":0.24691358024691357,\"y\":0.0},{\"x\":0.25925925925925924,\"y\":0.0},{\"x\":0.2716049382716049,\"y\":0.0},{\"x\":0.2839506172839506,\"y\":0.0},{\"x\":0.2962962962962963,\"y\":0.0},{\"x\":0.30864197530864196,\"y\":0.0},{\"x\":0.32098765432098764,\"y\":0.0},{\"x\":0.3333333333333333,\"y\":0.0},{\"x\":0.345679012345679,\"y\":0.0},{\"x\":0.35802469135802467,\"y\":0.0},{\"x\":0.37037037037037035,\"y\":0.0},{\"x\":0.38271604938271603,\"y\":0.0},{\"x\":0.3950617283950617,\"y\":0.0},{\"x\":0.4074074074074074,\"y\":0.0},{\"x\":0.41975308641975306,\"y\":0.0},{\"x\":0.41975308641975306,\"y\":0.05263157894736842},{\"x\":0.41975308641975306,\"y\":0.10526315789473684},{\"x\":0.43209876543209874,\"y\":0.10526315789473684},{\"x\":0.43209876543209874,\"y\":0.15789473684210525},{\"x\":0.4444444444444444,\"y\":0.15789473684210525},{\"x\":0.4567901234567901,\"y\":0.15789473684210525},{\"x\":0.4691358024691358,\"y\":0.15789473684210525},{\"x\":0.48148148148148145,\"y\":0.15789473684210525},{\"x\":0.49382716049382713,\"y\":0.15789473684210525},{\"x\":0.49382716049382713,\"y\":0.21052631578947367},{\"x\":0.49382716049382713,\"y\":0.2631578947368421},{\"x\":0.49382716049382713,\"y\":0.3157894736842105},{\"x\":0.5061728395061729,\"y\":0.3157894736842105},{\"x\":0.5061728395061729,\"y\":0.3684210526315789},{\"x\":0.5185185185185185,\"y\":0.3684210526315789},{\"x\":0.5185185185185185,\"y\":0.42105263157894735},{\"x\":0.5308641975308642,\"y\":0.42105263157894735},{\"x\":0.5308641975308642,\"y\":0.47368421052631576},{\"x\":0.5308641975308642,\"y\":0.5263157894736842},{\"x\":0.5308641975308642,\"y\":0.5789473684210527},{\"x\":0.5308641975308642,\"y\":0.631578947368421},{\"x\":0.5432098765432098,\"y\":0.631578947368421},{\"x\":0.5555555555555556,\"y\":0.631578947368421},{\"x\":0.5679012345679012,\"y\":0.631578947368421},{\"x\":0.5679012345679012,\"y\":0.6842105263157895},{\"x\":0.5802469135802469,\"y\":0.6842105263157895},{\"x\":0.5925925925925926,\"y\":0.6842105263157895},{\"x\":0.5925925925925926,\"y\":0.7368421052631579},{\"x\":0.5925925925925926,\"y\":0.7894736842105263},{\"x\":0.5925925925925926,\"y\":0.8421052631578947},{\"x\":0.5925925925925926,\"y\":0.8947368421052632},{\"x\":0.5925925925925926,\"y\":0.9473684210526315},{\"x\":0.6049382716049383,\"y\":0.9473684210526315},{\"x\":0.6172839506172839,\"y\":0.9473684210526315},{\"x\":0.6296296296296297,\"y\":0.9473684210526315},{\"x\":0.6419753086419753,\"y\":0.9473684210526315},{\"x\":0.6419753086419753,\"y\":1.0},{\"x\":0.654320987654321,\"y\":1.0},{\"x\":0.6666666666666666,\"y\":1.0},{\"x\":0.6790123456790124,\"y\":1.0},{\"x\":0.691358024691358,\"y\":1.0},{\"x\":0.7037037037037037,\"y\":1.0},{\"x\":0.7160493827160493,\"y\":1.0},{\"x\":0.7283950617283951,\"y\":1.0},{\"x\":0.7407407407407407,\"y\":1.0},{\"x\":0.7530864197530864,\"y\":1.0},{\"x\":0.7654320987654321,\"y\":1.0},{\"x\":0.7777777777777778,\"y\":1.0},{\"x\":0.7901234567901234,\"y\":1.0},{\"x\":0.8024691358024691,\"y\":1.0},{\"x\":0.8148148148148148,\"y\":1.0},{\"x\":0.8271604938271605,\"y\":1.0},{\"x\":0.8395061728395061,\"y\":1.0},{\"x\":0.8518518518518519,\"y\":1.0},{\"x\":0.8641975308641975,\"y\":1.0},{\"x\":0.8765432098765432,\"y\":1.0},{\"x\":0.8888888888888888,\"y\":1.0},{\"x\":0.9012345679012346,\"y\":1.0},{\"x\":0.9135802469135802,\"y\":1.0},{\"x\":0.9259259259259259,\"y\":1.0},{\"x\":0.9382716049382716,\"y\":1.0},{\"x\":0.9506172839506173,\"y\":1.0},{\"x\":0.9629629629629629,\"y\":1.0},{\"x\":0.9753086419753086,\"y\":1.0},{\"x\":0.9876543209876543,\"y\":1.0},{\"x\":1.0,\"y\":1.0}]},\"extension\":[]}\n",
       "        var id_name = '#vis-c853731a-027e-4544-903b-1bf37d944cbe';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x00005643503fc0e8 @properties={:diagrams=>[#<Nyaplot::Diagram:0x000056434fe4cda0 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"be5e71e9-db89-4745-a587-d3b2e00df85e\"}, @xrange=[0.0, 1.0], @yrange=[0.0, 1.0]>], :options=>{:x_label=>\"X\", :y_label=>\"Y\", :zoom=>true, :width=>700, :xrange=>[0.0, 1.0], :yrange=>[0.0, 1.0]}}>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_9d618a()\n",
    "  dataset = circle_dataset()  \n",
    "  train_data = dataset.clone\n",
    "  train_data[\"data\"] = dataset[\"data\"][0,900]\n",
    "\n",
    "  test_data = dataset.clone\n",
    "  test_data[\"data\"] = dataset[\"data\"][900,100]\n",
    "  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new\n",
    "  svm_learner.train train_data\n",
    "  \n",
    "  linear_model = svm_learner.model\n",
    "  puts \"Linear model has #{linear_model[\"data\"].size} support vectors\"\n",
    "  \n",
    "  scores = svm_learner.evaluate test_data  \n",
    "  metric = AUCMetric.new\n",
    "  fpr, tpr, auc = metric.roc_curve scores\n",
    "  \n",
    "  puts \"AUC on test set is #{auc}\"\n",
    "  assert_true(auc < 0.5, \"AUC should be < 0.5\")\n",
    "  plot fpr, tpr\n",
    "end\n",
    "test_9d618a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of the linear decision boundary. If this were a good classifier, the positive examples would be brighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fb78b85d599fc25142e126bd6197af9",
     "grade": true,
     "grade_id": "cell-3655e41da98e76aa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "#<GnuplotRB::Splot:0x00005643503ecd50 @options=Hamster::Hash[:term => \"png\", :key => false, :palette => \"rgb 34,35,36\", :yrange => 0..1, :xrange => 0..1, :style => \"data lines\"], @datasets=Hamster::Vector[#<GnuplotRB::Dataset:0x00005643503ec468 @type=:datablock, @data=#<GnuplotRB::Datablock:0x00005643503ec288 @stored_in_file=false, @data=\"0.0 0.0 0 -1.0000026463145693\\n0.0 0.1 0 -1.00000278940193\\n0.0 0.2 0 -1.000002932489291\\n0.0 0.30000000000000004 0 -1.0000030755766818\\n0.0 0.4 0 -1.0000032186640126\\n0.0 0.5 0 -1.0000033617513755\\n0.0 0.6000000000000001 0 -1.0000035048387945\\n0.0 0.7000000000000001 0 -1.0000036479261598\\n0.0 0.8 0 -1.000003791013456\\n0.0 0.9 0 -1.0000039341009086\\n0.0 1.0 0 -1.0000040771881815\\n0.1 0.0 0 -1.000001795694287\\n0.1 0.1 0 -1.0000019387816637\\n0.1 0.2 0 -1.000002081869031\\n0.1 0.30000000000000004 0 -1.0000022249564213\\n0.1 0.4 0 -1.000002368043771\\n0.1 0.5 0 -1.0000025111311266\\n0.1 0.6000000000000001 0 -1.0000026542185143\\n0.1 0.7000000000000001 0 -1.000002797305829\\n0.1 0.8 0 -1.0000029403931774\\n0.1 0.9 0 -1.0000030834806064\\n0.1 1.0 0 -1.0000032265680046\\n0.2 0.0 0 -1.0000009450740046\\n0.2 0.1 0 -1.0000010881613715\\n0.2 0.2 0 -1.000001231248758\\n0.2 0.30000000000000004 0 -1.0000013743361031\\n0.2 0.4 0 -1.0000015174234929\\n0.2 0.5 0 -1.000001660510811\\n0.2 0.6000000000000001 0 -1.0000018035982732\\n0.2 0.7000000000000001 0 -1.0000019466855405\\n0.2 0.8 0 -1.000002089772973\\n0.2 0.9 0 -1.0000022328602525\\n0.2 1.0 0 -1.000002375947684\\n0.30000000000000004 0.0 0 -1.0000000944537235\\n0.30000000000000004 0.1 0 -1.0000002375410866\\n0.30000000000000004 0.2 0 -1.0000003806284758\\n0.30000000000000004 0.30000000000000004 0 -1.000000523715828\\n0.30000000000000004 0.4 0 -1.0000006668031767\\n0.30000000000000004 0.5 0 -1.0000008098905142\\n0.30000000000000004 0.6000000000000001 0 -1.0000009529779001\\n0.30000000000000004 0.7000000000000001 0 -1.0000010960652852\\n0.30000000000000004 0.8 0 -1.000001239152684\\n0.30000000000000004 0.9 0 -1.000001382239998\\n0.30000000000000004 1.0 0 -1.0000015253273802\\n0.4 0.0 0 -0.99999924383344\\n0.4 0.1 0 -0.999999386920831\\n0.4 0.2 0 -0.9999995300081737\\n0.4 0.30000000000000004 0 -0.9999996730955792\\n0.4 0.4 0 -0.9999998161829469\\n0.4 0.5 0 -0.9999999592703037\\n0.4 0.6000000000000001 0 -1.0000001023576373\\n0.4 0.7000000000000001 0 -1.0000002454449577\\n0.4 0.8 0 -1.0000003885324165\\n0.4 0.9 0 -1.0000005316198213\\n0.4 1.0 0 -1.000000674707053\\n0.5 0.0 0 -0.9999983932131229\\n0.5 0.1 0 -0.9999985363005015\\n0.5 0.2 0 -0.9999986793878564\\n0.5 0.30000000000000004 0 -0.9999988224751555\\n0.5 0.4 0 -0.9999989655626481\\n0.5 0.5 0 -0.9999991086500379\\n0.5 0.6000000000000001 0 -0.9999992517372631\\n0.5 0.7000000000000001 0 -0.9999993948247486\\n0.5 0.8 0 -0.9999995379120772\\n0.5 0.9 0 -0.9999996809995231\\n0.5 1.0 0 -0.9999998240869911\\n0.6000000000000001 0.0 0 -0.9999975425928775\\n0.6000000000000001 0.1 0 -0.9999976856802111\\n0.6000000000000001 0.2 0 -0.9999978287676038\\n0.6000000000000001 0.30000000000000004 0 -0.9999979718549782\\n0.6000000000000001 0.4 0 -0.9999981149423822\\n0.6000000000000001 0.5 0 -0.9999982580297205\\n0.6000000000000001 0.6000000000000001 0 -0.9999984011170869\\n0.6000000000000001 0.7000000000000001 0 -0.9999985442043747\\n0.6000000000000001 0.8 0 -0.9999986872917841\\n0.6000000000000001 0.9 0 -0.9999988303792888\\n0.6000000000000001 1.0 0 -0.9999989734664594\\n0.7000000000000001 0.0 0 -0.9999966919725952\\n0.7000000000000001 0.1 0 -0.9999968350599777\\n0.7000000000000001 0.2 0 -0.9999969781473104\\n0.7000000000000001 0.30000000000000004 0 -0.9999971212346251\\n0.7000000000000001 0.4 0 -0.9999972643220056\\n0.7000000000000001 0.5 0 -0.9999974074095317\\n0.7000000000000001 0.6000000000000001 0 -0.9999975504967729\\n0.7000000000000001 0.7000000000000001 0 -0.9999976935841658\\n0.7000000000000001 0.8 0 -0.9999978366716206\\n0.7000000000000001 0.9 0 -0.9999979797590279\\n0.7000000000000001 1.0 0 -0.9999981228462014\\n0.8 0.0 0 -0.9999958413523107\\n0.8 0.1 0 -0.999995984439546\\n0.8 0.2 0 -0.9999961275270928\\n0.8 0.30000000000000004 0 -0.9999962706143712\\n0.8 0.4 0 -0.9999964137017779\\n0.8 0.5 0 -0.9999965567891603\\n0.8 0.6000000000000001 0 -0.9999966998765892\\n0.8 0.7000000000000001 0 -0.9999968429640281\\n0.8 0.8 0 -0.9999969860513244\\n0.8 0.9 0 -0.9999971291383998\\n0.8 1.0 0 -0.9999972722260382\\n0.9 0.0 0 -0.9999949907320509\\n0.9 0.1 0 -0.9999951338193721\\n0.9 0.2 0 -0.9999952769065609\\n0.9 0.30000000000000004 0 -0.9999954199941399\\n0.9 0.4 0 -0.9999955630814858\\n0.9 0.5 0 -0.9999957061688833\\n0.9 0.6000000000000001 0 -0.9999958492562833\\n0.9 0.7000000000000001 0 -0.9999959923436124\\n0.9 0.8 0 -0.9999961354312104\\n0.9 0.9 0 -0.9999962785182628\\n0.9 1.0 0 -0.9999964216056381\\n1.0 0.0 0 -0.9999941401116764\\n1.0 0.1 0 -0.9999942831990829\\n1.0 0.2 0 -0.9999944262864336\\n1.0 0.30000000000000004 0 -0.9999945693739284\\n1.0 0.4 0 -0.9999947124611435\\n1.0 0.5 0 -0.9999948555485481\\n1.0 0.6000000000000001 0 -0.9999949986357417\\n1.0 0.7000000000000001 0 -0.999995141723176\\n1.0 0.8 0 -0.999995284810727\\n1.0 0.9 0 -0.9999954278979786\\n1.0 1.0 0 -0.9999955709855066\">, @options=Hamster::Hash[:using => \"1:2:3:4\", :with => \"image\"]>, #<GnuplotRB::Dataset:0x00005643500d2b10 @type=:datablock, @data=#<GnuplotRB::Datablock:0x00005643500d2890 @stored_in_file=false, @data=\"0.9218440427570344 0.6528144466877661 0\\n0.9505665275752514 0.39477630375342654 0\\n0.2085951352913018 0.515909535904487 0\\n0.17733911362973376 0.7991860095162554 0\\n0.8591698256344152 0.9862978883435362 0\\n0.7594646957604924 0.7267611545711008 0\\n0.12266255673819748 0.9332501681271399 0\\n0.15480095358638535 0.2421702375387068 0\\n0.4376583047255461 0.9852005760926856 0\\n0.08865178952488362 0.46888893577845503 0\\n0.9085896680314479 0.62571367967711 0\\n0.7021462064480223 0.9877178988491825 0\\n0.9652681483773536 0.5047445976899271 0\\n0.7634704518313434 0.16557927644043768 0\\n0.5560183393705129 0.09031193129246695 0\\n0.16309146930896745 0.62770118628935 0\\n0.8864034783299267 0.10675910752549045 0\\n0.7808284519953917 0.21771911421161472 0\\n0.7040050235428325 0.8742995639757579 0\\n0.7706771605007622 0.7414904090201708 0\\n0.33123103174567314 0.874115752426034 0\\n0.8087500152401772 0.16197067697178347 0\\n0.5082753677161874 0.13952873972062352 0\\n0.14462726403145953 0.3647199008034221 0\\n0.09769982938093069 0.9547990706770428 0\\n0.18107164444132007 0.7886502798778368 0\\n0.28563762300249607 0.9637034575232788 0\\n0.4436839368843244 0.03416722838818553 0\\n0.012966946811569069 0.5153776473214018 0\\n0.878317702055916 0.5827654313192763 0\\n0.6859789348531043 0.8228424824670659 0\\n0.15219902969979882 0.9756560074496742 0\\n0.9899900761011932 0.5196179659720757 0\\n0.051656884769465394 0.9405985134154844 0\\n0.7015532617265223 0.07737291023694393 0\\n0.12378554093396688 0.9308825319381725 0\\n0.18943036721679796 0.727305485428207 0\\n0.40949598061367865 0.8798306558221476 0\\n0.07487048290631948 0.1247274615049414 0\\n0.8667766778273511 0.2109474074817489 0\\n0.17316229832081798 0.9642716202994865 0\\n0.06606077830678247 0.7618597336566321 0\\n0.3961768121881263 0.8202353569426146 0\\n0.935866737884648 0.6065867987814078 0\\n0.8613574722455065 0.5366168519475264 0\\n0.6692750237109518 0.9017149276259588 0\\n0.7692670151497313 0.3456212913817124 0\\n0.01978471638435564 0.9976292022945171 0\\n0.35003809324428625 0.9970970557430892 0\\n0.3208678698242097 0.7301729589082486 0\\n0.04059341531327754 0.5933470332974888 0\\n0.738323356046209 0.18201821118985195 0\\n0.028940674653574194 0.09476240736403552 0\\n0.0405518988872815 0.7342809864007457 0\\n0.9510842048737523 0.5975985641459411 0\\n0.11921335061505778 0.305067064859441 0\\n0.07098244584545821 0.8837842546715653 0\\n0.5519908731202199 0.8865296766373213 0\\n0.7147921553940219 0.7708242587450973 0\\n0.9128621496458227 0.35919726683162023 0\\n0.8197725211016891 0.5509235134342155 0\\n0.9139456932152354 0.0006945006391451125 0\\n0.21453046716368862 0.7474938745832822 0\\n0.04219008760389875 0.8651797571816602 0\\n0.3936635164375313 0.11999930806399495 0\\n0.005157218797247509 0.737113622433492 0\\n0.7575169629671565 0.8452627139229841 0\\n0.0017374484413756885 0.4714046380579108 0\\n0.2606862059052346 0.3467143684093681 0\\n0.7688043762205244 0.9212941468497508 0\\n0.7698033740765035 0.23959732651836274 0\\n0.9298510505948591 0.7815181755576289 0\\n0.820372704274428 0.40416257928868327 0\\n0.6539816147354206 0.9041490342328254 0\\n0.7371299560703425 0.8523299599583557 0\\n0.49185112220952876 0.8353233549654683 0\\n0.6167019023992019 0.8726995317606463 0\\n0.4861888220774204 0.8239301578026762 0\\n0.2669382453582373 0.23113867311730418 0\\n0.5829565622161805 0.012880756531671422 0\\n0.7958142672895511 0.3696183699914919 0\\n0.07417279074535821 0.8938663117254498 0\\n0.13022958977400112 0.015639074966103728 0\\n0.7662425894066718 0.258818081460245 0\\n0.3274912327505921 0.023689838246493733 0\\n0.042337253215986026 0.053092846370460633 0\\n0.9119086094838325 0.8876872543314881 0\\n0.33184756987860553 0.14012784716403004 0\\n0.17182223182389267 0.11346939102150488 0\\n0.7134859038050001 0.9744954492696554 0\\n0.997058696474505 0.35461947130372096 0\\n0.9202772661864588 0.23941153677879157 0\\n0.7111626352420624 0.754971254305981 0\\n0.7999703978453737 0.7782574670513857 0\\n0.1902394448073087 0.09380014183621621 0\\n0.6824146997268179 0.2331777311992561 0\\n0.6997066015818455 0.19350813181085724 0\\n0.19032866491648892 0.04228407180708904 0\\n0.7573804676967401 0.7610126315976227 0\\n0.13160988264106277 0.27427337236082083 0\\n0.6858721986122879 0.3602065659087602 0\\n0.9264877456852105 0.22183421218889565 0\\n0.40222734330413434 0.12207478833662311 0\\n0.058362716569605566 0.5390590730945028 0\\n0.8418520591368907 0.1282918021143198 0\\n0.27588537710360606 0.04826784931903594 0\\n0.20238581345700568 0.7418421855223264 0\\n0.23607133232332933 0.060627428977884645 0\\n0.7754769893010621 0.1680526219391174 0\\n0.2811018899008054 0.8697822949567738 0\\n0.23979323329002855 0.40722046975422854 0\\n0.02088250813067105 0.3504595774268152 0\\n0.810760046849563 0.9732521897150325 0\\n0.08226261136928037 0.9932425023063437 0\\n0.5069103620175278 0.947730049776127 0\\n0.9052679631347829 0.8535759703688685 0\\n0.9926869462072306 0.7944857020657461 0\\n0.17145748594893095 0.2570897584973654 0\\n0.2753776101962033 0.22580988453425543 0\\n0.17897771250243677 0.7883339048141889 0\\n0.6631663790230676 0.10715954464947142 0\\n0.6618540992890463 0.8409935962235936 0\\n0.9337637416081266 0.33227418619366633 0\\n0.4159384459595665 0.8475441184977365 0\\n0.5714220795409687 0.09830892851820416 0\\n0.128239048184846 0.29838726616571853 0\\n0.8059504353641362 0.5035909052529761 0\\n0.6789024459810129 0.7977678102711303 0\\n0.7224782448805794 0.3216795143794562 0\\n0.13124828960326618 0.5096280810774118 0\\n0.40596781798476234 0.10560133836835761 0\\n0.6970146341111784 0.24783589712241416 0\\n0.6123162578250378 0.17446842239453952 0\\n0.8163627198206503 0.04278306008601873 0\\n0.014458388646930609 0.8958533452033824 0\\n0.7812231893832453 0.3942533695786241 0\\n0.595252204189655 0.08169236191486273 0\\n0.7361407507289199 0.9489987720154156 0\\n0.8391437810809922 0.3961011143551182 0\\n0.44975353761782944 0.8410463748791746 0\\n0.11793859062986012 0.7954638483890694 0\\n0.14228746738213294 0.140802220192625 0\\n0.9177434742883037 0.9149154273738901 0\\n0.47290252538102695 0.06772380920819099 0\\n0.09017767529348042 0.5565927160805839 0\\n0.25255282803631207 0.05516063238893698 0\\n0.46368736847822745 0.9782841116600227 0\\n0.7918431893043548 0.9325561857252727 0\\n0.17822416668830487 0.875403206955042 0\\n0.853603865200085 0.658281127600624 0\\n0.3845281078157793 0.8782681476958349 0\\n0.027635933135939372 0.31413090821778944 0\\n0.23226483459554925 0.3396894926245737 0\\n0.6075945394159372 0.8629730454053022 0\\n0.8881955771786387 0.6302547271467814 0\\n0.20347315565160295 0.7469783588687209 0\\n0.7349285183057371 0.6239561077820841 0\\n0.14087901921896073 0.6204018890368099 0\\n0.7939002674855566 0.1829310323906692 0\\n0.11437196172802666 0.6480012079684161 0\\n0.8197945574342883 0.3867561277580076 0\\n0.9808518740439306 0.3147298834934842 0\\n0.9437500628871334 0.5835157193057041 0\\n0.8198051571430275 0.9896438505862156 0\\n0.028424998538348234 0.8464984411882482 0\\n0.674133948753287 0.6917389448896477 0\\n0.6692639407027849 0.06435562028719566 0\\n0.48716704351615914 0.9852727890819238 0\\n0.8487792603319153 0.5617593152890773 0\\n0.18819472446745011 0.2802967064768995 0\\n0.9068386055778337 0.48016416632880854 0\\n0.7577131765157051 0.6070567081126107 0\\n0.2796567635979891 0.1125941685286862 0\\n0.7739320346984717 0.5563119218701407 0\\n0.026988674636565402 0.1518866239663762 0\\n0.16488960615278014 0.44484651328269065 0\\n0.5032715433604753 0.15676407876229093 0\\n0.6889377530299244 0.30327810016974766 0\\n0.49145042667916905 0.056090420618465586 0\\n0.7323263531840157 0.4749304984502628 0\\n0.7853813663073851 0.2712762749890312 0\\n0.11933321411318942 0.7673469516130216 0\\n0.8911814030942583 0.31774071923548186 0\\n0.8356677350947356 0.8002613478178562 0\\n0.3873399223130156 0.7846243330199292 0\\n0.789261207811393 0.0002303004834521838 0\\n0.3154619007254321 0.7801005981508371 0\\n0.07178798811297238 0.08063614818515852 0\\n0.05632574332546958 0.9971820475611244 0\\n0.13160066357951172 0.2070480277683071 0\\n0.4369662779488177 0.9088551109885887 0\\n0.8075668681778353 0.7869326372396817 0\\n0.4955192113277822 0.15712715718562043 0\\n0.11700265271831112 0.6842207207813382 0\\n0.3449489385870339 0.23106009396940996 0\\n0.05571068153382641 0.5200233067638346 0\\n0.9795176672150321 0.7867332484946326 0\\n0.30175901968844754 0.16499850417022732 0\\n0.36932299075093644 0.8860973669217094 0\\n0.08294601174502869 0.6751865321730344 0\\n0.8746146281041364 0.5448895668478961 0\\n0.15706513719148696 0.644055584933476 0\\n0.9986848417553095 0.5429499770815911 0\\n0.01833188850473677 0.6185880389674258 0\\n0.6067007640830695 0.9112200407715516 0\\n0.3500471781774881 0.8171489601481414 0\\n0.7195964466657725 0.15194521855173126 0\\n0.4143976110728821 0.7233101028812977 0\\n0.8368960266233947 0.4677154103822846 0\\n0.9035152754121738 0.14620836300276063 0\\n0.984595343296979 0.41873553912808703 0\\n0.28589214428375465 0.9676996174809621 0\\n0.10308430664021073 0.35441301434719596 0\\n0.8458300594271149 0.5356041363215994 0\\n0.035321843019758825 0.3432868055279997 0\\n0.09403140906077756 0.20163706276623927 0\\n0.2117175115086246 0.8932104225382119 0\\n0.0899887502196891 0.4280425919641996 0\\n0.12392442524655767 0.960055127761895 0\\n0.04217463172247837 0.9022151819774503 0\\n0.6704522121987448 0.9943172086491466 0\\n0.19964023693809418 0.22043913316059416 0\\n0.34046150202194314 0.9578881527883587 0\\n0.8518980748907229 0.8090120847336989 0\\n0.7104373621134852 0.8214775614930783 0\\n0.6244893378251132 0.7121403028427051 0\\n0.6631208853615556 0.08007396723364235 0\\n0.22917957059115945 0.02134104378189361 0\\n0.3796963686843129 0.02240650500470387 0\\n0.1662281131962663 0.6353166522895253 0\\n0.5858575654477751 0.734209253127071 0\\n0.1576115725184909 0.3542840886554862 0\\n0.22820552910077152 0.26096663395639697 0\\n0.30555444922338026 0.8782589734940257 0\\n0.6964203498985394 0.90163211458793 0\\n0.5720292958249819 0.7846690747994846 0\\n0.7678149142064409 0.13075011702373507 0\\n0.2907669844061791 0.8030106638029032 0\\n0.6937761554298243 0.9403490579321332 0\\n0.9754588374095227 0.1709623320457485 0\\n0.8178359918838809 0.5318225079043042 0\\n0.287330370124118 0.6471022739308747 0\\n0.6713230591524074 0.2786429050458483 0\\n0.31837543777874466 0.8206453135423509 0\\n0.6557553575692325 0.06882336771172415 0\\n0.6301321651910686 0.245815207815543 0\\n0.8554204893075589 0.10961728059038145 0\\n0.8489729508081042 0.6381000174963667 0\\n0.8944518132616763 0.10850863876398631 0\\n0.134516998661104 0.8262797090203741 0\\n0.8481979793131252 0.8397755471014622 0\\n0.418226571937824 0.9500171977215427 0\\n0.8302912423195248 0.6119775590938924 0\\n0.44460615471750364 0.997020748799648 0\\n0.9900835747974304 0.11727626221482734 0\\n0.1603137661184152 0.9410895522548991 0\\n0.9383809672027363 0.7733301444344797 0\\n0.07812038621476225 0.08696203404735159 0\\n0.9633045095346335 0.15574025878453523 0\\n0.7023971723976666 0.7874476791984176 0\\n0.05340041185725275 0.9611094972955609 0\\n0.9901776155579596 0.5816360041085954 0\\n0.07658711730078893 0.13498523520808203 0\\n0.9062693820963073 0.34692043164684705 0\\n0.9787006913853554 0.7397301806224648 0\\n0.4223756853031171 0.8117961179239146 0\\n0.18343251335361954 0.9128530773755765 0\\n0.47202451332794426 0.06081508269149871 0\\n0.29211913393634903 0.6526426943774897 0\\n0.6584367715090604 0.7652208274496175 0\\n0.30041062826338794 0.08036723586266403 0\\n0.9839536121700394 0.8445946065892113 0\\n0.4183834831782033 0.2845132786728446 0\\n0.042565829384410336 0.5381220396692202 0\\n0.4026136149508597 0.8972834330558429 0\\n0.7429989245056511 0.8130719556918532 0\\n0.3645332107096585 0.13833633571194914 0\\n0.9547216710440194 0.8743972390881397 0\\n0.3591375451209641 0.25875316142598725 0\\n0.2612804381822077 0.5733294214719562 0\\n0.08016374423644612 0.3528590695163868 0\\n0.9432229930949778 0.26163556924883813 0\\n0.240867028128685 0.7267903471725905 0\\n0.09203012448510273 0.21845027187102728 0\\n0.21287630277062142 0.9972305506976692 0\\n0.12901618433509454 0.6610877761747838 0\\n0.9306774271097862 0.22869317914068976 0\\n0.8580611743576876 0.6613735093413686 0\\n0.7478771273079852 0.6564352423067211 0\\n0.49341111917527847 0.9608728958727625 0\\n0.7263935158599831 0.8643888287550853 0\\n0.3107257559595479 0.20656347537423692 0\\n0.9475164554809636 0.2055116082761601 0\\n0.9764331207130502 0.452780962917248 0\\n0.06787882455659466 0.7824103581746126 0\\n0.9650953716028214 0.26279131841300585 0\\n0.03936831371249738 0.9559039086683886 0\\n0.1765644171038646 0.15793041280537423 0\\n0.2774652197887041 0.8509228062936197 0\\n0.9237974914719014 0.09008927327316651 0\\n0.64430848472243 0.2018161529603999 0\\n0.32751978246522573 0.8026295102905842 0\\n0.2654815854106076 0.6694838405429389 0\\n0.9729437107309422 0.794410042472807 0\\n0.0034366398854988045 0.1969597656425236 0\\n0.36950480883066483 0.21518697012717192 0\\n0.4770149299491737 0.10629852175456633 0\\n0.13404076559783296 0.29110145680431154 0\\n0.6150722428584732 0.08500306833304638 0\\n0.33648252193000816 0.6825015219233586 0\\n0.19855142049350605 0.6863844290932214 0\\n0.42220050014370347 0.1442409011126078 0\\n0.2121803118920218 0.42191071576012484 0\\n0.01688132898411021 0.03666979349432542 0\\n0.05130242711265165 0.26926666338136707 0\\n0.27508248314256767 0.23977388116014897 0\\n0.05319473829727228 0.9987547707723766 0\\n0.972297024613452 0.9381652917773338 0\\n0.43043542394962986 0.8463278320609079 0\\n0.4822993721530723 0.083465863393826 0\\n0.8490870885530265 0.4481552027990324 0\\n0.9385218529560169 0.06716884011662783 0\\n0.3070618224084942 0.02006416815047296 0\\n0.7175676775349652 0.02413315053824261 0\\n0.7223841538467949 0.9000322993072121 0\\n0.7577670217981041 0.5106610727498264 0\\n0.5573384689595049 0.13921663891287162 0\\n0.08901244042795053 0.8295041795797328 0\\n0.8303637070958216 0.6671524293088078 0\\n0.8045926571525245 0.5108002112158829 0\\n0.5989543851482002 0.8917091292740918 0\\n0.2668043271140039 0.18961570753459067 0\\n0.8529212006658403 0.10824783629661117 0\\n0.3222854142058782 0.8520808182787273 0\\n0.37205643518896836 0.015388355729127001 0\\n0.752663083162423 0.9637896274382127 0\\n0.2667082169205922 0.8619608160317318 0\\n0.762616910915268 0.7680801608284181 0\\n0.7212775979250771 0.7514225475474391 0\\n0.9058900672695561 0.4327385548558079 0\\n0.1864122450050666 0.3085689920340371 0\\n0.9245377349212298 0.9082536528936218 0\\n0.9353447960669007 0.6718223643817769 0\\n0.9942109299911461 0.5848371294429877 0\\n0.5820496990627115 0.9876809655249732 0\\n0.22036581358078078 0.43252592080822816 0\\n0.7770436363124175 0.25765331549211057 0\\n0.6709129728908799 0.8214224590884713 0\\n0.7643608726893554 0.7472034080296163 0\\n0.05635318607243989 0.49327048625284065 0\\n0.7695140231255545 0.20011978865944546 0\\n0.10252281359345561 0.21848571770745284 0\\n0.9469717135133161 0.2393299544027222 0\\n0.1127581563911314 0.4508106336656057 0\\n0.6279916499573943 0.07142532151660963 0\\n0.5937003029265809 0.06882848740536351 0\\n0.13730045285311498 0.9484880251484725 0\\n0.6444798429344286 0.13290336728777508 0\\n0.9869515358935161 0.37883857303263424 0\\n0.3398630946227982 0.7197654034111763 0\\n0.22385731170459366 0.8506874674503105 0\\n0.22970770282537945 0.554455113295545 0\\n0.07160523825904785 0.16350283782794572 0\\n0.9752309186760025 0.05297475199812185 0\\n0.961521908733881 0.4873049136404446 0\\n0.28860397841433805 0.7382972526160946 0\\n0.37871790196268595 0.10333283806928817 0\\n0.24222935315527294 0.06704356319458715 0\\n0.9242090337790434 0.7888885357805403 0\\n0.028619282284040315 0.7018708135197484 0\\n0.1731181607019977 0.4666597340253893 0\\n0.8331601735622542 0.1838901245961383 0\\n0.5668810893538294 0.8422767133167839 0\\n0.9226543198543835 0.2034436635185075 0\\n0.6846127710306983 0.04611997559696457 0\\n0.947055194416189 0.4246054884039441 0\\n0.077919896573165 0.29270268535660715 0\\n0.20950083573386513 0.8628943455093542 0\\n0.6340473248630478 0.7871016014526944 0\\n0.6548435855045567 0.3335315199643648 0\\n0.21311693048543878 0.342762555039388 0\\n0.6205909278000388 0.18277645718849256 0\\n0.6051074594481126 0.9661509060702386 0\\n0.7530637981131276 0.9301302518089236 0\\n0.27031213532952125 0.23816763996817314 0\\n0.9283125304589098 0.7544059775605624 0\\n0.6553993011511449 0.21079614876789232 0\\n0.25862297233323883 0.4344830408090935 0\\n0.048707836115504 0.29737582763518644 0\\n0.4968125816507566 0.17248787807103727 0\\n0.7458139645300489 0.6540693900362781 0\\n0.888661266713224 0.13640069462717008 0\\n0.0067071120409143425 0.07035398757955524 0\\n0.7555942667485109 0.5060208384567726 0\\n0.129117911063526 0.37874738872368796 0\\n0.6429881910066634 0.14097791870138943 0\\n0.8691153463320979 0.5856023610822183 0\\n0.18354670417342767 0.222421598712094 0\\n0.862882184257438 0.48354049765915763 0\\n0.7993652949947996 0.22668446063948633 0\\n0.7990036600499216 0.1437326145699649 0\\n0.02351715633699647 0.7321195897813232 0\\n0.34171351228998725 0.1499520924731853 0\\n0.1320483023462683 0.8729525603937709 0\\n0.7437591499076371 0.9082651331484948 0\\n0.07276157102835978 0.5613174879297175 0\\n0.9694984168857761 0.2653338321126296 0\\n0.9999403047515074 0.655990503076287 0\\n0.00453554375936116 0.7877576198483013 0\\n0.07239596862910325 0.4348393489672211 0\\n0.956719390739733 0.49878641963449 0\\n0.04277121415898011 0.23201290869386182 0\\n0.4375806002834973 0.9694906359174271 0\\n0.7542516814641037 0.9834235444318371 0\\n0.09104311996834036 0.10163677547090388 0\\n0.8092386927316585 0.0807795011014244 0\\n0.13366024454140224 0.26709588188500366 0\\n0.38233516613303353 0.9453422297288833 0\\n0.8141892916087802 0.8961034783905277 0\\n0.6242919861024261 0.17638523745564627 0\\n0.7307219713176768 0.04346294895190228 0\\n0.9442420088316313 0.69444591226766 0\\n0.12118226206141236 0.21558074911589187 0\\n0.5789910215996892 0.24922983084681827 0\\n0.4847676052652232 0.8988584773605871 0\\n0.573612446728107 0.007506079633629148 0\\n0.48607437049264357 0.16629195628739135 0\\n0.7035342931601806 0.09770194942745436 0\\n0.7556852118849533 0.200237963210396 0\\n0.38337828997438206 0.9078495714011022 0\\n0.9172675029900464 0.3582266009363242 0\\n0.6124517830586222 0.01814512735405205 0\\n0.8104090955698164 0.237079578325468 0\\n0.10997588166709937 0.7524675302692622 0\\n0.9916214354775321 0.5485759086056817 0\\n0.22686195203310378 0.3502897739937776 0\\n0.40263002994833874 0.2741641350184284 0\\n0.8882368238878413 0.952857314397441 0\\n0.9404056111517083 0.9909001450148611 0\\n0.7280645772348943 0.748128550848372 0\\n0.8798222936708423 0.7472887057117477 0\\n0.07968674904987372 0.8208322269873501 0\\n0.29607760813593254 0.40090055565315785 0\\n0.23374772649149667 0.45088913032823685 0\\n0.9114170810157392 0.6425414332525586 0\\n0.448276100871299 0.7381635541679901 0\\n0.5745295948651726 0.12836575589112198 0\\n0.632602298817637 0.8951125882251547 0\\n0.9989280666665838 0.9683440356432466 0\\n0.2703763942521179 0.1404540325657636 0\\n0.7405799076654278 0.7103477819998387 0\\n0.4847052840501085 0.8548526024356449 0\\n0.023894127589041525 0.6538297093650529 0\\n0.3839322761124715 0.9979483561034931 0\\n0.8479619789711473 0.4005873423365165 0\\n0.13484719990136163 0.22743073177845385 0\\n0.7785176823865563 0.8276501824001663 0\\n0.0528612911585844 0.5985826567436251 0\\n0.9797019346229991 0.9602325282459757 0\\n0.8554827754005485 0.45187497194537074 0\\n0.5235751893343433 0.11007423603918598 0\\n0.951609141330048 0.5962209805599756 0\\n0.1760646648370403 0.8542944508832998 0\\n0.9552258821320927 0.3987110167281075 0\\n0.7771132986116095 0.18398354297357422 0\\n0.712864079488365 0.8063297957737801 0\\n0.8052505314660371 0.5151030545338003 0\\n0.8185557303371296 0.9570968011854247 0\\n0.029068633474335748 0.6193738722296336 0\\n0.9279101740518962 0.4075915071235299 0\\n0.870042482212284 0.22748149867674716 0\\n0.9144960448338393 0.591158538246312 0\\n0.5146031538850081 0.006021229966075237 0\\n0.16683370018324917 0.09747922949758236 0\\n0.1768526324529085 0.5905086209669602 0\\n0.9470614944750599 0.95379628508421 0\\n0.737897708496692 0.7999763297826216 0\\n0.2950601986428304 0.23199593067377045 0\\n0.04792400707640021 0.9615111871140637 0\\n0.10717861092087466 0.7238759064409097 0\\n0.04585753875277787 0.10192530391778809 0\\n0.8671643658965978 0.9453181589217615 0\\n0.5261089657269041 0.21354952524136883 0\\n0.18167997947505998 0.5407106358659184 0\\n0.11435262027328097 0.25880680988730353 0\\n0.9552301908705464 0.39155460176016355 0\\n0.6421051293207741 0.26943658466522424 0\\n0.7922631797116949 0.5995449201185178 0\\n0.9141522773901838 0.3898290375251594 0\\n0.14505212075221185 0.4357873976785218 0\\n0.8837561605892045 0.40127671842165547 0\\n0.1162442535676318 0.22292078845901264 0\\n0.9312411548813215 0.3130622978769888 0\\n0.2596926749543663 0.7905897070911628 0\\n0.8150788958666436 0.1265709767239419 0\\n0.17896063471835055 0.953408397688852 0\\n0.8766892540668537 0.46585495121921383 0\\n0.5473370208483488 0.06949926687059382 0\\n0.9354992023822336 0.8938793557873744 0\\n0.20129689788308414 0.12576195435324755 0\\n0.6852359927789431 0.21199564738412535 0\\n0.17071464098719125 0.6587798940786662 0\\n0.9157461735049055 0.84546581236436 0\\n0.344059358274834 0.20100388388327828 0\\n0.3222663155157882 0.011335359058094796 0\\n0.8873187447812715 0.5332306030573952 0\\n0.5652515252680494 0.8251789372972322 0\\n0.8439692345063355 0.15608142140403058 0\\n0.7409389814765767 0.7054659907301754 0\\n0.8721061169860934 0.8804565230735177 0\\n0.18822213229736695 0.6382582792856327 0\\n0.7845817888953565 0.8457278133330934 0\\n0.8309720972231414 0.3638102208734465 0\\n0.08305736664457652 0.7727664259984501 0\\n0.9066925911028727 0.22748954980083458 0\\n0.21286656411092086 0.30442973852391486 0\\n0.05865221449553659 0.37589085990896565 0\\n0.017097506478398694 0.842343749019416 0\\n0.9674298828451818 0.8916154631860282 0\\n0.8490079614731231 0.912235492013327 0\\n0.07957486943407 0.016450546862334825 0\\n0.8373926292177313 0.5701435814393557 0\\n0.16458639325265534 0.0852963214547795 0\\n0.1653686341619799 0.04484026659204576 0\\n0.21674286023641642 0.9741738329326316 0\\n0.911492790369272 0.8471826053970314 0\\n0.27163971097911743 0.17290085270818534 0\\n0.5872786352143635 0.1373263647349119 0\\n0.015451708152452337 0.7625972139703501 0\\n0.7752132128904203 0.15365707879824286 0\\n0.11853687172174154 0.06996217361357793 0\\n0.3144640779103256 0.9518883938340749 0\\n0.7912906088048943 0.16848356495273276 0\\n0.44078077864274245 0.07095416993878267 0\\n0.9797976352520049 0.02804258695048878 0\\n0.01625751171139944 0.16794123084290835 0\\n0.358155684405692 0.2449496110956918 0\\n0.19202874658389557 0.07751967928144965 0\\n0.5970099889670534 0.8138740057782167 0\\n0.0853529975260986 0.7244078957711735 0\\n0.47668531516159485 0.24033060395340589 0\\n0.8428632914320995 0.5138748567693658 0\\n0.006547043131096886 0.9998505339789108 0\\n0.4998572732628095 0.9265320832243158 0\\n0.9656701895163599 0.13920663444570291 0\\n0.21572737105557505 0.5157149445004784 0\\n0.9389574448497214 0.7481300172365897 0\\n0.0971566252489966 0.8689752430232737 0\\n0.6332847479015276 0.7507982972153849 0\\n0.35986579448089284 0.06839833929346562 0\\n0.9848816928957914 0.5548378234802938 0\\n0.7620535551622614 0.7394746260429387 0\\n0.027974706736456856 0.852379337373481 0\\n0.5030530325799972 0.8828154937194882 0\\n0.8808242947585261 0.06918469824238394 0\\n0.7322108710603419 0.9911574041465699 0\\n0.6453096770676324 0.0195987044173217 0\\n0.8636094253811791 0.645652088956623 0\\n0.07081508356393551 0.7796787108510512 0\\n0.5888766078461436 0.8528213506590059 0\\n0.22039345718775005 0.2373548405435758 0\\n0.17979033402624633 0.5459770077847684 0\\n0.9036754260753145 0.6424219986793679 0\\n0.3628001404459946 0.8602725068128542 0\\n0.9085907037038665 0.8148421680776745 0\\n0.11758387403704029 0.5800922521681934 0\\n0.7580786059065808 0.959534178662986 0\\n0.21064157515619186 0.24749150818489984 0\\n0.6654710540809878 0.9959514745372812 0\\n0.32196916819690113 0.14213987818417284 0\\n0.6239052958458752 0.7885083698811899 0\\n0.44075970083446603 0.9798651549802186 0\\n0.9879352341657588 0.609785090591869 0\\n0.30321374407894275 0.839052180884862 0\\n0.36959696105308604 0.12397566599485121 0\\n0.264060968191257 0.45521451161219384 0\\n0.7707315060528034 0.8550263572900687 0\\n0.3861233026035914 0.01528793920517868 0\\n0.5281723018749248 0.02388286875647705 0\\n0.9829575167446599 0.28651492750507834 0\\n0.5134931749404217 0.8534081429374275 0\\n0.5119872290263509 0.9640695349841437 0\\n0.05859924052825005 0.18015285119215385 0\\n0.7268867681010698 0.8921584279831619 0\\n0.6709325531334724 0.046029625206991165 0\\n0.12855297686240075 0.9463748001783918 0\\n0.9609890933049979 0.19061171232132923 0\\n0.12385230028120608 0.7215239480642128 0\\n0.4558058241930454 0.11634060421614123 0\\n0.28521850946465854 0.41440593728848973 0\\n0.04340196641189664 0.8251705138770062 0\\n0.012586165052150156 0.6912122030566429 0\\n0.9100891504191243 0.13597907552962363 0\\n0.9068717457187996 0.5259137114540825 0\\n0.16559542757332257 0.3632981479928096 0\\n0.736641456957579 0.9875536748118365 0\\n0.758590988343189 0.10526381605229196 0\\n0.9475642327517358 0.4629416784899172 0\\n0.6929658883879638 0.1460281161709429 0\\n0.8566660559808199 0.0984274803268882 0\\n0.8729179253742133 0.18424328645513222 0\\n0.008067694224748179 0.9444205914232068 0\\n0.6807441427103254 0.184729106681491 0\\n0.3052284897895331 0.2401931981293306 0\\n0.9249295052605263 0.14212607275186273 0\\n0.7766951213180666 0.01961796027112861 0\\n0.9573251129907183 0.7253410007064427 0\\n0.1558604880837795 0.8033941880726068 0\\n0.9545566992821711 0.1313969738183226 0\\n0.6374144377460093 0.7789483300746372 0\\n0.8840672716236586 0.08360774410543381 0\\n0.305556285160444 0.22391509021193157 0\\n0.6002383049742798 0.9683290713752598 0\\n0.15173541380964672 0.17353494770146227 0\\n0.32037274362034596 0.9913052770811988 0\\n0.10583420922594622 0.8605863174502286 0\\n0.9540841008923097 0.1286518841041171 0\\n0.49344399141454776 0.9362697048220888 0\\n0.08446373859510692 0.8804424434662612 0\\n0.5339278632949982 0.013568223677550284 0\\n0.1512877053740157 0.9866566242203412 0\\n0.7208124127179371 0.05970757314295572 0\\n0.07123311662696252 0.25305638462997326 0\\n0.06148131032581394 0.5732118637734626 0\\n0.2774874194193365 0.009334553782249455 0\\n0.13282494816292734 0.02989301831102653 0\\n0.41789251471327593 0.8908326253846913 0\\n0.11403778935286024 0.4733533978675071 0\\n0.7087193341325377 0.15873283095066082 0\\n0.8504791896815532 0.24381803498608434 0\\n0.37557866450656996 0.0921734901581307 0\\n0.9819438482987702 0.7107217512423788 0\\n0.8354065436411784 0.656449462271703 0\\n0.6209858260625586 0.6904738012260628 0\\n0.792460474033217 0.8732523193105305 0\\n0.875217346703785 0.21490406084666824 0\\n0.13035206148025658 0.33780967393520944 0\\n0.050231759562752054 0.05464200612624437 0\\n0.04382678462088485 0.4933534861111203 0\\n0.2914561056757613 0.7250220534123879 0\\n0.9010888863344266 0.4572554494740515 0\\n0.785309299669164 0.9348008540506354 0\\n0.8200992972658522 0.26155070623083887 0\\n0.020645189910759698 0.6247636940151786 0\\n0.9017058835406596 0.6453342758243008 0\\n0.7777561093798449 0.7430734649704599 0\\n0.9564509690988494 0.4247796399758371 0\\n0.5225774900776068 0.19557038654800518 0\\n0.5386459597223894 0.8548494579085151 0\\n0.726469016800061 0.12227133413997349 0\\n0.7894755042072715 0.6698000962554438 0\\n0.08216515141954794 0.9551988934093881 0\\n0.00847303804748889 0.5093449484255923 0\\n0.6680452836847747 0.26041373718521754 0\\n0.4672667769196215 0.835457695842586 0\\n0.780408720448465 0.9508292113480796 0\\n0.5576481799096262 0.7801098426361878 0\\n0.012791722768045855 0.6866355621900058 0\\n0.7401817382087382 0.8986862190091204 0\\n0.6869192522020569 0.05040391371184394 0\\n0.8922682684612887 0.8444069905929926 0\\n0.7186387247638524 0.8018758295677118 0\\n0.5829484780115263 0.9572296053416072 0\\n0.8629484780102489 0.7432126917600536 0\\n0.845042297429388 0.6422323447052652 0\\n0.061524587893278415 0.21998203187036858 0\\n0.20431283457433724 0.9853401644248738 0\\n0.7093864705367878 0.8970383663520499 0\\n0.04626393616850233 0.12466048302404265 0\\n0.026133867366910035 0.8928399563282647 0\\n0.7167906074500999 0.0201554596303678 0\\n0.5944911461410822 0.8442885089302352 0\\n0.14502492633095365 0.4902248949949767 0\\n0.8359356263595943 0.7805853970499409 0\\n0.7337076256347568 0.16418561330475445 0\\n0.19385145920081637 0.6915827207267374 0\\n0.2877144912377366 0.8317071973791037 0\\n0.8364614978825328 0.6121544875980232 0\\n0.5699883279551007 0.9186216906997868 0\\n0.8156489560544016 0.39192870890691667 0\\n0.09250682305068259 0.9945686435934272 0\\n0.9896912000538605 0.0775734721321305 0\\n0.06798739397449971 0.27373196446483394 0\\n0.7342441755166426 0.6934292342853339 0\\n0.2621858713802898 0.034865335529949415 0\\n0.021283000956855536 0.3185886655271646 0\\n0.5823825721539848 0.982320049867824 0\\n0.06310204270158781 0.42438331781374805 0\\n0.9986343681840881 0.825915224169346 0\\n0.8423761019107044 0.5093555527794451 0\\n0.14833651679432713 0.8969838450140187 0\\n0.8267500132111004 0.4707408756919549 0\\n0.36936384132873523 0.9575789685199624 0\\n0.021144560101708287 0.5133657079830153 0\\n0.15606140361516796 0.6842953491921566 0\\n0.9627977124179744 0.031623641587135376 0\\n0.8688562337701248 0.10315527584528361 0\\n0.10843731193072437 0.08650453457972263 0\\n0.05585269536302262 0.9948455343022927 0\\n0.7540743476229685 0.029690729240703595 0\\n0.07278411680758623 0.11538285616315858 0\\n0.8231640348985616 0.5720176718600082 0\\n0.20952354123257744 0.319445675895328 0\\n0.9636149211089786 0.18561174533874525 0\\n0.7370738501842169 0.006203269715889914 0\\n0.6508815774004376 0.097442978174486 0\\n0.4022924601382888 0.8066808566002718 0\\n0.9729198710627913 0.488509905380913 0\\n0.8400838333850863 0.8198088617011438 0\\n0.15774165943703844 0.5208914737419539 0\\n0.526537311457482 0.746957680521358 0\\n0.7563421130166094 0.09124241563987112 0\\n0.8599764765934392 0.7973948970306269 0\\n0.9436894936316126 0.42356300180551665 0\\n0.6141901868667825 0.8130143258905547 0\\n0.25490334226829114 0.8769298456246084 0\\n0.9838387087092049 0.8470246731268407 0\\n0.22862947088287822 0.35015808309367114 0\\n0.7113529773094905 0.9862756611989854 0\\n0.8211638148274347 0.26716481572690176 0\\n0.8439602787025839 0.15101443015085014 0\\n0.16757756709520155 0.5287206701735556 0\\n0.719725176584857 0.08271241901267434 0\\n0.21683188914332185 0.7523380449531059 0\\n0.11772026293401072 0.23244156777156022 0\\n0.9653795030002269 0.16378106761428723 0\\n0.7755761562091892 0.8357915510011237 0\\n0.8789077846420896 0.3545134829850801 0\\n0.16411695898946443 0.9884625563734671 0\\n0.9552943243187666 0.3708662025337389 0\\n0.7130712031763636 0.18995657404119792 0\\n0.03807937421139418 0.18724860757402784 0\\n0.5494003826518499 0.753170166688587 0\\n0.21876265767646974 0.1741484013521155 0\\n0.6563895407390026 0.7424817589249901 0\\n0.7293825845247195 0.7990766686987073 0\\n0.19645725420957894 0.11015555054069914 0\\n0.269448441594463 0.08158287358641092 0\\n0.685179135274712 0.0006089057350668892 0\\n0.9609037144433932 0.22476492356763955 0\\n0.8892533446540968 0.28435496923547865 0\\n0.0625422933629397 0.8460949476999107 0\\n0.046489214255882705 0.33994346096406713 0\\n0.25262365442729173 0.9351683348635401 0\\n0.8356057714983571 0.035276648774185126 0\\n0.8622401206517536 0.6961830149541205 0\\n0.5416774157134706 0.8795134476784846 0\\n0.8309571310569888 0.5371751017113321 0\\n0.673807912851659 0.014832545097278604 0\\n0.3430243146911688 0.8338574253043237 0\\n0.8781688680243619 0.3593554137194793 0\\n0.7471504673076599 0.6903609327698563 0\\n0.9465937634437346 0.2948416135943913 0\\n0.9597985863070492 0.45829479464971534 0\\n0.4507525888652454 0.20506105023613885 0\\n0.10825671642543055 0.15518395156047116 0\\n0.22607763586621143 0.960065575709795 0\\n0.7338045458334471 0.5366395227835731 0\\n0.8688461528246437 0.9083399992433578 0\\n0.3259283727427119 0.8230069328270335 0\\n0.9283164650355609 0.8098157578601356 0\\n0.31221406445609867 0.9457591588641137 0\\n0.2044090154547571 0.8166670550813663 0\\n0.024660307553471927 0.12711997911910067 0\">, @options=Hamster::Hash[:with => \"points\"]>, #<GnuplotRB::Dataset:0x00005643514fb9c0 @type=:datablock, @data=#<GnuplotRB::Datablock:0x00005643514fb948 @stored_in_file=false, @data=\"0.3697135150568239 0.541809194368996 0\\n0.6143530366968214 0.3974347209001604 0\\n0.4344521695492197 0.28748880590807224 0\\n0.5245829037461333 0.3062267201053054 0\\n0.4420230311050247 0.624410038264162 0\\n0.48185702173413003 0.5712683308639144 0\\n0.6779276629020332 0.3882925110077404 0\\n0.33092492403897766 0.2612862099138866 0\\n0.7088953404259424 0.4398037967911176 0\\n0.3079732704012298 0.4192540351531441 0\\n0.30130489415899775 0.41783633792315555 0\\n0.5767302478681051 0.8034394050034679 0\\n0.6374100484474298 0.6598609043226972 0\\n0.5242397982866717 0.669286992827128 0\\n0.4701458013043093 0.7260969067186163 0\\n0.6395033752686635 0.4598704266187099 0\\n0.41793904327278597 0.3797667563773859 0\\n0.5730239044578351 0.6922539080031099 0\\n0.5976949124454126 0.5324421463262518 0\\n0.5582746204707965 0.7665134736895042 0\\n0.6100756436656879 0.43059866280774384 0\\n0.40187449853224033 0.20272290173160956 0\\n0.33832322672741777 0.611639175504017 0\\n0.7261418662346204 0.6835504776358854 0\\n0.41330397902593663 0.6257490214688568 0\\n0.26159906257416865 0.3851450674835395 0\\n0.5156928363485125 0.43259086130246605 0\\n0.5844610647114797 0.42287549846930506 0\\n0.4097647661342999 0.331470879246169 0\\n0.41265906107731365 0.5438750829233178 0\\n0.47214240297147825 0.5228864807146039 0\\n0.5524387323047861 0.2950066747380008 0\\n0.5566062888071729 0.810116537496076 0\\n0.6032558872901308 0.6323226748842607 0\\n0.6459038370936689 0.4434165183686354 0\\n0.5145631003305968 0.5734820349607485 0\\n0.6784117207939186 0.5218179826806326 0\\n0.6224644847386734 0.5810446137525206 0\\n0.4573093709212157 0.6441327825916549 0\\n0.6151341321818604 0.22844961390891438 0\\n0.5034903081550539 0.46888070533609694 0\\n0.6109109840062746 0.6664858266155753 0\\n0.23222087970605731 0.4243961097583753 0\\n0.3514291637738317 0.3761565970467857 0\\n0.5513583160492619 0.6914827656516013 0\\n0.5538844846174218 0.5030321234857638 0\\n0.6161757966371149 0.38403902732054584 0\\n0.6514784105784167 0.6051036366811747 0\\n0.5838360302920361 0.3801158738696292 0\\n0.29821495442664525 0.44316396249895085 0\\n0.6418922624759771 0.45570011890138584 0\\n0.4060365600556691 0.34812715240433056 0\\n0.4545615957201147 0.28868629601986007 0\\n0.446300929393754 0.46204283448291794 0\\n0.7149253721891979 0.2792740704045378 0\\n0.275091153966887 0.707623793646068 0\\n0.2974942530970571 0.5663387142133713 0\\n0.44779721988903665 0.4294402992200723 0\\n0.42030937450330275 0.5566507959216644 0\\n0.4233034758058928 0.5370395105258589 0\\n0.28922869464401424 0.47485693191393863 0\\n0.3381207010820102 0.4675552334766053 0\\n0.6016290722354874 0.26661370833859044 0\\n0.18717791877443435 0.47425934483539767 0\\n0.29310437928696065 0.6741708509464708 0\\n0.2618903684111472 0.36751585096231265 0\\n0.37311806577848694 0.6268415259738295 0\\n0.49717715765354464 0.3428549964757398 0\\n0.4783264194989708 0.3181779822524121 0\\n0.7147521100717709 0.6269938319561159 0\\n0.5662606154421149 0.5065483503720125 0\\n0.788980442396218 0.6000811340878616 0\\n0.384729554783652 0.5967022368411519 0\\n0.4334135042136936 0.6738054547403356 0\\n0.317307928101739 0.25357999520142604 0\\n0.3638874620974294 0.725321416561149 0\\n0.4716462262987685 0.1971973236671064 0\\n0.6295428806319451 0.5157303827576686 0\\n0.4443430120787303 0.645909454282884 0\\n0.4529465393054031 0.3943376116120735 0\\n0.32496383422529584 0.4221799146977856 0\\n0.7471965333507108 0.6328474885346138 0\\n0.44060121661310325 0.32368102514684804 0\\n0.24642121068946754 0.4215626070363456 0\\n0.6004414355115029 0.5644111965724558 0\\n0.6753728377345805 0.5416539845825817 0\\n0.7650601747998891 0.39112951641959914 0\\n0.5891334084918795 0.5741696269029503 0\\n0.2430399991699308 0.566914958251412 0\\n0.44117675736337325 0.6161039955400722 0\\n0.3833182694172407 0.5895544757446665 0\\n0.35501672345169033 0.38173518505342763 0\\n0.4883994521167325 0.29828230443932513 0\\n0.6372656846978871 0.3842405675534487 0\\n0.47309821386526996 0.7002000433203495 0\\n0.3105828963273518 0.5895246228598298 0\\n0.32763644409586 0.5390387616117102 0\\n0.7512519137797236 0.533482441536792 0\\n0.5642517416506012 0.515910002859987 0\\n0.3592582590056904 0.4593507294115682 0\\n0.40657381484626365 0.3252633962952648 0\\n0.2903775723640306 0.69396435387101 0\\n0.2883910400327777 0.6607584048132161 0\\n0.44421031069612515 0.41967073854544856 0\\n0.2341708927536772 0.4584830674407566 0\\n0.35353564041947616 0.3150446272257137 0\\n0.48637630335247195 0.6838702565318391 0\\n0.4516644747352445 0.4035397361877804 0\\n0.5385250372944466 0.6459691715680052 0\\n0.7400395871235722 0.6786364544313137 0\\n0.3862904004973642 0.4481523588103342 0\\n0.3915687527806849 0.5524004550041061 0\\n0.3290748533630785 0.3153971409822799 0\\n0.33377198207663306 0.5160518920650159 0\\n0.23285603297632218 0.4090174585736711 0\\n0.22886863662057455 0.37987491718855826 0\\n0.6601216453244706 0.32640966255512527 0\\n0.5422838088301821 0.3067035690211617 0\\n0.3951729954522699 0.5088817275121342 0\\n0.4541950595112292 0.2938018187108661 0\\n0.448919359158204 0.4361667011311935 0\\n0.3495941007103154 0.625722189198928 0\\n0.35379799535057277 0.730849607654155 0\\n0.2140476884970146 0.46145708899658044 0\\n0.3128189817008742 0.6662870506979166 0\\n0.6000092867364609 0.48405046491237314 0\\n0.20601549823137266 0.39448864877089573 0\\n0.2234072689543357 0.5460834923001583 0\\n0.4643178746631029 0.656748514159546 0\\n0.30288638378731614 0.5062519458213591 0\\n0.7086300026534279 0.40639074357546023 0\\n0.5443312963787754 0.593837325326353 0\\n0.42604457216678215 0.7078379088581673 0\\n0.3745741535185383 0.5339082107865742 0\\n0.6644440028129954 0.4048624344159061 0\\n0.2820148454793514 0.5261494347766308 0\\n0.3856063793201757 0.6327570407397668 0\\n0.2674198437989541 0.3551187429476387 0\\n0.756595028533235 0.5420410810442 0\\n0.52509285784988 0.40986409060964946 0\\n0.5023424857453319 0.740180225595801 0\\n0.5618855325124503 0.7354297972719218 0\\n0.5886769643753712 0.622872263806174 0\\n0.46015140944495814 0.5508085782026124 0\\n0.4711672208291823 0.30042190985433237 0\\n0.5993911694612387 0.6810409336512758 0\\n0.4712763248516574 0.6698342103884507 0\\n0.3387295164035765 0.6002256026933289 0\\n0.6351184246152319 0.6162682523664933 0\\n0.6428099391217212 0.6589464152176724 0\\n0.6608637577223379 0.4941072755250019 0\\n0.342751308267745 0.3965565629182205 0\\n0.6214364555567372 0.3350875186676765 0\\n0.708417906471406 0.44389105631972414 0\\n0.3287801361234314 0.3964500308056118 0\\n0.5623170336968627 0.377186185544783 0\\n0.3478405472752235 0.470145369558288 0\\n0.6883373623397643 0.45333447207236666 0\\n0.4834668452318387 0.5443661303354604 0\\n0.35380300500463724 0.25162647922392545 0\\n0.6183456203725826 0.6149244060571842 0\\n0.3535975681819603 0.42442916672527176 0\\n0.5473464777327791 0.33570023443733354 0\\n0.5317807243581119 0.5580318691396656 0\\n0.3121683454240982 0.5773812188871277 0\\n0.5451024416058111 0.323815106228485 0\\n0.5213963664029442 0.4036219560682751 0\\n0.48272344192383343 0.6034689183844681 0\\n0.6456449638098354 0.3212092700884317 0\\n0.7846765900588496 0.5428968693419772 0\\n0.3999010439269576 0.5800086195453337 0\\n0.33960597801776315 0.5088188702950146 0\\n0.340518281001711 0.23823447387366048 0\\n0.6078074469528075 0.2842451454746161 0\\n0.6607068626062191 0.723471089446295 0\\n0.39091677149913595 0.30792861687486506 0\\n0.33474294504315116 0.41339620108362385 0\\n0.8082497266569326 0.43418450106330975 0\\n0.23952890535196691 0.5670105540438541 0\\n0.4042159323032041 0.3879351547824089 0\\n0.24072792387752895 0.6007059437806004 0\\n0.49958551878743673 0.5279721790828041 0\\n0.39474849070445184 0.41828026143555863 0\\n0.40020919456099524 0.4316697823111222 0\\n0.6070264349646128 0.7674696231127318 0\\n0.31502373150432394 0.41229346391079935 0\\n0.3619009289991707 0.5094765800329513 0\\n0.3702154039546913 0.4785993267913141 0\\n0.33656518619044096 0.4628965520033953 0\\n0.5096637671070883 0.6718873092260186 0\\n0.5121071711394137 0.4734342742800117 0\\n0.4017715303862115 0.6911561311360835 0\\n0.524762045104445 0.6104974235918463 0\\n0.3196318459555122 0.49520662959751993 0\\n0.4963975582904888 0.256346413053132 0\\n0.6165979442647048 0.32525341114278583 0\\n0.4519679951926052 0.5762321414017798 0\\n0.2619924758075499 0.43323162234675205 0\\n0.5233740831035093 0.4987415248563235 0\\n0.6635338438362427 0.5242833719072204 0\\n0.41134376902454106 0.5561512288696662 0\\n0.2290234963640514 0.5281753539097074 0\\n0.3783346575469848 0.464429526978569 0\\n0.5760326377965584 0.7331093290623751 0\\n0.6623858702947101 0.4776272755971903 0\\n0.5822551666590048 0.7372723009314582 0\\n0.6498099691191005 0.36308482495689665 0\\n0.4895837465055414 0.4626370128657613 0\\n0.5976386131030156 0.7427679881429603 0\\n0.5142618140825629 0.32271335232409326 0\\n0.3908473845308089 0.6912815296130099 0\\n0.33222615836924374 0.3987541211073984 0\\n0.2642903779958865 0.5278570976760697 0\\n0.36593993304028816 0.5878615405965368 0\\n0.5783917766162816 0.43320376384297776 0\\n0.6647801861080054 0.7680183707402277 0\\n0.6016819758938255 0.6765975551074465 0\\n0.41982168708499 0.5982599396108977 0\\n0.5498684110535639 0.3958940024849078 0\\n0.6005593791162308 0.3881096629416132 0\\n0.5062450155701644 0.25961219797199275 0\\n0.4921382444696455 0.7111236151114841 0\\n0.5289573274872326 0.5361054353705196 0\\n0.5120227084163196 0.6285364992507503 0\\n0.488175314921641 0.38716820209052094 0\\n0.44203891549875185 0.32029718272936847 0\\n0.6770061875113998 0.6022582316747018 0\\n0.7454477626398182 0.6205334062013041 0\\n0.3490690117322808 0.3698880107643031 0\\n0.7755010664965593 0.64900444631843 0\\n0.4475798772619788 0.43469829083993705 0\\n0.7278228040171991 0.33911437657707366 0\\n0.6598696664273662 0.5630675280674049 0\\n0.666105573858625 0.6508058330240868 0\\n0.3181769340393954 0.6653555028027468 0\\n0.516104499014336 0.2998399389871137 0\">, @options=Hamster::Hash[:with => \"points\"]>], @cmd=\"splot \">"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_3655e4()\n",
    "  dataset = circle_dataset()  \n",
    "  train_data = dataset.clone\n",
    "  train_data[\"data\"] = dataset[\"data\"][0,900]\n",
    "\n",
    "  test_data = dataset.clone\n",
    "  test_data[\"data\"] = dataset[\"data\"][900,100]\n",
    "  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: LinearKernel.new\n",
    "  svm_learner.train train_data  \n",
    "  plot_decision_boundary dataset[\"data\"], svm_learner\n",
    "end\n",
    "test_3655e4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "175500355dee80e3e16f377b8eadb446",
     "grade": false,
     "grade_id": "cell-003013312d593bfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2.6 (5 Points)\n",
    "With the linear Support Vector Machine complete, let's try several values of the $C$ parameter on a sample dataset. Using your ```to_linear_weights``` and ```calculate_margin```, compare the margins on the dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dfd9f64de5f434d45a23faf6181d38e",
     "grade": true,
     "grade_id": "cell-0026d495a9e0ffbf",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b> Daru::DataFrame(7x3) </b>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  \n",
       "    <tr>\n",
       "      <th></th>\n",
       "      \n",
       "        <th>c</th>\n",
       "      \n",
       "        <th>margin</th>\n",
       "      \n",
       "        <th>weights</th>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "</thead>\n",
       "  <tbody>\n",
       "  \n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      \n",
       "        <td>0.001</td>\n",
       "      \n",
       "        <td>-0.7751791683871789</td>\n",
       "      \n",
       "        <td>{\"x1\"=>0.07054944560700004, \"x2\"=>0.055473829790000026}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      \n",
       "        <td>0.1</td>\n",
       "      \n",
       "        <td>-0.14327149208347909</td>\n",
       "      \n",
       "        <td>{\"x1\"=>0.5418967094972553, \"x2\"=>0.6353688763466115}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      \n",
       "        <td>1</td>\n",
       "      \n",
       "        <td>0.2809754813868902</td>\n",
       "      \n",
       "        <td>{\"x1\"=>1.0633739433059786, \"x2\"=>1.2870656701510783}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      \n",
       "        <td>2</td>\n",
       "      \n",
       "        <td>0.26147979561749024</td>\n",
       "      \n",
       "        <td>{\"x1\"=>1.1222268693164803, \"x2\"=>1.3960280383116597}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      \n",
       "        <td>3</td>\n",
       "      \n",
       "        <td>0.27094690657560516</td>\n",
       "      \n",
       "        <td>{\"x1\"=>1.3345763000768156, \"x2\"=>1.730719950526327}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      \n",
       "        <td>4</td>\n",
       "      \n",
       "        <td>0.36419992512237764</td>\n",
       "      \n",
       "        <td>{\"x1\"=>1.6761233756074425, \"x2\"=>2.173478319798825}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      \n",
       "        <td>5</td>\n",
       "      \n",
       "        <td>0.36419992512237764</td>\n",
       "      \n",
       "        <td>{\"x1\"=>1.6761233756074425, \"x2\"=>2.173478319798825}</td>\n",
       "      \n",
       "    </tr>\n",
       "  \n",
       "\n",
       "  \n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "#<Daru::DataFrame(7x3)>\n",
       "                     c     margin    weights\n",
       "          0      0.001 -0.7751791 {\"x1\"=>0.0\n",
       "          1        0.1 -0.1432714 {\"x1\"=>0.5\n",
       "          2          1 0.28097548 {\"x1\"=>1.0\n",
       "          3          2 0.26147979 {\"x1\"=>1.1\n",
       "          4          3 0.27094690 {\"x1\"=>1.3\n",
       "          5          4 0.36419992 {\"x1\"=>1.6\n",
       "          6          5 0.36419992 {\"x1\"=>1.6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_0026d4()\n",
    "  dataset = two_gaussians_sep_dataset()  \n",
    "  learners = [0.001, 0.1, 1, 2, 3, 4, 5].map do |c|\n",
    "    SupportVectorMachineLearner.new complexity: c, kernel: LinearKernel.new      \n",
    "  end\n",
    "  \n",
    "  stats = learners.map do |learner|\n",
    "    learner.train dataset  \n",
    "    model = learner.model\n",
    "    weights = to_linear_weights model\n",
    "    margin = calculate_margin dataset, weights\n",
    "      \n",
    "    {c: learner.parameters[\"complexity\"], margin: margin, weights: weights}\n",
    "  end\n",
    "  \n",
    "  Daru::DataFrame.new(stats)\n",
    "end\n",
    "test_0026d4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queston 2.7 (10 Points)\n",
    "\n",
    "For the dataset in the previous test, the figures below show 3 different margins. Assign the appropriate value of $C\\in \\left\\{0.001,1,4\\right\\}$ to the figures below.\n",
    "\n",
    "| (A) | (B) | (C) |\n",
    "|-----|----|---|\n",
    "|![A](sep_1.png)|![B](sep_2.png)|![C](sep_3.png)|\n",
    "\n",
    "If you think that the values of C producing the figures should be ordered as -1, 2, 5, then return the following:\n",
    "\n",
    "```ruby\n",
    "def answer_a4bbdd()\n",
    "  [-1, 2, 5]  \n",
    "end\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82ca7b117532b0e7706f4920edd05c55",
     "grade": false,
     "grade_id": "cell-a4bbdde497a6c69b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":answer_a4bbdd"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_a4bbdd()\n",
    "  # BEGIN YOUR CODE\n",
    "  [0.001, 4, 1]\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a870bfcd8c3c20b00241779e43d9543",
     "grade": true,
     "grade_id": "cell-5109bdec1496648b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_5109bd()\n",
    "  assert_equal 3, answer_a4bbdd().size\n",
    "end\n",
    "test_5109bd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06c0c9eebdbcce6390e0d1b7a814d376",
     "grade": false,
     "grade_id": "cell-27b29f6d4e77fb22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3.1 (5 Points)\n",
    "\n",
    "The simple nonlinear dataset above did not do well at all with the linear kernel. Let's see if we can do better with the Gaussian kernel. Recall the Gaussian Kernel, also known as the RBF kernel, is defined as follows:\n",
    "\n",
    "# $K_G(x_i, x_j) = e^{-\\gamma \\left( x_i - x_j \\right) ^2}$\n",
    "\n",
    "Note that in the case of missing features, we should interpret them as having a value of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd3df70d3d8ed96686627ddb8f94b773",
     "grade": false,
     "grade_id": "cell-60f47219ede361e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":func"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GaussianKernel\n",
    "  def initialize gamma\n",
    "    @gamma = gamma\n",
    "  end\n",
    "\n",
    "  def update_parameter parameter\n",
    "    parameter.kernel_type = Libsvm::KernelType::RBF\n",
    "    parameter.gamma = @gamma\n",
    "  end  \n",
    "  \n",
    "  def func x_i, x_j\n",
    "    # BEGIN YOUR CODE\n",
    "    Hash[x_i[\"features\"].to_a.map {|k, v| [k, v.nil? ?    0: v] }]\n",
    "    Hash[x_j[\"features\"].to_a.map {|k, v| [k, v.nil? ?    0: v] }]\n",
    "    x = x_i[\"features\"].merge(x_j[\"features\"]) { |k, v1, v2| v1 - v2 }\n",
    " return Math.exp(-@gamma*((norm x)**2))\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb924f3d25f0b72cf773ae05f6c5cfd9",
     "grade": true,
     "grade_id": "cell-6e3abcf7e49fc947",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_6e3abc()\n",
    "  kernel1 = GaussianKernel.new(0.01)\n",
    "  kernel2 = GaussianKernel.new(0.02)  \n",
    "  examples =  [\n",
    "    {\"features\" => {\"a\" => 2, \"b\" => 3}, \"label\" => 1}, \n",
    "    {\"features\" => {\"b\" => 7, \"c\" => 1}, \"label\" => 7},\n",
    "    {\"features\" => {\"a\" => 2, \"b\" => 3}, \"label\" => 2}\n",
    "  ]\n",
    "  \n",
    "  z = {\"features\" => {\"a\" => -5, \"b\" => 2}} \n",
    "  \n",
    "  assert_in_delta 0.60653, kernel1.func(z, examples[0]), 1e-3, \"1\"\n",
    "  assert_in_delta 0.36788, kernel2.func(examples[0], z), 1e-3, \"2\"  \n",
    "  \n",
    "  assert_in_delta 0.60049, kernel1.func(z, examples[1]), 1e-3, \"3\"  \n",
    "  assert_in_delta 0.36059, kernel2.func(examples[1], z), 1e-3, \"4\"    \n",
    "  \n",
    "  assert_in_delta 0.8105, kernel1.func(examples[0], examples[1]), 1e-3, \"5\"  \n",
    "end\n",
    "\n",
    "test_6e3abc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a47bfc662b9d95913bcefde524e1927",
     "grade": false,
     "grade_id": "cell-83d227285e307607",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3.2 (5 Points)\n",
    "Evaluate a trained model with the Gaussian Kernel on the new dataset. Your ```predict``` function should not require any changes to support the new kernel.\n",
    "\n",
    "We should the the AUC improve considerably on the Circle dataset. Compare the support vectors learned in your test with the original dataset.\n",
    "\n",
    "![circle_dataset.png](./circle_dataset.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e31d17418a2d1682090d19a5d8e284b0",
     "grade": false,
     "grade_id": "cell-0496400b8bbff711",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":plot_circle_dataset_with_sv"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_circle_dataset_with_sv(model)\n",
    "  circle = circle_dataset()[\"data\"]\n",
    "  x1 = circle.map {|r| r[\"features\"][\"x1\"]}\n",
    "  x2 = circle.map {|r| r[\"features\"][\"x2\"]}\n",
    "  target = circle.map {|r| \"Y = #{r[\"label\"].to_s}\"}\n",
    "  target.size  \n",
    "  \n",
    "  s_x1 = model[\"data\"].map {|r| r[\"features\"][\"x1\"]}\n",
    "  s_x2 = model[\"data\"].map {|r| r[\"features\"][\"x2\"]}\n",
    "  s_target = model[\"data\"].map {|r| \"SV Y = #{r[\"label\"]}\"}\n",
    "  df = Daru::DataFrame.new({x1: (s_x1), x2: (s_x2), target: (s_target)})\n",
    "  df.to_category :target\n",
    "  df.plot(type: :scatter, x: :x1, y: :x2, categorized: {by: :target, method: :color}) do |plot, diagram|\n",
    "    plot.xrange [0,1]\n",
    "    plot.x_label \"X1\"\n",
    "    plot.yrange [0,1]  \n",
    "    plot.y_label \"X2\"\n",
    "    plot.legend true\n",
    "  end.show()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5e4a43b3ea1066b5d39f8fb1edf1962",
     "grade": true,
     "grade_id": "cell-29dfa90f823f7ca2",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian model test-set AUC is 0.9896036387264457\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-1a5a02b6-6ac8-4003-ac1a-717eac358e3b'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"55a4abdb-7c5d-4ef1-b79a-d7aab02263e6\"}],\"options\":{\"x_label\":\"X\",\"y_label\":\"Y\",\"zoom\":true,\"width\":700,\"xrange\":[0.0,1.0],\"yrange\":[0.0,1.0]}}],\"data\":{\"55a4abdb-7c5d-4ef1-b79a-d7aab02263e6\":[{\"x\":0.0,\"y\":0.0},{\"x\":0.0,\"y\":0.05263157894736842},{\"x\":0.0,\"y\":0.10526315789473684},{\"x\":0.0,\"y\":0.15789473684210525},{\"x\":0.0,\"y\":0.21052631578947367},{\"x\":0.0,\"y\":0.2631578947368421},{\"x\":0.0,\"y\":0.3157894736842105},{\"x\":0.0,\"y\":0.3684210526315789},{\"x\":0.0,\"y\":0.42105263157894735},{\"x\":0.0,\"y\":0.47368421052631576},{\"x\":0.0,\"y\":0.5263157894736842},{\"x\":0.0,\"y\":0.5789473684210527},{\"x\":0.0,\"y\":0.631578947368421},{\"x\":0.0,\"y\":0.6842105263157895},{\"x\":0.0,\"y\":0.7368421052631579},{\"x\":0.0,\"y\":0.7894736842105263},{\"x\":0.0,\"y\":0.8421052631578947},{\"x\":0.012345679012345678,\"y\":0.8421052631578947},{\"x\":0.024691358024691357,\"y\":0.8421052631578947},{\"x\":0.037037037037037035,\"y\":0.8421052631578947},{\"x\":0.037037037037037035,\"y\":0.8947368421052632},{\"x\":0.037037037037037035,\"y\":0.9473684210526315},{\"x\":0.04938271604938271,\"y\":0.9473684210526315},{\"x\":0.06172839506172839,\"y\":0.9473684210526315},{\"x\":0.07407407407407407,\"y\":0.9473684210526315},{\"x\":0.08641975308641975,\"y\":0.9473684210526315},{\"x\":0.09876543209876543,\"y\":0.9473684210526315},{\"x\":0.1111111111111111,\"y\":0.9473684210526315},{\"x\":0.12345679012345678,\"y\":0.9473684210526315},{\"x\":0.12345679012345678,\"y\":1.0},{\"x\":0.13580246913580246,\"y\":1.0},{\"x\":0.14814814814814814,\"y\":1.0},{\"x\":0.16049382716049382,\"y\":1.0},{\"x\":0.1728395061728395,\"y\":1.0},{\"x\":0.18518518518518517,\"y\":1.0},{\"x\":0.19753086419753085,\"y\":1.0},{\"x\":0.20987654320987653,\"y\":1.0},{\"x\":0.2222222222222222,\"y\":1.0},{\"x\":0.2345679012345679,\"y\":1.0},{\"x\":0.24691358024691357,\"y\":1.0},{\"x\":0.25925925925925924,\"y\":1.0},{\"x\":0.2716049382716049,\"y\":1.0},{\"x\":0.2839506172839506,\"y\":1.0},{\"x\":0.2962962962962963,\"y\":1.0},{\"x\":0.30864197530864196,\"y\":1.0},{\"x\":0.32098765432098764,\"y\":1.0},{\"x\":0.3333333333333333,\"y\":1.0},{\"x\":0.345679012345679,\"y\":1.0},{\"x\":0.35802469135802467,\"y\":1.0},{\"x\":0.37037037037037035,\"y\":1.0},{\"x\":0.38271604938271603,\"y\":1.0},{\"x\":0.3950617283950617,\"y\":1.0},{\"x\":0.4074074074074074,\"y\":1.0},{\"x\":0.41975308641975306,\"y\":1.0},{\"x\":0.43209876543209874,\"y\":1.0},{\"x\":0.4444444444444444,\"y\":1.0},{\"x\":0.4567901234567901,\"y\":1.0},{\"x\":0.4691358024691358,\"y\":1.0},{\"x\":0.48148148148148145,\"y\":1.0},{\"x\":0.49382716049382713,\"y\":1.0},{\"x\":0.5061728395061729,\"y\":1.0},{\"x\":0.5185185185185185,\"y\":1.0},{\"x\":0.5308641975308642,\"y\":1.0},{\"x\":0.5432098765432098,\"y\":1.0},{\"x\":0.5555555555555556,\"y\":1.0},{\"x\":0.5679012345679012,\"y\":1.0},{\"x\":0.5802469135802469,\"y\":1.0},{\"x\":0.5925925925925926,\"y\":1.0},{\"x\":0.6049382716049383,\"y\":1.0},{\"x\":0.6172839506172839,\"y\":1.0},{\"x\":0.6296296296296297,\"y\":1.0},{\"x\":0.6419753086419753,\"y\":1.0},{\"x\":0.654320987654321,\"y\":1.0},{\"x\":0.6666666666666666,\"y\":1.0},{\"x\":0.6790123456790124,\"y\":1.0},{\"x\":0.691358024691358,\"y\":1.0},{\"x\":0.7037037037037037,\"y\":1.0},{\"x\":0.7160493827160493,\"y\":1.0},{\"x\":0.7283950617283951,\"y\":1.0},{\"x\":0.7407407407407407,\"y\":1.0},{\"x\":0.7530864197530864,\"y\":1.0},{\"x\":0.7654320987654321,\"y\":1.0},{\"x\":0.7777777777777778,\"y\":1.0},{\"x\":0.7901234567901234,\"y\":1.0},{\"x\":0.8024691358024691,\"y\":1.0},{\"x\":0.8148148148148148,\"y\":1.0},{\"x\":0.8271604938271605,\"y\":1.0},{\"x\":0.8395061728395061,\"y\":1.0},{\"x\":0.8518518518518519,\"y\":1.0},{\"x\":0.8641975308641975,\"y\":1.0},{\"x\":0.8765432098765432,\"y\":1.0},{\"x\":0.8888888888888888,\"y\":1.0},{\"x\":0.9012345679012346,\"y\":1.0},{\"x\":0.9135802469135802,\"y\":1.0},{\"x\":0.9259259259259259,\"y\":1.0},{\"x\":0.9382716049382716,\"y\":1.0},{\"x\":0.9506172839506173,\"y\":1.0},{\"x\":0.9629629629629629,\"y\":1.0},{\"x\":0.9753086419753086,\"y\":1.0},{\"x\":0.9876543209876543,\"y\":1.0},{\"x\":1.0,\"y\":1.0}]},\"extension\":[]}\n",
       "        var id_name = '#vis-1a5a02b6-6ac8-4003-ac1a-717eac358e3b';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x0000564351478e80 @properties={:diagrams=>[#<Nyaplot::Diagram:0x000056435144e0b8 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"55a4abdb-7c5d-4ef1-b79a-d7aab02263e6\"}, @xrange=[0.0, 1.0], @yrange=[0.0, 1.0]>], :options=>{:x_label=>\"X\", :y_label=>\"Y\", :zoom=>true, :width=>700, :xrange=>[0.0, 1.0], :yrange=>[0.0, 1.0]}}>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_29dfa9()\n",
    "  dataset = circle_dataset()  \n",
    "  train_data = dataset.clone\n",
    "  train_data[\"data\"] = dataset[\"data\"][0,900]\n",
    "\n",
    "  test_data = dataset.clone\n",
    "  test_data[\"data\"] = dataset[\"data\"][900,100]\n",
    "  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: GaussianKernel.new(2)\n",
    "  svm_learner.train train_data\n",
    "  \n",
    "  gaussian_model = svm_learner.model\n",
    "  scores = svm_learner.evaluate test_data  \n",
    "  metric = AUCMetric.new\n",
    "  fpr, tpr, auc = metric.roc_curve scores\n",
    "  \n",
    "  puts \"Gaussian model test-set AUC is #{auc}\"\n",
    "  assert_true(auc > 0.9, \"AUC > 0.9\")\n",
    "  plot fpr, tpr\n",
    "end\n",
    "test_29dfa9()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e3c38f017c3c8ff23f6dd47e976f983",
     "grade": true,
     "grade_id": "cell-7dfd7c7a0aa16b6d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian model has 250 support vectors\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAMAAAACDyzWAAADAFBMVEX///8AAACgoKD/AAAAwAAAgP/AAP8A7u7AQADIyABBaeH/wCAAgEDAgP8wYICLAABAgAD/gP9//9SlKir//wBA4NAAAAAaGhozMzNNTU1mZmZ/f3+ZmZmzs7PAwMDMzMzl5eX////wMjKQ7pCt2ObwVfDg///u3YL/tsGv7u7/1wAA/wAAZAAA/38iiyIui1cAAP8AAIsZGXAAAIAAAM2HzusA////AP8AztH/FJP/f1DwgID/RQD6gHLplnrw5oy9t2u4hgv19dyggCD/pQDugu6UANPdoN2QUEBVay+AFACAFBSAQBSAQICAYMCAYP+AgAD/gED/oED/oGD/oHD/wMD//4D//8DNt57w//Cgts3B/8HNwLB8/0Cg/yC+vr4AAAADAAAGAAAKAAANAAAQAAATAAAWAAAaAAAdAAAgAAAjAAAmAAAqAAAtAAAwAAAzAAA3AAA6AAA9AABAAABDAABHAABKAABNAABQAABTAABXAABaAABdAABgAABjAABnAABqAABtAABwAABzAAB3AAB6AAB9AACAAQCEBACHBwCKCgCNDgCQEQCUFACXFwCaGgCdHgCgIQCkJACnJwCqKgCtLgCwMQC0NAC3NwC6OwC9PgDAQQDERADHRwDKSwDNTgDQUQDUVADXVwDaWwDdXgDhYQDkZADnZwDqawDtbgDxcQD0dAD3dwD6ewD9fgD/gQL/hAX/iAj/iwv/jg7/kRL/lBX/mBj/mxv/nh7/oSL/pCX/qCj/qyv/ri//sTL/tDX/uDj/uzv/vj//wUL/xEX/yEj/y0v/zk//0VL/1FX/2Fj/21v/3l//4WL/5WX/6Gj/62v/7m//8XL/9XX/+Hj/+3v//n///4L//4X//4j//4z//4///5L//5X//5j//5z//5///6L//6X//6j//6z//6///7L//7X//7j//7z//7///8L//8X//8j//8z//8///9L//9X//9n//9z//9///+L//+X//+n//+z//+////L///X///n///z///+2qOOnAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAgAElEQVR4nO2di5MkVfbXO+ef6I6ZiI76D+5/cf4Qn6HgI3wwgM+AQHQUdkN+y8w6E+sjZAEZWcBdVHRQWEIFdmLWWQQ1VGZlhtXxwQgCq6Jh3nvOufece09WVXdXVWZVnU93Z2VmDzNVmR++59583Dw4cBzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzH2X4eHvsN7Cqz84fd4YWx38XUeZgY+33sIEdHs4Pjzg2cy8OKsd/NbnFu1k/OH479NqbMwxZjv6ndwgWcg+mfW7hKZt4IHGSOfm7hipgdHY39FqaK4RtYEj429hudIt0A9Z+bHbp/A1iuQevgY8TYb3didP/PpBbwuDs/ytubPnbFVQKC9M8t1Cwn4MwPwQxg2oeI5Uo/t7DQ/V+TSsAjzz8TO/5qBwFM/1zCSPediRZwRg3D2ShvcbI8jujKW5dgUjHKBunbLVR0/8ek6YQ4DY9LGgH7+IPkYTFwUMDIU2N/nJHo/reJC7iIxw1kz5cETMlH2s0R8Cli7E+1ebr/ZeICzsfST/c9UMDHsoAAIguH/NtDCbtvTVzAuVj+JQe1i+QbtwCNBIRGv32z0AU8OcPmgVgDqFzWjAUEqV+/8BRYAu6Nhd03Ji7gIEPRB+mFTSQu5do7JOBTcwTcBwm7r01cwAGGay+oDLzUyxe/EvBYaf5BzsXcLCy2wVOmj98b+1Ovke5/mriANrZ9iBAwWXcpA2jhY1JE0SspDpoCfo8Z+8Ovhe5Lk0bAC34ybrjvoXsdqFsrYHKwJCBE1/BLJCA0An6vYuyNsGK6/2FSnws+cgEPnkaG6m9OwFq/YqHuG9cCtoFo6LdzGnYPTCoBzx0f7L2AT0uEe4+DEpBkg8bAEpDxV6nUNr7VCTik3w5p2P13k7YNuOcCPm0gBOTWH1XbUorrEExBeOmqKtlZudrHRf4hz4+9bc6EC7gMln4JaFt/WT9LwLQWrkbganpVTT6VgKZt0OhHjL2JTkv330xcQMmQfYWUgEW5OgKhCHjp0tWrSsA4MRIwGwdzBXxeMfaGOgXpMqv/qjAvyd9jAQfjLzqYPMRZceQltQFbAVP1vWoIeDVbCHgcRhg3JCA0+m2lhd1/MXEBGdO5VsBr12TlhSoDVW0m/+RCkRAF/J4tIP03LCAbB9ssYXffxAVEBkJPLJAU1yLpBa5VVVi4qEOvSkBe1xrXO8crRQI+P0fA7bGw+88mLmBiqOpWa64RJGDiUluCa+dMAZGXXnpJJmAjIP2x+QJGroy9CRfR/SeT+p6Qvbwk37IPUeuuCQF1HoreB/U9Us2dJ2BvXrQvMyBg0U6p2OhHjL0l59DdM/FzwQfPIa2C1Yqcecq+a5iFcO0aCagTjjxsGoIv1QK+JLraWUAUb7gEQ+XflC10AQd4TiJKbx1/pejSHNQrIrKNx6EHYoozpNxLDZBWqsI7JCDkFVcsxt6sDd1dk30X8LkWMCpwrVy1xIn4GgEq8UDa2M+03lUC4hyLZgoIWUBTvwlK2H1mst8CGvr1AqKEULohbd3VL2lO+JccfO019I5VZAFfAhjKPyXg65G64QdCQGKOgJOysPuPJlMQcLQLcCz7iOQhFP+aogvXKiOBveMQxNecgHXnt0SdxesoYHLw9bbRV+xjAeeK+INxtq+i+7XJ+AKOdgWYGX+1g889p3q9xTmoEjCpB62AuR6XvgfG3DwB2b0cgf2CEJBz8cpyAv6AGWMrZ7o7JuMLONIVYKZ5+QWyh68k8pE/UO61x2Wydq/JOZpH/QRFQGj1ex1TEEQWiq7J89E4zkGYJ+APKja/rRPdfzCpBbxw2B2e2/h727iAA9EnBUytwVeYaGKpxJyELCAJVxysbEwz7yAvyRKcRQTpHwgBoRRjWiME5OSb0xys9RvNwuUEvHB4fHDcbdrAjQs4VHtLAcYVrygB++kruQqX6ntN93zbBMT5dxiVf00CZvEYIaCYz7KhgFfADsEB/cbQsPv3JpWAaXTeC5seI3rDAi5o+VH8PRd9Y/uoGqOFuvqKQqsMlAIK/dDANM0CFiFBJJ/+UQL+FBHhZwuImgHMtfDZzWz17t+ZaAGPu+M0nW3mPeX3VgloXye2Iqg72hrIJZgEfEUBZUphWPnHKWi0Afv5d5r8i3N5gRJQdjt0Coo2YfEPQfWKhm38zRXwWWJdmzvT/VsTvZ/PdbMD1nCDNAKu8d9SJ2SHBVTe4QznIIjK/Jrhn46+FH9SwHekeriCBMyKCfEgJ2Be9dOfVgL+1LJPVN95Aj6rWON2P+j+jUktYJzOdlfAqwaNgCL9lICiDYgrf06wdElAqAUk66SAlIW0kOLwHaGYKLsqBEGkH5B9SFxUJbjIRyyh33otTGXtXyvaSrfjCWjphxeulAagLr6g50Gt+3kBUEPsANfxl8NO1N/SFSF+kajagOSeKME/1QKygWkRqFko4w8wAU0Be9kATAfXYWH3iYnVBkQNN4VxBdiaBLT9iwLiudom/mTaKQFB65cEVDpVnY95CZhmfkEkD6WEoDohsvKKEiwXI0KyOQI+O0/A1VvY/SsTqxc8+pOS1iLggH5CwITV99AyJgGVfYh0sOr8zhFQBWAUkGZfbwWU7T7MvSKggJuhov4aVRgVmy9g5MlVbf/lBIzHAc9t/DhgzRoEHLYPIQHfRLSA1AOWRwWr6ouTn0cJ04R+r6zTAoq6/E7Wr4RgfC2NP3i9afzFF1ArhH/yLIzdDyH7kDSfvlv9iBXsge4jk0mcCalZuYCD6SccFP4VCy0Bc+D9XFhnCPgxUglY1KOVRTkpIEgHU+cDhgTklcD66dOAKQy1gEWwnIBZQLD8W4WF3a9Mxj8XbLDqNzXXv6s5AN80qA7BUN9DJSDoFSzNxxn4WApYSjDqZynHS/lHBF3V+1ACcl88twK4MZBejb5vERBYQBjQ76wWdrdN9kDA+fplAS39erJ6FIal7Krmnwb6+AtJvoACUhZCLWCdfIaAxC24pbUDPhJTBFS5B/XRcEKVWKq/mbxmQL8zSNj9S5OdF/BHyMIKHBlyMH4nAXvVQAk4YCCwe8m/kojwsWoSGqUX+yGsIS727rGFt26J0GMBxSdYKOCPkVJ4VQIWDef5dzoLu1+a7LqAP5IMOnj1zUH7WMD4/WYS0GoDaijxQvQPgijIgD8h6RekfUU5UDr2+vXS3cIJCthPdBJS+gnZ6NpEklJo+WNJEVDkH87AQv8SF0+yU2+Z7LaAPzKQ3vHMm0sImKvxL2MCygy04u9jkYFUgjml4sp+be+fVFAmoFi6dUsKWIKwX5AtwXQKRgSdci8L2EsHSkEoWQhNOa5ta1ZcJJbcqb8w2WUBG/cgfWcJ6XbdbNaboK2rmoGFX8Yv0eBT6ZfjLyZgkG3AnIDRwPDZZ/23IaBaElGh7GMrKQFfywJCDjt4rVpXC0h/H1Vk7gajemCoVwt4UbF4p+6dgG34JQEhRyFQ+qFkWro3dSICNliSe/EFPeypiy/VWHQsJd3HH5dIzCH4WS9goPvCTAFr/bJzWsD0+pqIv0bAnIXCtyxiq2OOP1gg4EWLuTv1Q5OdFdCqvlJAlXq82YcETNrF3EMB07xExB8S4GP0j3siJf36nxh+IcibEw37Gv0qAW/dCjj94AOWTApYrZOOkXrQFOR+8ckBAXVZBlO/+RJ2H5i0+3oCg0SfXUDLvkyz5i1RZmXRzf5lAX9pC4iUOgupA5L8wy+Rg/BZKr5BGQhNFloNdqiWA/r3AaTvLGIOvarzIUswCagCEeDy5fidSnAWMKsnlJwn4JCF3fsm9b6exCjlZxXQSD56VQr25r31Vvx5C18HSnAWTErXCkhDTZCAQA3A7J8UMJCCuQqTgCQhDPjXEj4IgRJX+PfBB23bTwlYklCmIRIFvKzzri7BFy8uFDDyULVT/4VJva8nMUr5GQW0Sq8qwSBcFAKmn1rAbByipQtlWkY7gXvJiADcE5HHAkmzQF/JRDpSU2wMosIOA7c++KAXL5bgkBIQJUyFDQOxEvDH0rimHP8Y4+8yvcSZy5eNBLxIKxbpx+Sd+s9NjH295QLabb9GQCndW0JChAVUuhllN9CPHG4HEyncw54I5R8moGz2Zf/iFHjuMwq0zz8PqqGnKzAu9vkX9UuTDwwBkbqlx8blUi2zT3OZJBTFF8NvyD5o/csadv/MZNcEpCN7bduvXn6rkq+V8K23SvoNtvtC9K8Z8in0MRin90oF7gWsRkXhDCw2Ip9H+/qvBDX0VBMQ0goUD0KKQdH+0238GzduANy4oboaSjvpGiXg5ZKAvHRZV18YcBAG9It075nsmIDqBEeTgLWD8wWEjz76qP+xBQyBppZ/sQqTiOkr9YTB9o8iMPvXixcFDChgSD/hVi3grZCKfEpLkYCNg1G/RC/iDZmA1AGhElx0GxIwkR2bL6ClXy9gpJJPXZJ/3C+cT7t/awW0TrMZAkbJoIm7WsCPSMA4hRJ4aQpoYPIvGIPesX7pGy38rPHvs2Jg7o8k36D37/MQyD+akkjc+PsgUDvvAyrEtKLS7wYlINxg0Lgcg2mxKAamgLhQBg2zqzD/0vbvoa45Y5nYoQScc72B0u+tVsD8msszyQfRPiRlITX6IKVfSPaBMjDkadYv+vfll/33l6LZF2QGBl5NvkX5Ygayf1SO4fMUfRx7KCC6h+VYC0j2lQTMHooEhB8r2+ByO0cLNFYYPK1FVOkHQ/EXBXzHZHcEHPZPFWSyzRYwOYjxV4gOUhaG/jvmHnD9TWkXinhBLsTlNBvlQwOThSID+XgMlmHp3+efqyqMAqblXrqepF1KvdIPURX4BjtXHIQShDkCdeDxi2gY4uo8cCcPWde/tuUX0DUwBfynJrsi4CL9OAvbuINWQLxYXJbgOAdkIGZQsuAe/RUy9MK9kNOQ/SMD8St+K/+wAn/+OZkXKPPC53UVjpM7d/rGX+in8QXjjyfACvbh2DsWWMAcfUVAtpEOu6CASjthH9IImOaUgA9pAbWH3T8xqff1JEYpP7GAS+pH11S9mY0D7WFeGBTwo6hdXEjZFkJu791j8Yp/FIhJOTSQKjGHIeVeEhBly+6pDBQaphHNon5JwP6V/QtSQAjkH2dfki0vkJAlG4t3dgkuD6kQQ2iLMYxz+4+cswV822SUc8HyxhM0/rz6/Qnf1JL6XX1Tn3F7qxwIVAcD0bvAiYcGspdRv5AEvH//frIQSuDd0waSpF9S6vWLX4owRA+p/8Hq5ewLIedhKcR3yL9oHqRO8J20joKQ24C9W4Hyj7IP6y8XYhZQ324AMCAgx109aHs1iDaQc+xiI+A/NhlDQDUE19H59g+c7E0tH39FvnwNTI4/crAEXyj+FQV7/6KB8FFvX/qKr/frBMwNQux9BNYva/dlCNnK+GsutqXq6n4wSkm2pfrLCYhx2EMHZXIGQi2giENE3mFFS7rrwfo1AkI7iPYjj8AjPSUBi4fTE1ANwXVotDhP8qaWca/ccATtpS9pFQtIlZZ6HMU/0QaMP+khU2hfoEdOqY4vLXzJ4RclCyL42MYUit/Er2++ycZRuZX59znlH7rHAtI88GCjJOKNKgFz9MmSW9/vXO45rQ+9GEO2t8O4R/ceoVWsYZ2A/8hkBAH1EFxnE5AGMzCM04vyDG915R+UF3XcpRgYsoC0TP5R/mUbY01mC+8l/US5zQko9Ev5F91DB0OaD3zohdqBqB9mXZoG8jCggHcCC5hCMP7c5jag6nCkFxDlNzvYjkHCh16Uc9WC0C+lX0pAMX2kEtA46t8zgoB6AJru/FFXS7j0m1Ljq2kBpYNQ21YLSM6lsx0fyQQM1Oz7KNVd5L7Mv9QWvB9kIlIDkO0j0/DnS+6M5Pqb/cO5bzgNRRlOXd87+YczMCqXJzgTbge4fRsnt3MGighMgXgDXmkEbIeDgFca5+QxwBKBxTgxUxaygP/QZBQB45SH4Do6Pzs4PjxVJ8QaZ1IKSA+IUQXXutwU18grXkq1xTafaA7qxIuhV+KPJz258pZSG1TyYRJ+o9xDeM03LGBOOZF+GIhwh95y+nWU7jbo226FfbkiV3Enl6txYEsIgsy88igf9G9AQMlDB90/MBk9AcWqzFIDVJrjnOIoawkx2AaYxqklfaEppFMefRJiBkJuDkLt330RgiEn4m8j4bfc5y0pmNuEycqkHWdg1q+sTN9ySPmcgdE/do8SMIUehZ/lIBmYx1gSykkBuVGoj8OYAsa+SZSOm35ziDv175s0F6SeP2wq4ooxhuCqxmRd5v+KAf/UQM/knxCwbfuhfsaVzliQk3pA/t2/zwLK9l8IysWQ/AtoYD/lQhxC1Q4M35TwK/kXW4Ih5Cz8AhG9jNT2C9j5hTiF4h9EBwcEhBuqG2IIWO6+10dk1GVaKgHZvPn2Jf8Oup+ZNAeij/qKuOZzIWoIrlmcXNBvY7GAQ/qxgNnBpQQ0r/mLKQh9DkI65BaP/fVyZQFL/zek5LtfwjAo/zgKZSuQcvEbkXTBELCffoEG0jT7R+3A2OrDXvDt27n+Ngb2nRLAXKQhNpsxv0hI0KiTweJqQXHoJXc3FurX79S/Z9JcET07WPv4bHkIrt67WZfagNr4RQLOsS/xHD3p4zoibvHInRChZ3uJaZqieBACXfQC9+lQm6rCWHpDFrGY16+Nr4FlDHTwDw38JqBlgSttUPUX1wj30k+aig5wPBQYWD9Ounb4FRawHJKxS3BOwPZwIPCpunJshht/6bjLQv2WFjCx7gEC+UxIDL7jo3Y8rvkCDqeffMhCbx8LGIHr9e2/KRbhzeY603S9S/qDAU/4piCkA83QNv3QQGr/hSDyL+rHMYgFuH/9slRgPOryDfc4uBDnuvyFIBQHwxfcJcZeCaUfqpah1YBr44s6DtiU4Ly6uRKhETCRo29x64926hsm1r6erbkRuJC5As7z77nymJnrtYD4UoovCmhc6BwwBbEEB1wmH/HEr+785r5HfEXryECSEP0rifjb37Juqdhy8Q3ZP5z/QpOSMARVkFP+JdXC7VrA2CGJRZli+7Y8L5w7JG0CtieCy0UKkC8XvPwE8cgCAeVOfd3E2Nezo6MVC3VS5gi4QD8WUFknBeQ0FMf+mivt8WpnSFecpmk+1QbAJztKzwMFvJ/sE1/U+suvGIeR78J3/Xcg/74RBorpF41/KQFDEEU5rgukWisgrib/IB+SlgKyhKUL3JwIrrshSr8nnuDVC/WjQxt6pCT7iujZ4dj+DQvIzd/FRRgqAWkJrstfF6oEpHuNQkhVGM+sAf3gubbAUcgK3s+NPc4/0Q1hFeP0u16/kH5CfBXJF4R/qvMh2oBRwZD9+/TTT9NP2+7D+kuX7d8W50T4Ai0FCYjigS1gXpvEUw7mNBzWr9+pPzFp9vVxZ1wdsGGGBHxaYwv4XNKsTsDrZS29vPfee9B/2W3AgP6hjXh9AbCF97A9x6fc+OgLx1wuu0VEwXffoYBJwSRiWsHdjpJ/svMhvwNZGH/x6afJwJAstPy7HbjwlgsTBuHkG4o//AVZVyzMAlYaNjv1VZPmOOD4l6MOCfi0TS1gTLbrcF1GnSXgeyRgnL4njsEEgF/mOz5iIQ58jTP9hXSdKQZjyEsYd9TWq0JP65eMw5+k4Hecibr+8jQUD7HoclsQvaMMpBlxGCa1//ppuQx1rn8vvKA8k/7lAlyqbxGQfvWEptGv36l/16Q5Djh+/tkCDuiXz1Wq+kvqSfuEjoD6oYL4E+EBOIA7Ini1n7zlA0oC3gvlqpev4oqvvkrpF0LQ0gkPhX+Boi+INTgxOh/FxWxhyN4VC2kRJYzfMQFD8i4sEvCFBLxQVLMEVA2/XIdr+554wtypr5hU+3rWTfSK6Nq3RsA0RbuyeqohKBQsNwaCmokFOQlItx5hYy8U/SB2KrkEZwO/6uXrv7+K09ILVgYGqR/nHxnHszj/dUJ1PoKYz06W+ivARYitwth4TcU3XhnIV+cPAS+wgDyJFAGxDVj3PDj5GgHtnbqUgNOgeVNt4FkrQDw6uhFQdE7eawSMtVhs1YA3HcG9Un9ZwdILDvRbli+9YhRWDb8k5Hfav+848UJWMX5//XX4mhwMX1Ot1f6l6afx3ynJJw3MQRg+pdob1NX5RjUuzmkB0wLXZW2ZEg+W0K/fqX/HZBsEtCquXiKu5aeaGwdgAK7r+OP/KjcGi4X9PN7vRncVFQPF/W4Yjyn2soGBFOyhkyE4Ve4F4V7xL3VKknhfs4Ff9yt07qWfXq8gPGsTEH/zPkAcfuqGvjq/FbBx7oUXDAXhica5tvQO69fv1JdMpi+gYR+iRATxCN/8KN8Sezn+qvQDqSJZGKeffJKSMOgmIAkYRBHG/JMGfkUCciCmRqGuvlx3Zf3ti28IJf/QwIA+ql5xtEzW36DzL2CE978jAd8P/ZctYDwgkw17YYjYMHwmsYSAc3bqiyaTF9CIP3E/DL9y5c3PksYfiBKK+ts0+0Do914REJ+jF0Ka3GsJ2cHc+MOZtKIqyVkz3dvg9h//6mtZf0sG8gw3C6V2VRXmxdgGhCge+cezuf5yNQZu/A3bpwR85hmIFg4zd6fWQyQhExfQ1K8WEJ4u0pF2+AM47S3EEiyafVLJXIJpnh/kGMS08e9eyMW2bgKSi19RQf42Er7VVbjOwJx3ZgJmC/mwX9X008CnAbULlH9Zxvff1wLCfAFB1uBneJNhFp5Qv36n2v/GpAV8nBgQkEuxKr0lCqHM4nqr49sKWJ4kCtWTRSsJk1/KwJCMy/JxCf5W8F3QLUC0kfQSNbioV5wMd++GcLf/CnP9+5TvDUneQRDDkQJbKG6OA0tAKFMKPpzkKGwtXLhT/7bJlAV8XGM0AWMCYsZdkwkoqL0UucdGKv3e+8SkhGG551dWWxLwq9ICpPgL32oD+/kQSv3F4sshV3rBsvjmTET3yMJh/1LcAYZeeF/6169NCrJ5suUHxgugfkMCKgsX79S/ZTLCFdGLSW/qcZu67yFT7lpZ0hrCtfzg+/iSe8FNFNr6kXshe5jaejnmSitQGcnxJw1Mc0nLb0X1LU0+KryyJcjLd6N/aZJeE7nmCv1k1tG1MAMClgTElp4WUBTcZ3T9bVms39ICHm7giuiFdIP69QI+rhLw2rWSgJyDFHvsXj+bHwGdBMTH75YIhIX6ZfvSz10SQHQ2vqq6wygg5l0xLy2nVUXJrw1CNR9i9U3+Yf7dzQaihJC7IJR+udjKBKTtkeZfhXgetl96Nc0l/bKAWUyt24B8Pcvt1L9pUgt4PDsY/4nV3bB/vYBxkqaP69ArRwFVJ+SaeOC9EJCLcSnK8/QTVVjse7SC+h6BD8MgUTUULWT/uAIX/x70hAcPtHRB9zzid6q9ybxUhJWBd+/CXTomGHULSb7APeDYBoRKwVdRO5wCmqgjEVLjb0kBl9ypf8PEviJ65Auyhu1DUMBLEbgEMgFBHAXkaIR88SNvYbnQRyFQ/EHT7WgS8BP2L/CUegUyCtm/b7OBsgaHUFY8SAIGnOjM+zpQt+RrjD+quyFU8t3Fj5H+DLX7QPiXGoM5ElMJTs6RgISYeVW0/pYScNmd2v11E0vAc/KOyc0zP/0gTqOAyb4oYJwIAXU5vlZdgCsdJBNTIqJ+IIVTU6q/d3P7KxfCIFKJLZQ1V+QfNgBLND54wAaihA9iFlLyBfqmxh/7J+yXDt6FcDN+3bwJOEK/6vgCuxcnKfSgXA8l7MvJ+Got2xnTL9IZo8f3tAIedd2Yj0y3a64SkEj28WxKQ1mMr+nwywJyLVYCvtvT/5dJwCQhyG5HyCLeFe2vomLglhn97tuKgQRk/6KBOB9yQcZCHP27y7nXaKcFvMkG9g4GKR9DGfhqIyDQGoAs4O8giwU8wY5NF7nofBgYI7oZqGCTDDT6qgS8BNK4SwV5QIazDoYEzDPkH0ECZvtQvn5NFUDCP9k3+A2i80+1AZOGD8KDUAso18UQ7AUsimfJc8NPCHgT0L5kIInIAhYJ3xdltxaQZqR/wkJbwBPt2e6vmZhtwGqggs0x2Oso7b8SfzQj/aMFqsS531FlYLVC6hdn+6934ZPaQKgCCJJ2QH1T7qVG/0ISME3Dt00tpu8HVHYf4AzPl1SkyiyrfdX2K/pR2SUD02FojEIRf7mN1yiYg4+ahlk94FfkTPotK+Dswuxg0wLm4SofJoxeh3QRuPWX24C1gOlFdHybIqz0Ax2AmfBJ+YKmAKIRgOFE07vFPZzrX8Jv6oqcOh8p6wL7RxkYRAI+yLf/BjsFSwLGogs3S/714vXT/Dlucgq+Wkwr9rGMPCOyD/TsmfTrBfyhSS3gYbpPfJO9YB6u8uEKdk7Yh33fSkBtYCnMaWCi0tcdEPBdEjApmGZTDvbf4d34E9BDPKAr/QOe5j5q7CqgdYH9+w2uCbogU60NrGGeck8E/wAPhprQTU3q+gLq1xt3Mwl4U+Yers4LLCC8WrUB83zS8HeGBVTNwpPv5+6KiXkmpJdwc9BwlbV/RCUgx1tVjmtAjhEoa27lI1dcFPBdEYXJv2RgnMasU+VPJWU+GoO+Bf5OAoaSiNT7yP4F1g2DT/RE4ovyj0eEETkMd1k/LWDx7ybIKgzwRqm9toCILLvQuhgtPM1+7up7nZDRzwXjUEUD+vUC9l+YhoDFt+p7GAl46dJVLWCaNRJQ+qaTEKMwzgc6sx9U/uX9D1yN720rZ0gAABcASURBVGLqpfALaGLARcrDQAX5QbYsZPP4qyRgZR+NixULsnoDNw0aAW/eZPPgjTfizxvKv3xSRArXGweVdYXT7ejueZPRBYztzUH7kDKX4y57Z4QgDcwGRT92sen6igQs+kkhw7t4fAPoCMfNm62AKQHRrt8U7aj+ooHcHAy/RkSXozgo2oDZvzIu4B1MQC7I6Z+2/EMHlYt99PXWpR+x/MarmuTbMgKeckd3PzCZgICD6YcJmOcefbT/frQWsEnANDQlcVXNckUW1bfJv1KLsR0YUySkChfkLhYCpjYg+YXC5RIc8hf7xwb2lPr7oKRgSsUyGH6o8y+IVBzSD5Ov5CHLl4ZiSYs0/4bU8HdYQCj11zLw1Du6s3UeX8C5/hUBH00CRgWjhVm6OgGlcsDDRVdDRveU0KsS8F3dKy4pQgJWGvKhP2r8UfDlTgj5R4U52icMjPNBV+L4faf1j1qAd+4IA/lMx1ICCulyS1ANESTaeajdQP6dYUd3f9VkbAHn65cFfPRRJSBaqA+9oH4i+uJXHX/IywjUur2rVIR3xQ4l94Kxu9E0yF2OkA/DBGlgwPyrDUwWckkuAgao+yCa9zXi/w7RBryJRVepVidghPoDLCAMCniWPT1JARfpx+mXkQKihOJAIA8KfVVUYSsB0T7k5aJb5d+7omN5k/av4R8NqpAEFCn4G9n7jfaFX1f24TJp+WsqxiGrBnUCShflRfZ83RVbKBx8o067Kg21f2IUDlvAs+3r7vsmYyfgwcGFpf2jraN17FOxxJ8cIl+UYCv+Xu7d49mXSUDZGLx5sxGw8g+EfziqAndCcnckt/9+HWr3Sgaigf0fgDuhtPGg9EHu3JHz/RxdaFUZyEpSCc6yCdtaAeEN/XSkNTT+iO57JuMLiGdClom/JgFBrCmyAehKbOqnBUz1WApotazqBISiH/1zOKZHPg3CBIo6YR1NKQHTbPgV9D+/wscv0IO4ci9YVmGWTQkIMhLjiY/c6aAItEpv/HX9eC76D7ggr0q/XsBnTazxAQ/Huh51kX6NgDkTn3rqqdq5cvRFPDXkZbKOC3DxTyFPLAj/dAZCGc43D3PJQ2tJ/7JslXulDRi/0misAL2Bv+rhp39o9XLjL7yvBIR03Wlz7TP1eGFO56MVEEfhYFbn30FnX09jCHi+G/OC6Hn6KQGLfVG/BKRpqcRCQOD4g5KAmtwofLkcSKv9KwbiHw7ZP3qEMI9vGaggZ/9AGJhDr3wn69KEgWhhXXzv5HJL7zYLyPdepjU/+clPoP8iMP50Aqp4BFDRB/XwWKvQrxfwr5i0Ap47vDDyFfkH3ZB/ho6Q9UMBcxA2Albe1QJyTQZKjcq/WkkazI39K6MMhqJiD5ddEP2OnIS8slcvFl8IKB4LmCSU3Q/yq2kD8ji8uBKtEwL23xR+UkA16ppIwGaAwFXt1L9s0gg4647HFzBNF+lHAj6lBRQv1CJkqsprCyhrVdxLAwLqzCtUY05/SAgBcwYWAXv74FchTYqALGFcV+JP3u4Bqt7y9afZOhDDkEL6VgkoG3ztQ6pXrl+/U/+SSSPg4YWDiQiILBDwKQPloXAwSTecgBh++rwp7bFKQDmcZVAjrPJ/rPT7kFZSbzfaB8nAJOCvIGkXqArXAgauzPka51q7hiwdzYi1vZf5A17WI++aAq5ypy4nYLwfaVICIgMGWvrVQRhnXi6pN9AJib8o1Sk33/n0AaL8+yUNpxqUgCUBP1RAnIT+O+QmIf6dudgm47CYptgjaaltGB8uXqSrZeTbPoR2P5Ee9t5lAeMaFX1W7q1cP7ok/y8qjEvy0zOzJihgYjn9SDoE57//fUg/SrqmE/KGErAEimi2x4nQLT6+IY2m38bfhx8aAvb6hfRCxRfYOlIxLbFySUucCfkB95DnKgHfp/LLouXUYwFLZeZzvqzgoIAr3qn2MD/Vvj5PI6SOe2f63IOTy+gnEzCb9/1+Dn8MAdmwop3SLwvIjziTwkEZZZ8TsPaPBPwwiIr8Ycy9Yh8U6bg7TKveftsQkOdzLKJrAEYCQgue840ubka/fqf+BRNrX081ASUL9SMB6YQPlBeeZRGTfsI2eKMUXnXKIE569UA8aU8cgZZ94UY/JkQDITn4ISmUS3AWsByOgUo/LaBaWVdeLLttEJZzbCxgYu369TvV3kvbKmBioYDsWpWAwsNIc5rKErCko3zWo9ZvIP1QOcq/QAIqf4SAotsh4s8Ur/wFNMCQqrzU7hO1l/QDuuQAg1CyRv36nfqkyVYLmJijYHGMddMuJmibGwLqg7c/o/SD2j9UkgW0go8EDJiCSkBdGPlwdDYwmgcLBSxDXKnEqwQs5Zeuureut4oarmmn2mNaTuBccMvJ39R8/VTele4IkH/YFYzTNwyg+MfCaQdVIlZtPp2Apf2XoGgDEXLsXuD0W0JAPawfqCwE0SYsl7jMFXAN+zPR/XmTHREwMqyfFlDY9/18+Et0BS0BY/ABTpRwOhDpvnctoDCOVkjxeg2lgG+jhaQf/iHp69tQq1iPKymGtwJxTMbIPlvAFe/Kwu4LmDD1q13kEkzGGQfDxAEZSjwyD4ZKsMjH19nC2sG08HYRUKtl1FadlpaAzcCmYoTJOM4VCUhHXsThl83GX0/350x2TcDEHP1yHgLHX6HuCdIZe046qHyTCQgx/mjQBXEzcluCP3z77ToBLfX4CROlBHNOqj+s1UO4NYg/CR14q77dbUm6P2uykwIi8wX8vhBtSMA4wY4HUxfgLCCP+sHulZs/UT6OPyPc6lCDt/WTndBReLsqxjr9xKDOsjsCeZTdZxYLuJKtPkj3Z0x2WEBkwMFatKFHRP7wh/33D38m2oDzBRwYD4QErETTVTh3jK+LR43hsxcxDUsCglV8lYBlzFM1vNWcArzCbW7S/WmTnRcQmaPfgIBpnRjDBH5oFeAkoEozsARskw7Ud7YqVl8l4PXqIcdZQGUfhx3oFSX+5PBWq77dbUm6P2XSPC0znYkb+ZGZ6/q/wvavlNt63Q+1gHFSB2AvYPENdArW/uGjgbSAUAkI2TWtdXniWJq+UGM8aAHkGM/17Kb163fqYyZb87jW1WGoZoHFV5PWiAzk0GMBQaqoBgNh/ZhyeAUFLOJdb6XTD9+m316+PCSgOBKDxskF6eIm/VtWwNEf0RDZQLtgCf8M/SBbyFGohMtVWA9C2M++qATkRMTsYxFj54PKL0bggIDXL6enV9L7zEW3klHkn/2Aj2zh+jd2ortoMo6AeUjAA7Pob6pheiL9ciKKYDSyDpohuF6H5FyVgEJErr70VPeSf0VAqd9lJWBqPdT9j9T3UJG3gkGez0j3iEm9r7vzR+t/UA0PCZgwiv5Ge0bL6ofmkR1pOY3uBM9XLT3RHxahKBqBIATE1zit2nylIl9X+YfK5SpMAsZV1ZPdWMCq/o7m30H3J03aNuBs/WNE05CAYqF6r+v951uW0K+YxwmY7IvfEZmA0kXg6gtV+NGEBLxy5Ur8wQSUD9euBLwsj5yXBMwr6GQINf3aDsho+tEV0X9CMTRI+bqH6MUhAY/pH5mCgIn58VcnIMUfC4gWiuHPqwQsUfciqPqb1l0hAVX1zQJmHbNv5d5JVi9f2Rxnq+AbP/0i3R83Mfc1u7Em0O9jehqJUfRHPDg5rJ9qA5JxSkBc0F3fUnpBdj9kMX5RmHcFv+JEhCD1h0XgqZt328Idr7NbnIAb3rLdHzNphuiNkwvrNeBc+utnJKBR9Mc+Or5QQBQukwVkFUlAmXMv1gmYSzDgYMn5hWW8UhKwF1AHnjxuSfe3kZPwJNClnr2Ig7V34/r1O/WPmtSDlHdJh7X1QlKd1wmY0EVftw1GYjgIn69C7/lGwDR9kfsZLKBKKRaQbbsiPYQrvAbKYb+SgBIoTvY/T4oMBDJxEvotK+DB8ZE4QrImUD3lnC76o8uXMfyTAxwXAZssTCPvKN2ycqJDgvVWJuCVSsDaP+OcjbzF/MmSgJLR9et36h8xGWVfp4TlR3LO4kQX/ekIGBnUz05AMiHq1zoIousr408j1qOE6QZG0QBsBcQwxNJr33xRLBxnM3Z/2GSUfR2PA56LxwF776yiPy0BE6Z/jY08hyOPAU+AJlWTkAQDmXxqLsub76KFYlxdlOFJLr1DAqKFY23B7g+ZjLOv+UxIDD6j6E9QwMQ8/YSAz7KAUAmIa3P5rQtvI2BS8GnhHz0qvhIw68etvzmMt/G6h0wmua8n+aaY+Ram1h+U0itMFGv72Ua54QRk9VQnRgn4JDb9Fuo3on8H3R80meS+nuSbKnTzLBSDf3LmKfHQTvJhqOVXrWIBGw0pCJ+U66apX7/V/oDJJPf1JN8UUw4QLdKvRCDpWBqFoq0mMw8sI1HB/M2FGLgoP5mr7/wEHHOj9XS/32SS+3qSbwqpj08O6VcloBQw6yLkMAREvWIbUIZe8k4JGK9pgov41002/g6WF/B4A1fDLGKyAg4cHrf9e/bZSkBMxBx+lS3wpBQwdzrwq8SfElC1Ci9enCPghjeUQff7TJpTcd2FdZ4JWY6pCjj3fTX6VQLSMslWNdhyLGr/ZPrp4ENS+GECAl/hOU39+o33e02sASoPjo+tv2FzTFPApc4OGhpKAXPaWQmYV5WjLqUXAsLGol8W8CLIy4ynp1+/+X6PSXNB6ppPwy3FFAU80cnpIQEb4+QihyBJJI78ydQrRwUvCgG5CFsWrm2TnIzud5s0FyOc69uAY9+YNEEBT/OWKv0s3YwEVBINC5j/CKiXmqno12/B32XSCBivlh/bwMkJeJZrc0z/jATEVY1BkLNQ1OSn9R8YFnB1m+DMdAPkP5CulJqlwXnPjWzAxARcxaVhrWymgHaMXZQSgmz8LRDw7O9746Q24JqviF78Jkb912tW924WCTion7QQpHFm42+L9aN71DwBC6u+MtaIwjn6GWpBFXlDqbnat70x4t2S3gbMrOnC7CX1M+x66KGHIP6U5Nul+IucO/QzIZm1vpFlim9tF5sH0cM5CbjO970HTETADdyXMj/+6gYeXkEH+AO45PqtgUkIGPXbzL1RQ/FXxRtfwllCkHH9VswEBNz4XXkLBcy6UQLWuH+rY3wBR3oHwwJWthkCooXjvO9dY2wBx70p2QjCRrQBAcd827vEuAKOf098JeFA2rl+a2NUAyagH+H+jcWIDkwh/hSu3wa40E1kdKzJ6ccs1G8Sw+lsKbOjqQg49T24KP061/A0nDs+mISA27LrFlVft/DkTEDAbdtnCxp/buGJGF/ALd1Z8/sebuGyjC3gLu8mt3CYPC56LeBmt9ke7B+3cD6jJuC+7BjvIw8zooB7s0voc7qFFqMJuKf7wi2U4MPiullZs6lts987wS0cZEObxbe+W2izkS3i2z3jFlZsYGNs7o6PbcEtLKx/O/iWtnELE+veBL6N5+IWrvfT7/nGXZK9tnCtH3xvt+op2FcL1/iZ93J7no09tHBtH3ffNuSK6Dfbflm4rk+6P1twpeTNtjcWrudD7se2Wzv7YOE6Pt/Ob7SNsuMWrv6j7fLWGo3dtXDln2o3N9Mk2EkLV/yBdm8DTY1ds3Cln2WntsyU2SELV/kxdmSTbAu7YeHqPsEObIwtZL6F9UAsE2RV1rh+IzJgYTsS0ARZkTeu3+i0DrYjAU2QlZjj8TcBzJ2wFwK6fhNgYCfsg4B+x8cEGNr+uy+gqzcBhnfCrgvo+k2AeTthtwV0/SbA/J2w0wK6fhNgwU7YYQE9/ibA/J3QjgQ0QU6pkes3AXZiJ5zuM+zCJ996dmMnnOZT7MT/edvOruyEk3+MXfnkW83u7IQTf5Cd+eRbzO7od2Kfdumjby07tQ9O9GFcvwmwYzvhJJ9mtz75drJj+p1Eqp376NuI3AcXDrvDc3Jh8mc9LJa1yvWbAJ327/jguCMDz6eFbTRwSa9cv1HpcJxttS4F3oVD+gNx4fzhxt/Y2VnKLI+/8akv+j3ujtN0hr/dZQFdvwnQ3nHUzQ5YQ6rHpUW4RSyUyy+3nwDGPjiXVs1IwIPz/DTUbWORXfH37uC4mJtfJ+DR0axPwG00cL5abt4EsPcBqoca6oXtYp5hrt8EGNwJh6LfsZsCun7jM2/cl77fcS4eB7zQ/5nDvgTPjo42+M5WxfANfe7f+MzdB3wmJAo4O98vnJ9t5l2tlIGP6PpNgL3YCQMjOmz4XTgte6GfrdqefPRJszf7oP2ce/PRp8z+7IPmk+7PR58u+5QB1Ufdp48+VfZrH6gPu18ffaLs2T4QH9f1mwB7txM6Y84Zi73Tr2i3hx99clRX3Is7Po6PtvSOj8XgZ3b9JoDaB+qOj1l34eB4Rw3s8sQZja590Ie64+N8vMrg+Hjj72sTdH656RSo90F1x8c2Xmu/JK7fFGj2gbreedad69uA23i58xy4jbsbT7zbbhbd8THr0i1HO2WgbOO6g6Oy+I6PWbrp8txO7SPVxo24hSOxxB0f2AbkBuFOoNu4Gbdw0yx1x8fBUay+O5WA+p4+jVu4MZa94yM2lnarDVjd1dxiHJlyVsz87Svv+Dg4t61jXw0xLwEFbuEa2esNe5L7SN3CdbDvm/TwhKMpuYUrxbdlbuOeBLdwRfhGrK72OQnCQrfxVPhmOzveST41vtlOjQzNOBSES3hyfIOdHnWJZBqPLp6idAtPgm+pM6BOH5+bHYhetIfhUvgmOgvG6ePqMI5bOJfhOz76Fs3hNo4vvlnakycz6+yQW9jQtbc9qObMQRzh2QVcRHP6eM54iG6hZv4dH/FU7wUXcBF1As4OF4zH6RZm5t/xEf+vdgEXUp0+Pl7uoQBu4cGiOz5SHrqAi1Gnj2cneS7Zflu48Bkf8aZLF3Ax6hLJoxNfHakt3BsfF9/xkSqxC7gE4hLJGR7562Yn/CvYwn3xbyD4VXPmPG3LnbrgdGqow177U5EHP2VzNZwn4FoxDnvtwcmTZe/4wBUbelP7yeBhrx228AR3fBy4gOtlwWGvnbRw5z7QNrPMYa/dsnCHPsousPRhrx2xcBc+w05xssNe227hVr/53eQUh72218LtfNc7zikPe22hhdv2fveEsxz22iYLt+aN7h1nPewlDltPeCdP9505K2HaYTjht+acnuZxGRO1UL+n+g7W3Rraap8YeFzGhCzEq3v0m7HvYHW2kHmPy5iKhe27mHMHq7NVLH5cxgQsbP79xXewOlvCko/LGNNC84r72cESd7A60+ckj8sYx0L7ivs4Xe4OVmfSnPhxGRu20P63TnwHqzNVTve4jE1ZOPSPnO4OVmeKnP5xGeu/5n/4Lz/9HazOxDjr4zLWN57rvL/ujHewOhNiFY/LWEMYLvjrVnAHq7P1qLN4F1Yp4dhHHp1tQJ0PO58WVmSh6+csgTof1oluwRkt9PhzlqE6i1dfk31aC10/Zzn00eBUj5sHo5zcQtfPWZLqfFi8F8o+HnICCz3+nKXRCZguyptzCHGZIzWun3MC1PmwJZ8OOs9C1885GfJ82NkfT+v6OSdEnQ877EvwSS6KqiwcvuPDcYaQZ/HSnUHnZyf7C/J4rnPu+HCc9bLgjg/HWRsXzAupjDs+HGf1zI5sAds7PhxnDZw7PhgQME5nLqCzdjwBnVGZ0wZc7pii45wF+26OZrRDx1kPtoD5CLfjrJeB+9n8TIizGfyGSmdUXEBnPI78dkrHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcRzHcabK/wfLYnsYPmGvdwAAAABJRU5ErkJggg==",
      "text/plain": [
       "#<GnuplotRB::Splot:0x00005643514da0b8 @options=Hamster::Hash[:term => \"png\", :key => false, :palette => \"rgb 34,35,36\", :yrange => 0..1, :xrange => 0..1, :style => \"data lines\"], @datasets=Hamster::Vector[#<GnuplotRB::Dataset:0x00005643514d9d70 @type=:datablock, @data=#<GnuplotRB::Datablock:0x00005643514d9cf8 @stored_in_file=false, @data=\"0.0 0.0 0 -5.3804889953989\\n0.0 0.1 0 -4.697797701168377\\n0.0 0.2 0 -4.007314559046009\\n0.0 0.30000000000000004 0 -3.4128506438786204\\n0.0 0.4 0 -3.0117494833949117\\n0.0 0.5 0 -2.8726219112575238\\n0.0 0.6000000000000001 0 -3.018883295377965\\n0.0 0.7000000000000001 0 -3.4231302719426195\\n0.0 0.8 0 -4.014088650727043\\n0.0 0.9 0 -4.693912745674385\\n0.0 1.0 0 -5.360517598883003\\n0.1 0.0 0 -4.75056411378853\\n0.1 0.1 0 -3.8500796487239928\\n0.1 0.2 0 -2.951715464862632\\n0.1 0.30000000000000004 0 -2.1851837642331877\\n0.1 0.4 0 -1.671372608981902\\n0.1 0.5 0 -1.4951743128466948\\n0.1 0.6000000000000001 0 -1.6854435099934637\\n0.1 0.7000000000000001 0 -2.208158651683583\\n0.1 0.8 0 -2.974876257839008\\n0.1 0.9 0 -3.863807610171185\\n0.1 1.0 0 -4.747105917493\\n0.2 0.0 0 -4.119931541851159\\n0.2 0.1 0 -3.0153434663357253\\n0.2 0.2 0 -1.9230453257853326\\n0.2 0.30000000000000004 0 -0.9964840656171727\\n0.2 0.4 0 -0.37805898960659245\\n0.2 0.5 0 -0.16751626217290916\\n0.2 0.6000000000000001 0 -0.39867574220662316\\n0.2 0.7000000000000001 0 -1.0314929180806374\\n0.2 0.8 0 -1.9618669478940456\\n0.2 0.9 0 -3.0461230799848518\\n0.2 1.0 0 -4.132779317079729\\n0.30000000000000004 0.0 0 -3.5844167360457395\\n0.30000000000000004 0.1 0 -2.3151356112219124\\n0.30000000000000004 0.2 0 -1.0669265720757135\\n0.30000000000000004 0.30000000000000004 0 -0.012026141087255304\\n0.30000000000000004 0.4 0 0.690178031222028\\n0.30000000000000004 0.5 0 0.9282195884071403\\n0.30000000000000004 0.6000000000000001 0 0.6643521061866391\\n0.30000000000000004 0.7000000000000001 0 -0.05664991424234245\\n0.30000000000000004 0.8 0 -1.1183498079703451\\n0.30000000000000004 0.9 0 -2.3597866731975365\\n0.30000000000000004 1.0 0 -3.610745301644222\\n0.4 0.0 0 -3.2322316115923604\\n0.4 0.1 0 -1.8604101878486787\\n0.4 0.2 0 -0.5155355607492158\\n0.4 0.30000000000000004 0 0.6187177440969442\\n0.4 0.4 0 1.3726483898166482\\n0.4 0.5 0 1.627678745024273\\n0.4 0.6000000000000001 0 1.343667690667992\\n0.4 0.7000000000000001 0 0.5682335312521936\\n0.4 0.8 0 -0.5747260568812793\\n0.4 0.9 0 -1.91375620349783\\n0.4 1.0 0 -3.2672150519884537\\n0.5 0.0 0 -3.1236247138840065\\n0.5 0.1 0 -1.7263956129901423\\n0.5 0.2 0 -0.35804625337345986\\n0.5 0.30000000000000004 0 0.7952133980771627\\n0.5 0.4 0 1.5614286010402418\\n0.5 0.5 0 1.8204719840222712\\n0.5 0.6000000000000001 0 1.5316647990306098\\n0.5 0.7000000000000001 0 0.7432128898521411\\n0.5 0.8 0 -0.4193874794291732\\n0.5 0.9 0 -1.782383397282521\\n0.5 1.0 0 -3.1615629311619755\\n0.6000000000000001 0.0 0 -3.2763532661486434\\n0.6000000000000001 0.1 0 -1.934740528392346\\n0.6000000000000001 0.2 0 -0.6197035535452473\\n0.6000000000000001 0.30000000000000004 0 0.4892814776463128\\n0.6000000000000001 0.4 0 1.2263943195887617\\n0.6000000000000001 0.5 0 1.4757493113540372\\n0.6000000000000001 0.6000000000000001 0 1.1980757184123298\\n0.6000000000000001 0.7000000000000001 0 0.4398404804573506\\n0.6000000000000001 0.8 0 -0.6779394519322235\\n0.6000000000000001 0.9 0 -1.9877289613456148\\n0.6000000000000001 1.0 0 -3.3119691522857657\\n0.7000000000000001 0.0 0 -3.6614525397406763\\n0.7000000000000001 0.1 0 -2.448386936692312\\n0.7000000000000001 0.2 0 -1.2558677038882085\\n0.7000000000000001 0.30000000000000004 0 -0.24826726400343002\\n0.7000000000000001 0.4 0 0.4223129372717471\\n0.7000000000000001 0.5 0 0.6494755410811459\\n0.7000000000000001 0.6000000000000001 0 0.3971340721417649\\n0.7000000000000001 0.7000000000000001 0 -0.29206674661822607\\n0.7000000000000001 0.8 0 -1.3070648878739295\\n0.7000000000000001 0.9 0 -2.4942255637615536\\n0.7000000000000001 1.0 0 -3.6909682816742255\\n0.8 0.0 0 -4.210606100588294\\n0.8 0.1 0 -3.1807446187970787\\n0.8 0.2 0 -2.1628541252962528\\n0.8 0.30000000000000004 0 -1.2998087293305973\\n0.8 0.4 0 -0.7241744573057822\\n0.8 0.5 0 -0.5288219280615944\\n0.8 0.6000000000000001 0 -0.7452719158356675\\n0.8 0.7000000000000001 0 -1.3362821821858555\\n0.8 0.8 0 -2.2049293126249774\\n0.8 0.9 0 -3.217351464533143\\n0.8 1.0 0 -4.232300811441176\\n0.9 0.0 0 -4.832759791621209\\n0.9 0.1 0 -4.0162781333364626\\n0.9 0.2 0 -3.2021945647158336\\n0.9 0.30000000000000004 0 -2.5081345767821377\\n0.9 0.4 0 -2.0436833100824963\\n0.9 0.5 0 -1.885773390986309\\n0.9 0.6000000000000001 0 -2.0605206132377\\n0.9 0.7000000000000001 0 -2.5370070243146072\\n0.9 0.8 0 -3.2349152150323506\\n0.9 0.9 0 -4.043614612982552\\n0.9 1.0 0 -4.846908070984679\\n1.0 0.0 0 -5.434867033697086\\n1.0 0.1 0 -4.836253114068097\\n1.0 0.2 0 -4.231019600721667\\n1.0 0.30000000000000004 0 -3.7105939421455347\\n1.0 0.4 0 -3.36065436170298\\n1.0 0.5 0 -3.2415199451228816\\n1.0 0.6000000000000001 0 -3.373649051247727\\n1.0 0.7000000000000001 0 -3.7326823786548022\\n1.0 0.8 0 -4.25556439085921\\n1.0 0.9 0 -4.8558005254982035\\n1.0 1.0 0 -5.443173103572279\">, @options=Hamster::Hash[:using => \"1:2:3:4\", :with => \"image\"]>, #<GnuplotRB::Dataset:0x00005643514cfd70 @type=:datablock, @data=#<GnuplotRB::Datablock:0x00005643514cfcd0 @stored_in_file=false, @data=\"0.9218440427570344 0.6528144466877661 0\\n0.9505665275752514 0.39477630375342654 0\\n0.2085951352913018 0.515909535904487 0\\n0.17733911362973376 0.7991860095162554 0\\n0.8591698256344152 0.9862978883435362 0\\n0.7594646957604924 0.7267611545711008 0\\n0.12266255673819748 0.9332501681271399 0\\n0.15480095358638535 0.2421702375387068 0\\n0.4376583047255461 0.9852005760926856 0\\n0.08865178952488362 0.46888893577845503 0\\n0.9085896680314479 0.62571367967711 0\\n0.7021462064480223 0.9877178988491825 0\\n0.9652681483773536 0.5047445976899271 0\\n0.7634704518313434 0.16557927644043768 0\\n0.5560183393705129 0.09031193129246695 0\\n0.16309146930896745 0.62770118628935 0\\n0.8864034783299267 0.10675910752549045 0\\n0.7808284519953917 0.21771911421161472 0\\n0.7040050235428325 0.8742995639757579 0\\n0.7706771605007622 0.7414904090201708 0\\n0.33123103174567314 0.874115752426034 0\\n0.8087500152401772 0.16197067697178347 0\\n0.5082753677161874 0.13952873972062352 0\\n0.14462726403145953 0.3647199008034221 0\\n0.09769982938093069 0.9547990706770428 0\\n0.18107164444132007 0.7886502798778368 0\\n0.28563762300249607 0.9637034575232788 0\\n0.4436839368843244 0.03416722838818553 0\\n0.012966946811569069 0.5153776473214018 0\\n0.878317702055916 0.5827654313192763 0\\n0.6859789348531043 0.8228424824670659 0\\n0.15219902969979882 0.9756560074496742 0\\n0.9899900761011932 0.5196179659720757 0\\n0.051656884769465394 0.9405985134154844 0\\n0.7015532617265223 0.07737291023694393 0\\n0.12378554093396688 0.9308825319381725 0\\n0.18943036721679796 0.727305485428207 0\\n0.40949598061367865 0.8798306558221476 0\\n0.07487048290631948 0.1247274615049414 0\\n0.8667766778273511 0.2109474074817489 0\\n0.17316229832081798 0.9642716202994865 0\\n0.06606077830678247 0.7618597336566321 0\\n0.3961768121881263 0.8202353569426146 0\\n0.935866737884648 0.6065867987814078 0\\n0.8613574722455065 0.5366168519475264 0\\n0.6692750237109518 0.9017149276259588 0\\n0.7692670151497313 0.3456212913817124 0\\n0.01978471638435564 0.9976292022945171 0\\n0.35003809324428625 0.9970970557430892 0\\n0.3208678698242097 0.7301729589082486 0\\n0.04059341531327754 0.5933470332974888 0\\n0.738323356046209 0.18201821118985195 0\\n0.028940674653574194 0.09476240736403552 0\\n0.0405518988872815 0.7342809864007457 0\\n0.9510842048737523 0.5975985641459411 0\\n0.11921335061505778 0.305067064859441 0\\n0.07098244584545821 0.8837842546715653 0\\n0.5519908731202199 0.8865296766373213 0\\n0.7147921553940219 0.7708242587450973 0\\n0.9128621496458227 0.35919726683162023 0\\n0.8197725211016891 0.5509235134342155 0\\n0.9139456932152354 0.0006945006391451125 0\\n0.21453046716368862 0.7474938745832822 0\\n0.04219008760389875 0.8651797571816602 0\\n0.3936635164375313 0.11999930806399495 0\\n0.005157218797247509 0.737113622433492 0\\n0.7575169629671565 0.8452627139229841 0\\n0.0017374484413756885 0.4714046380579108 0\\n0.2606862059052346 0.3467143684093681 0\\n0.7688043762205244 0.9212941468497508 0\\n0.7698033740765035 0.23959732651836274 0\\n0.9298510505948591 0.7815181755576289 0\\n0.820372704274428 0.40416257928868327 0\\n0.6539816147354206 0.9041490342328254 0\\n0.7371299560703425 0.8523299599583557 0\\n0.49185112220952876 0.8353233549654683 0\\n0.6167019023992019 0.8726995317606463 0\\n0.4861888220774204 0.8239301578026762 0\\n0.2669382453582373 0.23113867311730418 0\\n0.5829565622161805 0.012880756531671422 0\\n0.7958142672895511 0.3696183699914919 0\\n0.07417279074535821 0.8938663117254498 0\\n0.13022958977400112 0.015639074966103728 0\\n0.7662425894066718 0.258818081460245 0\\n0.3274912327505921 0.023689838246493733 0\\n0.042337253215986026 0.053092846370460633 0\\n0.9119086094838325 0.8876872543314881 0\\n0.33184756987860553 0.14012784716403004 0\\n0.17182223182389267 0.11346939102150488 0\\n0.7134859038050001 0.9744954492696554 0\\n0.997058696474505 0.35461947130372096 0\\n0.9202772661864588 0.23941153677879157 0\\n0.7111626352420624 0.754971254305981 0\\n0.7999703978453737 0.7782574670513857 0\\n0.1902394448073087 0.09380014183621621 0\\n0.6824146997268179 0.2331777311992561 0\\n0.6997066015818455 0.19350813181085724 0\\n0.19032866491648892 0.04228407180708904 0\\n0.7573804676967401 0.7610126315976227 0\\n0.13160988264106277 0.27427337236082083 0\\n0.6858721986122879 0.3602065659087602 0\\n0.9264877456852105 0.22183421218889565 0\\n0.40222734330413434 0.12207478833662311 0\\n0.058362716569605566 0.5390590730945028 0\\n0.8418520591368907 0.1282918021143198 0\\n0.27588537710360606 0.04826784931903594 0\\n0.20238581345700568 0.7418421855223264 0\\n0.23607133232332933 0.060627428977884645 0\\n0.7754769893010621 0.1680526219391174 0\\n0.2811018899008054 0.8697822949567738 0\\n0.23979323329002855 0.40722046975422854 0\\n0.02088250813067105 0.3504595774268152 0\\n0.810760046849563 0.9732521897150325 0\\n0.08226261136928037 0.9932425023063437 0\\n0.5069103620175278 0.947730049776127 0\\n0.9052679631347829 0.8535759703688685 0\\n0.9926869462072306 0.7944857020657461 0\\n0.17145748594893095 0.2570897584973654 0\\n0.2753776101962033 0.22580988453425543 0\\n0.17897771250243677 0.7883339048141889 0\\n0.6631663790230676 0.10715954464947142 0\\n0.6618540992890463 0.8409935962235936 0\\n0.9337637416081266 0.33227418619366633 0\\n0.4159384459595665 0.8475441184977365 0\\n0.5714220795409687 0.09830892851820416 0\\n0.128239048184846 0.29838726616571853 0\\n0.8059504353641362 0.5035909052529761 0\\n0.6789024459810129 0.7977678102711303 0\\n0.7224782448805794 0.3216795143794562 0\\n0.13124828960326618 0.5096280810774118 0\\n0.40596781798476234 0.10560133836835761 0\\n0.6970146341111784 0.24783589712241416 0\\n0.6123162578250378 0.17446842239453952 0\\n0.8163627198206503 0.04278306008601873 0\\n0.014458388646930609 0.8958533452033824 0\\n0.7812231893832453 0.3942533695786241 0\\n0.595252204189655 0.08169236191486273 0\\n0.7361407507289199 0.9489987720154156 0\\n0.8391437810809922 0.3961011143551182 0\\n0.44975353761782944 0.8410463748791746 0\\n0.11793859062986012 0.7954638483890694 0\\n0.14228746738213294 0.140802220192625 0\\n0.9177434742883037 0.9149154273738901 0\\n0.47290252538102695 0.06772380920819099 0\\n0.09017767529348042 0.5565927160805839 0\\n0.25255282803631207 0.05516063238893698 0\\n0.46368736847822745 0.9782841116600227 0\\n0.7918431893043548 0.9325561857252727 0\\n0.17822416668830487 0.875403206955042 0\\n0.853603865200085 0.658281127600624 0\\n0.3845281078157793 0.8782681476958349 0\\n0.027635933135939372 0.31413090821778944 0\\n0.23226483459554925 0.3396894926245737 0\\n0.6075945394159372 0.8629730454053022 0\\n0.8881955771786387 0.6302547271467814 0\\n0.20347315565160295 0.7469783588687209 0\\n0.7349285183057371 0.6239561077820841 0\\n0.14087901921896073 0.6204018890368099 0\\n0.7939002674855566 0.1829310323906692 0\\n0.11437196172802666 0.6480012079684161 0\\n0.8197945574342883 0.3867561277580076 0\\n0.9808518740439306 0.3147298834934842 0\\n0.9437500628871334 0.5835157193057041 0\\n0.8198051571430275 0.9896438505862156 0\\n0.028424998538348234 0.8464984411882482 0\\n0.674133948753287 0.6917389448896477 0\\n0.6692639407027849 0.06435562028719566 0\\n0.48716704351615914 0.9852727890819238 0\\n0.8487792603319153 0.5617593152890773 0\\n0.18819472446745011 0.2802967064768995 0\\n0.9068386055778337 0.48016416632880854 0\\n0.7577131765157051 0.6070567081126107 0\\n0.2796567635979891 0.1125941685286862 0\\n0.7739320346984717 0.5563119218701407 0\\n0.026988674636565402 0.1518866239663762 0\\n0.16488960615278014 0.44484651328269065 0\\n0.5032715433604753 0.15676407876229093 0\\n0.6889377530299244 0.30327810016974766 0\\n0.49145042667916905 0.056090420618465586 0\\n0.7323263531840157 0.4749304984502628 0\\n0.7853813663073851 0.2712762749890312 0\\n0.11933321411318942 0.7673469516130216 0\\n0.8911814030942583 0.31774071923548186 0\\n0.8356677350947356 0.8002613478178562 0\\n0.3873399223130156 0.7846243330199292 0\\n0.789261207811393 0.0002303004834521838 0\\n0.3154619007254321 0.7801005981508371 0\\n0.07178798811297238 0.08063614818515852 0\\n0.05632574332546958 0.9971820475611244 0\\n0.13160066357951172 0.2070480277683071 0\\n0.4369662779488177 0.9088551109885887 0\\n0.8075668681778353 0.7869326372396817 0\\n0.4955192113277822 0.15712715718562043 0\\n0.11700265271831112 0.6842207207813382 0\\n0.3449489385870339 0.23106009396940996 0\\n0.05571068153382641 0.5200233067638346 0\\n0.9795176672150321 0.7867332484946326 0\\n0.30175901968844754 0.16499850417022732 0\\n0.36932299075093644 0.8860973669217094 0\\n0.08294601174502869 0.6751865321730344 0\\n0.8746146281041364 0.5448895668478961 0\\n0.15706513719148696 0.644055584933476 0\\n0.9986848417553095 0.5429499770815911 0\\n0.01833188850473677 0.6185880389674258 0\\n0.6067007640830695 0.9112200407715516 0\\n0.3500471781774881 0.8171489601481414 0\\n0.7195964466657725 0.15194521855173126 0\\n0.4143976110728821 0.7233101028812977 0\\n0.8368960266233947 0.4677154103822846 0\\n0.9035152754121738 0.14620836300276063 0\\n0.984595343296979 0.41873553912808703 0\\n0.28589214428375465 0.9676996174809621 0\\n0.10308430664021073 0.35441301434719596 0\\n0.8458300594271149 0.5356041363215994 0\\n0.035321843019758825 0.3432868055279997 0\\n0.09403140906077756 0.20163706276623927 0\\n0.2117175115086246 0.8932104225382119 0\\n0.0899887502196891 0.4280425919641996 0\\n0.12392442524655767 0.960055127761895 0\\n0.04217463172247837 0.9022151819774503 0\\n0.6704522121987448 0.9943172086491466 0\\n0.19964023693809418 0.22043913316059416 0\\n0.34046150202194314 0.9578881527883587 0\\n0.8518980748907229 0.8090120847336989 0\\n0.7104373621134852 0.8214775614930783 0\\n0.6244893378251132 0.7121403028427051 0\\n0.6631208853615556 0.08007396723364235 0\\n0.22917957059115945 0.02134104378189361 0\\n0.3796963686843129 0.02240650500470387 0\\n0.1662281131962663 0.6353166522895253 0\\n0.5858575654477751 0.734209253127071 0\\n0.1576115725184909 0.3542840886554862 0\\n0.22820552910077152 0.26096663395639697 0\\n0.30555444922338026 0.8782589734940257 0\\n0.6964203498985394 0.90163211458793 0\\n0.5720292958249819 0.7846690747994846 0\\n0.7678149142064409 0.13075011702373507 0\\n0.2907669844061791 0.8030106638029032 0\\n0.6937761554298243 0.9403490579321332 0\\n0.9754588374095227 0.1709623320457485 0\\n0.8178359918838809 0.5318225079043042 0\\n0.287330370124118 0.6471022739308747 0\\n0.6713230591524074 0.2786429050458483 0\\n0.31837543777874466 0.8206453135423509 0\\n0.6557553575692325 0.06882336771172415 0\\n0.6301321651910686 0.245815207815543 0\\n0.8554204893075589 0.10961728059038145 0\\n0.8489729508081042 0.6381000174963667 0\\n0.8944518132616763 0.10850863876398631 0\\n0.134516998661104 0.8262797090203741 0\\n0.8481979793131252 0.8397755471014622 0\\n0.418226571937824 0.9500171977215427 0\\n0.8302912423195248 0.6119775590938924 0\\n0.44460615471750364 0.997020748799648 0\\n0.9900835747974304 0.11727626221482734 0\\n0.1603137661184152 0.9410895522548991 0\\n0.9383809672027363 0.7733301444344797 0\\n0.07812038621476225 0.08696203404735159 0\\n0.9633045095346335 0.15574025878453523 0\\n0.7023971723976666 0.7874476791984176 0\\n0.05340041185725275 0.9611094972955609 0\\n0.9901776155579596 0.5816360041085954 0\\n0.07658711730078893 0.13498523520808203 0\\n0.9062693820963073 0.34692043164684705 0\\n0.9787006913853554 0.7397301806224648 0\\n0.4223756853031171 0.8117961179239146 0\\n0.18343251335361954 0.9128530773755765 0\\n0.47202451332794426 0.06081508269149871 0\\n0.29211913393634903 0.6526426943774897 0\\n0.6584367715090604 0.7652208274496175 0\\n0.30041062826338794 0.08036723586266403 0\\n0.9839536121700394 0.8445946065892113 0\\n0.4183834831782033 0.2845132786728446 0\\n0.042565829384410336 0.5381220396692202 0\\n0.4026136149508597 0.8972834330558429 0\\n0.7429989245056511 0.8130719556918532 0\\n0.3645332107096585 0.13833633571194914 0\\n0.9547216710440194 0.8743972390881397 0\\n0.3591375451209641 0.25875316142598725 0\\n0.2612804381822077 0.5733294214719562 0\\n0.08016374423644612 0.3528590695163868 0\\n0.9432229930949778 0.26163556924883813 0\\n0.240867028128685 0.7267903471725905 0\\n0.09203012448510273 0.21845027187102728 0\\n0.21287630277062142 0.9972305506976692 0\\n0.12901618433509454 0.6610877761747838 0\\n0.9306774271097862 0.22869317914068976 0\\n0.8580611743576876 0.6613735093413686 0\\n0.7478771273079852 0.6564352423067211 0\\n0.49341111917527847 0.9608728958727625 0\\n0.7263935158599831 0.8643888287550853 0\\n0.3107257559595479 0.20656347537423692 0\\n0.9475164554809636 0.2055116082761601 0\\n0.9764331207130502 0.452780962917248 0\\n0.06787882455659466 0.7824103581746126 0\\n0.9650953716028214 0.26279131841300585 0\\n0.03936831371249738 0.9559039086683886 0\\n0.1765644171038646 0.15793041280537423 0\\n0.2774652197887041 0.8509228062936197 0\\n0.9237974914719014 0.09008927327316651 0\\n0.64430848472243 0.2018161529603999 0\\n0.32751978246522573 0.8026295102905842 0\\n0.2654815854106076 0.6694838405429389 0\\n0.9729437107309422 0.794410042472807 0\\n0.0034366398854988045 0.1969597656425236 0\\n0.36950480883066483 0.21518697012717192 0\\n0.4770149299491737 0.10629852175456633 0\\n0.13404076559783296 0.29110145680431154 0\\n0.6150722428584732 0.08500306833304638 0\\n0.33648252193000816 0.6825015219233586 0\\n0.19855142049350605 0.6863844290932214 0\\n0.42220050014370347 0.1442409011126078 0\\n0.2121803118920218 0.42191071576012484 0\\n0.01688132898411021 0.03666979349432542 0\\n0.05130242711265165 0.26926666338136707 0\\n0.27508248314256767 0.23977388116014897 0\\n0.05319473829727228 0.9987547707723766 0\\n0.972297024613452 0.9381652917773338 0\\n0.43043542394962986 0.8463278320609079 0\\n0.4822993721530723 0.083465863393826 0\\n0.8490870885530265 0.4481552027990324 0\\n0.9385218529560169 0.06716884011662783 0\\n0.3070618224084942 0.02006416815047296 0\\n0.7175676775349652 0.02413315053824261 0\\n0.7223841538467949 0.9000322993072121 0\\n0.7577670217981041 0.5106610727498264 0\\n0.5573384689595049 0.13921663891287162 0\\n0.08901244042795053 0.8295041795797328 0\\n0.8303637070958216 0.6671524293088078 0\\n0.8045926571525245 0.5108002112158829 0\\n0.5989543851482002 0.8917091292740918 0\\n0.2668043271140039 0.18961570753459067 0\\n0.8529212006658403 0.10824783629661117 0\\n0.3222854142058782 0.8520808182787273 0\\n0.37205643518896836 0.015388355729127001 0\\n0.752663083162423 0.9637896274382127 0\\n0.2667082169205922 0.8619608160317318 0\\n0.762616910915268 0.7680801608284181 0\\n0.7212775979250771 0.7514225475474391 0\\n0.9058900672695561 0.4327385548558079 0\\n0.1864122450050666 0.3085689920340371 0\\n0.9245377349212298 0.9082536528936218 0\\n0.9353447960669007 0.6718223643817769 0\\n0.9942109299911461 0.5848371294429877 0\\n0.5820496990627115 0.9876809655249732 0\\n0.22036581358078078 0.43252592080822816 0\\n0.7770436363124175 0.25765331549211057 0\\n0.6709129728908799 0.8214224590884713 0\\n0.7643608726893554 0.7472034080296163 0\\n0.05635318607243989 0.49327048625284065 0\\n0.7695140231255545 0.20011978865944546 0\\n0.10252281359345561 0.21848571770745284 0\\n0.9469717135133161 0.2393299544027222 0\\n0.1127581563911314 0.4508106336656057 0\\n0.6279916499573943 0.07142532151660963 0\\n0.5937003029265809 0.06882848740536351 0\\n0.13730045285311498 0.9484880251484725 0\\n0.6444798429344286 0.13290336728777508 0\\n0.9869515358935161 0.37883857303263424 0\\n0.3398630946227982 0.7197654034111763 0\\n0.22385731170459366 0.8506874674503105 0\\n0.22970770282537945 0.554455113295545 0\\n0.07160523825904785 0.16350283782794572 0\\n0.9752309186760025 0.05297475199812185 0\\n0.961521908733881 0.4873049136404446 0\\n0.28860397841433805 0.7382972526160946 0\\n0.37871790196268595 0.10333283806928817 0\\n0.24222935315527294 0.06704356319458715 0\\n0.9242090337790434 0.7888885357805403 0\\n0.028619282284040315 0.7018708135197484 0\\n0.1731181607019977 0.4666597340253893 0\\n0.8331601735622542 0.1838901245961383 0\\n0.5668810893538294 0.8422767133167839 0\\n0.9226543198543835 0.2034436635185075 0\\n0.6846127710306983 0.04611997559696457 0\\n0.947055194416189 0.4246054884039441 0\\n0.077919896573165 0.29270268535660715 0\\n0.20950083573386513 0.8628943455093542 0\\n0.6340473248630478 0.7871016014526944 0\\n0.6548435855045567 0.3335315199643648 0\\n0.21311693048543878 0.342762555039388 0\\n0.6205909278000388 0.18277645718849256 0\\n0.6051074594481126 0.9661509060702386 0\\n0.7530637981131276 0.9301302518089236 0\\n0.27031213532952125 0.23816763996817314 0\\n0.9283125304589098 0.7544059775605624 0\\n0.6553993011511449 0.21079614876789232 0\\n0.25862297233323883 0.4344830408090935 0\\n0.048707836115504 0.29737582763518644 0\\n0.4968125816507566 0.17248787807103727 0\\n0.7458139645300489 0.6540693900362781 0\\n0.888661266713224 0.13640069462717008 0\\n0.0067071120409143425 0.07035398757955524 0\\n0.7555942667485109 0.5060208384567726 0\\n0.129117911063526 0.37874738872368796 0\\n0.6429881910066634 0.14097791870138943 0\\n0.8691153463320979 0.5856023610822183 0\\n0.18354670417342767 0.222421598712094 0\\n0.862882184257438 0.48354049765915763 0\\n0.7993652949947996 0.22668446063948633 0\\n0.7990036600499216 0.1437326145699649 0\\n0.02351715633699647 0.7321195897813232 0\\n0.34171351228998725 0.1499520924731853 0\\n0.1320483023462683 0.8729525603937709 0\\n0.7437591499076371 0.9082651331484948 0\\n0.07276157102835978 0.5613174879297175 0\\n0.9694984168857761 0.2653338321126296 0\\n0.9999403047515074 0.655990503076287 0\\n0.00453554375936116 0.7877576198483013 0\\n0.07239596862910325 0.4348393489672211 0\\n0.956719390739733 0.49878641963449 0\\n0.04277121415898011 0.23201290869386182 0\\n0.4375806002834973 0.9694906359174271 0\\n0.7542516814641037 0.9834235444318371 0\\n0.09104311996834036 0.10163677547090388 0\\n0.8092386927316585 0.0807795011014244 0\\n0.13366024454140224 0.26709588188500366 0\\n0.38233516613303353 0.9453422297288833 0\\n0.8141892916087802 0.8961034783905277 0\\n0.6242919861024261 0.17638523745564627 0\\n0.7307219713176768 0.04346294895190228 0\\n0.9442420088316313 0.69444591226766 0\\n0.12118226206141236 0.21558074911589187 0\\n0.5789910215996892 0.24922983084681827 0\\n0.4847676052652232 0.8988584773605871 0\\n0.573612446728107 0.007506079633629148 0\\n0.48607437049264357 0.16629195628739135 0\\n0.7035342931601806 0.09770194942745436 0\\n0.7556852118849533 0.200237963210396 0\\n0.38337828997438206 0.9078495714011022 0\\n0.9172675029900464 0.3582266009363242 0\\n0.6124517830586222 0.01814512735405205 0\\n0.8104090955698164 0.237079578325468 0\\n0.10997588166709937 0.7524675302692622 0\\n0.9916214354775321 0.5485759086056817 0\\n0.22686195203310378 0.3502897739937776 0\\n0.40263002994833874 0.2741641350184284 0\\n0.8882368238878413 0.952857314397441 0\\n0.9404056111517083 0.9909001450148611 0\\n0.7280645772348943 0.748128550848372 0\\n0.8798222936708423 0.7472887057117477 0\\n0.07968674904987372 0.8208322269873501 0\\n0.29607760813593254 0.40090055565315785 0\\n0.23374772649149667 0.45088913032823685 0\\n0.9114170810157392 0.6425414332525586 0\\n0.448276100871299 0.7381635541679901 0\\n0.5745295948651726 0.12836575589112198 0\\n0.632602298817637 0.8951125882251547 0\\n0.9989280666665838 0.9683440356432466 0\\n0.2703763942521179 0.1404540325657636 0\\n0.7405799076654278 0.7103477819998387 0\\n0.4847052840501085 0.8548526024356449 0\\n0.023894127589041525 0.6538297093650529 0\\n0.3839322761124715 0.9979483561034931 0\\n0.8479619789711473 0.4005873423365165 0\\n0.13484719990136163 0.22743073177845385 0\\n0.7785176823865563 0.8276501824001663 0\\n0.0528612911585844 0.5985826567436251 0\\n0.9797019346229991 0.9602325282459757 0\\n0.8554827754005485 0.45187497194537074 0\\n0.5235751893343433 0.11007423603918598 0\\n0.951609141330048 0.5962209805599756 0\\n0.1760646648370403 0.8542944508832998 0\\n0.9552258821320927 0.3987110167281075 0\\n0.7771132986116095 0.18398354297357422 0\\n0.712864079488365 0.8063297957737801 0\\n0.8052505314660371 0.5151030545338003 0\\n0.8185557303371296 0.9570968011854247 0\\n0.029068633474335748 0.6193738722296336 0\\n0.9279101740518962 0.4075915071235299 0\\n0.870042482212284 0.22748149867674716 0\\n0.9144960448338393 0.591158538246312 0\\n0.5146031538850081 0.006021229966075237 0\\n0.16683370018324917 0.09747922949758236 0\\n0.1768526324529085 0.5905086209669602 0\\n0.9470614944750599 0.95379628508421 0\\n0.737897708496692 0.7999763297826216 0\\n0.2950601986428304 0.23199593067377045 0\\n0.04792400707640021 0.9615111871140637 0\\n0.10717861092087466 0.7238759064409097 0\\n0.04585753875277787 0.10192530391778809 0\\n0.8671643658965978 0.9453181589217615 0\\n0.5261089657269041 0.21354952524136883 0\\n0.18167997947505998 0.5407106358659184 0\\n0.11435262027328097 0.25880680988730353 0\\n0.9552301908705464 0.39155460176016355 0\\n0.6421051293207741 0.26943658466522424 0\\n0.7922631797116949 0.5995449201185178 0\\n0.9141522773901838 0.3898290375251594 0\\n0.14505212075221185 0.4357873976785218 0\\n0.8837561605892045 0.40127671842165547 0\\n0.1162442535676318 0.22292078845901264 0\\n0.9312411548813215 0.3130622978769888 0\\n0.2596926749543663 0.7905897070911628 0\\n0.8150788958666436 0.1265709767239419 0\\n0.17896063471835055 0.953408397688852 0\\n0.8766892540668537 0.46585495121921383 0\\n0.5473370208483488 0.06949926687059382 0\\n0.9354992023822336 0.8938793557873744 0\\n0.20129689788308414 0.12576195435324755 0\\n0.6852359927789431 0.21199564738412535 0\\n0.17071464098719125 0.6587798940786662 0\\n0.9157461735049055 0.84546581236436 0\\n0.344059358274834 0.20100388388327828 0\\n0.3222663155157882 0.011335359058094796 0\\n0.8873187447812715 0.5332306030573952 0\\n0.5652515252680494 0.8251789372972322 0\\n0.8439692345063355 0.15608142140403058 0\\n0.7409389814765767 0.7054659907301754 0\\n0.8721061169860934 0.8804565230735177 0\\n0.18822213229736695 0.6382582792856327 0\\n0.7845817888953565 0.8457278133330934 0\\n0.8309720972231414 0.3638102208734465 0\\n0.08305736664457652 0.7727664259984501 0\\n0.9066925911028727 0.22748954980083458 0\\n0.21286656411092086 0.30442973852391486 0\\n0.05865221449553659 0.37589085990896565 0\\n0.017097506478398694 0.842343749019416 0\\n0.9674298828451818 0.8916154631860282 0\\n0.8490079614731231 0.912235492013327 0\\n0.07957486943407 0.016450546862334825 0\\n0.8373926292177313 0.5701435814393557 0\\n0.16458639325265534 0.0852963214547795 0\\n0.1653686341619799 0.04484026659204576 0\\n0.21674286023641642 0.9741738329326316 0\\n0.911492790369272 0.8471826053970314 0\\n0.27163971097911743 0.17290085270818534 0\\n0.5872786352143635 0.1373263647349119 0\\n0.015451708152452337 0.7625972139703501 0\\n0.7752132128904203 0.15365707879824286 0\\n0.11853687172174154 0.06996217361357793 0\\n0.3144640779103256 0.9518883938340749 0\\n0.7912906088048943 0.16848356495273276 0\\n0.44078077864274245 0.07095416993878267 0\\n0.9797976352520049 0.02804258695048878 0\\n0.01625751171139944 0.16794123084290835 0\\n0.358155684405692 0.2449496110956918 0\\n0.19202874658389557 0.07751967928144965 0\\n0.5970099889670534 0.8138740057782167 0\\n0.0853529975260986 0.7244078957711735 0\\n0.47668531516159485 0.24033060395340589 0\\n0.8428632914320995 0.5138748567693658 0\\n0.006547043131096886 0.9998505339789108 0\\n0.4998572732628095 0.9265320832243158 0\\n0.9656701895163599 0.13920663444570291 0\\n0.21572737105557505 0.5157149445004784 0\\n0.9389574448497214 0.7481300172365897 0\\n0.0971566252489966 0.8689752430232737 0\\n0.6332847479015276 0.7507982972153849 0\\n0.35986579448089284 0.06839833929346562 0\\n0.9848816928957914 0.5548378234802938 0\\n0.7620535551622614 0.7394746260429387 0\\n0.027974706736456856 0.852379337373481 0\\n0.5030530325799972 0.8828154937194882 0\\n0.8808242947585261 0.06918469824238394 0\\n0.7322108710603419 0.9911574041465699 0\\n0.6453096770676324 0.0195987044173217 0\\n0.8636094253811791 0.645652088956623 0\\n0.07081508356393551 0.7796787108510512 0\\n0.5888766078461436 0.8528213506590059 0\\n0.22039345718775005 0.2373548405435758 0\\n0.17979033402624633 0.5459770077847684 0\\n0.9036754260753145 0.6424219986793679 0\\n0.3628001404459946 0.8602725068128542 0\\n0.9085907037038665 0.8148421680776745 0\\n0.11758387403704029 0.5800922521681934 0\\n0.7580786059065808 0.959534178662986 0\\n0.21064157515619186 0.24749150818489984 0\\n0.6654710540809878 0.9959514745372812 0\\n0.32196916819690113 0.14213987818417284 0\\n0.6239052958458752 0.7885083698811899 0\\n0.44075970083446603 0.9798651549802186 0\\n0.9879352341657588 0.609785090591869 0\\n0.30321374407894275 0.839052180884862 0\\n0.36959696105308604 0.12397566599485121 0\\n0.264060968191257 0.45521451161219384 0\\n0.7707315060528034 0.8550263572900687 0\\n0.3861233026035914 0.01528793920517868 0\\n0.5281723018749248 0.02388286875647705 0\\n0.9829575167446599 0.28651492750507834 0\\n0.5134931749404217 0.8534081429374275 0\\n0.5119872290263509 0.9640695349841437 0\\n0.05859924052825005 0.18015285119215385 0\\n0.7268867681010698 0.8921584279831619 0\\n0.6709325531334724 0.046029625206991165 0\\n0.12855297686240075 0.9463748001783918 0\\n0.9609890933049979 0.19061171232132923 0\\n0.12385230028120608 0.7215239480642128 0\\n0.4558058241930454 0.11634060421614123 0\\n0.28521850946465854 0.41440593728848973 0\\n0.04340196641189664 0.8251705138770062 0\\n0.012586165052150156 0.6912122030566429 0\\n0.9100891504191243 0.13597907552962363 0\\n0.9068717457187996 0.5259137114540825 0\\n0.16559542757332257 0.3632981479928096 0\\n0.736641456957579 0.9875536748118365 0\\n0.758590988343189 0.10526381605229196 0\\n0.9475642327517358 0.4629416784899172 0\\n0.6929658883879638 0.1460281161709429 0\\n0.8566660559808199 0.0984274803268882 0\\n0.8729179253742133 0.18424328645513222 0\\n0.008067694224748179 0.9444205914232068 0\\n0.6807441427103254 0.184729106681491 0\\n0.3052284897895331 0.2401931981293306 0\\n0.9249295052605263 0.14212607275186273 0\\n0.7766951213180666 0.01961796027112861 0\\n0.9573251129907183 0.7253410007064427 0\\n0.1558604880837795 0.8033941880726068 0\\n0.9545566992821711 0.1313969738183226 0\\n0.6374144377460093 0.7789483300746372 0\\n0.8840672716236586 0.08360774410543381 0\\n0.305556285160444 0.22391509021193157 0\\n0.6002383049742798 0.9683290713752598 0\\n0.15173541380964672 0.17353494770146227 0\\n0.32037274362034596 0.9913052770811988 0\\n0.10583420922594622 0.8605863174502286 0\\n0.9540841008923097 0.1286518841041171 0\\n0.49344399141454776 0.9362697048220888 0\\n0.08446373859510692 0.8804424434662612 0\\n0.5339278632949982 0.013568223677550284 0\\n0.1512877053740157 0.9866566242203412 0\\n0.7208124127179371 0.05970757314295572 0\\n0.07123311662696252 0.25305638462997326 0\\n0.06148131032581394 0.5732118637734626 0\\n0.2774874194193365 0.009334553782249455 0\\n0.13282494816292734 0.02989301831102653 0\\n0.41789251471327593 0.8908326253846913 0\\n0.11403778935286024 0.4733533978675071 0\\n0.7087193341325377 0.15873283095066082 0\\n0.8504791896815532 0.24381803498608434 0\\n0.37557866450656996 0.0921734901581307 0\\n0.9819438482987702 0.7107217512423788 0\\n0.8354065436411784 0.656449462271703 0\\n0.6209858260625586 0.6904738012260628 0\\n0.792460474033217 0.8732523193105305 0\\n0.875217346703785 0.21490406084666824 0\\n0.13035206148025658 0.33780967393520944 0\\n0.050231759562752054 0.05464200612624437 0\\n0.04382678462088485 0.4933534861111203 0\\n0.2914561056757613 0.7250220534123879 0\\n0.9010888863344266 0.4572554494740515 0\\n0.785309299669164 0.9348008540506354 0\\n0.8200992972658522 0.26155070623083887 0\\n0.020645189910759698 0.6247636940151786 0\\n0.9017058835406596 0.6453342758243008 0\\n0.7777561093798449 0.7430734649704599 0\\n0.9564509690988494 0.4247796399758371 0\\n0.5225774900776068 0.19557038654800518 0\\n0.5386459597223894 0.8548494579085151 0\\n0.726469016800061 0.12227133413997349 0\\n0.7894755042072715 0.6698000962554438 0\\n0.08216515141954794 0.9551988934093881 0\\n0.00847303804748889 0.5093449484255923 0\\n0.6680452836847747 0.26041373718521754 0\\n0.4672667769196215 0.835457695842586 0\\n0.780408720448465 0.9508292113480796 0\\n0.5576481799096262 0.7801098426361878 0\\n0.012791722768045855 0.6866355621900058 0\\n0.7401817382087382 0.8986862190091204 0\\n0.6869192522020569 0.05040391371184394 0\\n0.8922682684612887 0.8444069905929926 0\\n0.7186387247638524 0.8018758295677118 0\\n0.5829484780115263 0.9572296053416072 0\\n0.8629484780102489 0.7432126917600536 0\\n0.845042297429388 0.6422323447052652 0\\n0.061524587893278415 0.21998203187036858 0\\n0.20431283457433724 0.9853401644248738 0\\n0.7093864705367878 0.8970383663520499 0\\n0.04626393616850233 0.12466048302404265 0\\n0.026133867366910035 0.8928399563282647 0\\n0.7167906074500999 0.0201554596303678 0\\n0.5944911461410822 0.8442885089302352 0\\n0.14502492633095365 0.4902248949949767 0\\n0.8359356263595943 0.7805853970499409 0\\n0.7337076256347568 0.16418561330475445 0\\n0.19385145920081637 0.6915827207267374 0\\n0.2877144912377366 0.8317071973791037 0\\n0.8364614978825328 0.6121544875980232 0\\n0.5699883279551007 0.9186216906997868 0\\n0.8156489560544016 0.39192870890691667 0\\n0.09250682305068259 0.9945686435934272 0\\n0.9896912000538605 0.0775734721321305 0\\n0.06798739397449971 0.27373196446483394 0\\n0.7342441755166426 0.6934292342853339 0\\n0.2621858713802898 0.034865335529949415 0\\n0.021283000956855536 0.3185886655271646 0\\n0.5823825721539848 0.982320049867824 0\\n0.06310204270158781 0.42438331781374805 0\\n0.9986343681840881 0.825915224169346 0\\n0.8423761019107044 0.5093555527794451 0\\n0.14833651679432713 0.8969838450140187 0\\n0.8267500132111004 0.4707408756919549 0\\n0.36936384132873523 0.9575789685199624 0\\n0.021144560101708287 0.5133657079830153 0\\n0.15606140361516796 0.6842953491921566 0\\n0.9627977124179744 0.031623641587135376 0\\n0.8688562337701248 0.10315527584528361 0\\n0.10843731193072437 0.08650453457972263 0\\n0.05585269536302262 0.9948455343022927 0\\n0.7540743476229685 0.029690729240703595 0\\n0.07278411680758623 0.11538285616315858 0\\n0.8231640348985616 0.5720176718600082 0\\n0.20952354123257744 0.319445675895328 0\\n0.9636149211089786 0.18561174533874525 0\\n0.7370738501842169 0.006203269715889914 0\\n0.6508815774004376 0.097442978174486 0\\n0.4022924601382888 0.8066808566002718 0\\n0.9729198710627913 0.488509905380913 0\\n0.8400838333850863 0.8198088617011438 0\\n0.15774165943703844 0.5208914737419539 0\\n0.526537311457482 0.746957680521358 0\\n0.7563421130166094 0.09124241563987112 0\\n0.8599764765934392 0.7973948970306269 0\\n0.9436894936316126 0.42356300180551665 0\\n0.6141901868667825 0.8130143258905547 0\\n0.25490334226829114 0.8769298456246084 0\\n0.9838387087092049 0.8470246731268407 0\\n0.22862947088287822 0.35015808309367114 0\\n0.7113529773094905 0.9862756611989854 0\\n0.8211638148274347 0.26716481572690176 0\\n0.8439602787025839 0.15101443015085014 0\\n0.16757756709520155 0.5287206701735556 0\\n0.719725176584857 0.08271241901267434 0\\n0.21683188914332185 0.7523380449531059 0\\n0.11772026293401072 0.23244156777156022 0\\n0.9653795030002269 0.16378106761428723 0\\n0.7755761562091892 0.8357915510011237 0\\n0.8789077846420896 0.3545134829850801 0\\n0.16411695898946443 0.9884625563734671 0\\n0.9552943243187666 0.3708662025337389 0\\n0.7130712031763636 0.18995657404119792 0\\n0.03807937421139418 0.18724860757402784 0\\n0.5494003826518499 0.753170166688587 0\\n0.21876265767646974 0.1741484013521155 0\\n0.6563895407390026 0.7424817589249901 0\\n0.7293825845247195 0.7990766686987073 0\\n0.19645725420957894 0.11015555054069914 0\\n0.269448441594463 0.08158287358641092 0\\n0.685179135274712 0.0006089057350668892 0\\n0.9609037144433932 0.22476492356763955 0\\n0.8892533446540968 0.28435496923547865 0\\n0.0625422933629397 0.8460949476999107 0\\n0.046489214255882705 0.33994346096406713 0\\n0.25262365442729173 0.9351683348635401 0\\n0.8356057714983571 0.035276648774185126 0\\n0.8622401206517536 0.6961830149541205 0\\n0.5416774157134706 0.8795134476784846 0\\n0.8309571310569888 0.5371751017113321 0\\n0.673807912851659 0.014832545097278604 0\\n0.3430243146911688 0.8338574253043237 0\\n0.8781688680243619 0.3593554137194793 0\\n0.7471504673076599 0.6903609327698563 0\\n0.9465937634437346 0.2948416135943913 0\\n0.9597985863070492 0.45829479464971534 0\\n0.4507525888652454 0.20506105023613885 0\\n0.10825671642543055 0.15518395156047116 0\\n0.22607763586621143 0.960065575709795 0\\n0.7338045458334471 0.5366395227835731 0\\n0.8688461528246437 0.9083399992433578 0\\n0.3259283727427119 0.8230069328270335 0\\n0.9283164650355609 0.8098157578601356 0\\n0.31221406445609867 0.9457591588641137 0\\n0.2044090154547571 0.8166670550813663 0\\n0.024660307553471927 0.12711997911910067 0\">, @options=Hamster::Hash[:with => \"points\"]>, #<GnuplotRB::Dataset:0x0000564351491cc8 @type=:datablock, @data=#<GnuplotRB::Datablock:0x0000564351491c28 @stored_in_file=false, @data=\"0.3697135150568239 0.541809194368996 0\\n0.6143530366968214 0.3974347209001604 0\\n0.4344521695492197 0.28748880590807224 0\\n0.5245829037461333 0.3062267201053054 0\\n0.4420230311050247 0.624410038264162 0\\n0.48185702173413003 0.5712683308639144 0\\n0.6779276629020332 0.3882925110077404 0\\n0.33092492403897766 0.2612862099138866 0\\n0.7088953404259424 0.4398037967911176 0\\n0.3079732704012298 0.4192540351531441 0\\n0.30130489415899775 0.41783633792315555 0\\n0.5767302478681051 0.8034394050034679 0\\n0.6374100484474298 0.6598609043226972 0\\n0.5242397982866717 0.669286992827128 0\\n0.4701458013043093 0.7260969067186163 0\\n0.6395033752686635 0.4598704266187099 0\\n0.41793904327278597 0.3797667563773859 0\\n0.5730239044578351 0.6922539080031099 0\\n0.5976949124454126 0.5324421463262518 0\\n0.5582746204707965 0.7665134736895042 0\\n0.6100756436656879 0.43059866280774384 0\\n0.40187449853224033 0.20272290173160956 0\\n0.33832322672741777 0.611639175504017 0\\n0.7261418662346204 0.6835504776358854 0\\n0.41330397902593663 0.6257490214688568 0\\n0.26159906257416865 0.3851450674835395 0\\n0.5156928363485125 0.43259086130246605 0\\n0.5844610647114797 0.42287549846930506 0\\n0.4097647661342999 0.331470879246169 0\\n0.41265906107731365 0.5438750829233178 0\\n0.47214240297147825 0.5228864807146039 0\\n0.5524387323047861 0.2950066747380008 0\\n0.5566062888071729 0.810116537496076 0\\n0.6032558872901308 0.6323226748842607 0\\n0.6459038370936689 0.4434165183686354 0\\n0.5145631003305968 0.5734820349607485 0\\n0.6784117207939186 0.5218179826806326 0\\n0.6224644847386734 0.5810446137525206 0\\n0.4573093709212157 0.6441327825916549 0\\n0.6151341321818604 0.22844961390891438 0\\n0.5034903081550539 0.46888070533609694 0\\n0.6109109840062746 0.6664858266155753 0\\n0.23222087970605731 0.4243961097583753 0\\n0.3514291637738317 0.3761565970467857 0\\n0.5513583160492619 0.6914827656516013 0\\n0.5538844846174218 0.5030321234857638 0\\n0.6161757966371149 0.38403902732054584 0\\n0.6514784105784167 0.6051036366811747 0\\n0.5838360302920361 0.3801158738696292 0\\n0.29821495442664525 0.44316396249895085 0\\n0.6418922624759771 0.45570011890138584 0\\n0.4060365600556691 0.34812715240433056 0\\n0.4545615957201147 0.28868629601986007 0\\n0.446300929393754 0.46204283448291794 0\\n0.7149253721891979 0.2792740704045378 0\\n0.275091153966887 0.707623793646068 0\\n0.2974942530970571 0.5663387142133713 0\\n0.44779721988903665 0.4294402992200723 0\\n0.42030937450330275 0.5566507959216644 0\\n0.4233034758058928 0.5370395105258589 0\\n0.28922869464401424 0.47485693191393863 0\\n0.3381207010820102 0.4675552334766053 0\\n0.6016290722354874 0.26661370833859044 0\\n0.18717791877443435 0.47425934483539767 0\\n0.29310437928696065 0.6741708509464708 0\\n0.2618903684111472 0.36751585096231265 0\\n0.37311806577848694 0.6268415259738295 0\\n0.49717715765354464 0.3428549964757398 0\\n0.4783264194989708 0.3181779822524121 0\\n0.7147521100717709 0.6269938319561159 0\\n0.5662606154421149 0.5065483503720125 0\\n0.788980442396218 0.6000811340878616 0\\n0.384729554783652 0.5967022368411519 0\\n0.4334135042136936 0.6738054547403356 0\\n0.317307928101739 0.25357999520142604 0\\n0.3638874620974294 0.725321416561149 0\\n0.4716462262987685 0.1971973236671064 0\\n0.6295428806319451 0.5157303827576686 0\\n0.4443430120787303 0.645909454282884 0\\n0.4529465393054031 0.3943376116120735 0\\n0.32496383422529584 0.4221799146977856 0\\n0.7471965333507108 0.6328474885346138 0\\n0.44060121661310325 0.32368102514684804 0\\n0.24642121068946754 0.4215626070363456 0\\n0.6004414355115029 0.5644111965724558 0\\n0.6753728377345805 0.5416539845825817 0\\n0.7650601747998891 0.39112951641959914 0\\n0.5891334084918795 0.5741696269029503 0\\n0.2430399991699308 0.566914958251412 0\\n0.44117675736337325 0.6161039955400722 0\\n0.3833182694172407 0.5895544757446665 0\\n0.35501672345169033 0.38173518505342763 0\\n0.4883994521167325 0.29828230443932513 0\\n0.6372656846978871 0.3842405675534487 0\\n0.47309821386526996 0.7002000433203495 0\\n0.3105828963273518 0.5895246228598298 0\\n0.32763644409586 0.5390387616117102 0\\n0.7512519137797236 0.533482441536792 0\\n0.5642517416506012 0.515910002859987 0\\n0.3592582590056904 0.4593507294115682 0\\n0.40657381484626365 0.3252633962952648 0\\n0.2903775723640306 0.69396435387101 0\\n0.2883910400327777 0.6607584048132161 0\\n0.44421031069612515 0.41967073854544856 0\\n0.2341708927536772 0.4584830674407566 0\\n0.35353564041947616 0.3150446272257137 0\\n0.48637630335247195 0.6838702565318391 0\\n0.4516644747352445 0.4035397361877804 0\\n0.5385250372944466 0.6459691715680052 0\\n0.7400395871235722 0.6786364544313137 0\\n0.3862904004973642 0.4481523588103342 0\\n0.3915687527806849 0.5524004550041061 0\\n0.3290748533630785 0.3153971409822799 0\\n0.33377198207663306 0.5160518920650159 0\\n0.23285603297632218 0.4090174585736711 0\\n0.22886863662057455 0.37987491718855826 0\\n0.6601216453244706 0.32640966255512527 0\\n0.5422838088301821 0.3067035690211617 0\\n0.3951729954522699 0.5088817275121342 0\\n0.4541950595112292 0.2938018187108661 0\\n0.448919359158204 0.4361667011311935 0\\n0.3495941007103154 0.625722189198928 0\\n0.35379799535057277 0.730849607654155 0\\n0.2140476884970146 0.46145708899658044 0\\n0.3128189817008742 0.6662870506979166 0\\n0.6000092867364609 0.48405046491237314 0\\n0.20601549823137266 0.39448864877089573 0\\n0.2234072689543357 0.5460834923001583 0\\n0.4643178746631029 0.656748514159546 0\\n0.30288638378731614 0.5062519458213591 0\\n0.7086300026534279 0.40639074357546023 0\\n0.5443312963787754 0.593837325326353 0\\n0.42604457216678215 0.7078379088581673 0\\n0.3745741535185383 0.5339082107865742 0\\n0.6644440028129954 0.4048624344159061 0\\n0.2820148454793514 0.5261494347766308 0\\n0.3856063793201757 0.6327570407397668 0\\n0.2674198437989541 0.3551187429476387 0\\n0.756595028533235 0.5420410810442 0\\n0.52509285784988 0.40986409060964946 0\\n0.5023424857453319 0.740180225595801 0\\n0.5618855325124503 0.7354297972719218 0\\n0.5886769643753712 0.622872263806174 0\\n0.46015140944495814 0.5508085782026124 0\\n0.4711672208291823 0.30042190985433237 0\\n0.5993911694612387 0.6810409336512758 0\\n0.4712763248516574 0.6698342103884507 0\\n0.3387295164035765 0.6002256026933289 0\\n0.6351184246152319 0.6162682523664933 0\\n0.6428099391217212 0.6589464152176724 0\\n0.6608637577223379 0.4941072755250019 0\\n0.342751308267745 0.3965565629182205 0\\n0.6214364555567372 0.3350875186676765 0\\n0.708417906471406 0.44389105631972414 0\\n0.3287801361234314 0.3964500308056118 0\\n0.5623170336968627 0.377186185544783 0\\n0.3478405472752235 0.470145369558288 0\\n0.6883373623397643 0.45333447207236666 0\\n0.4834668452318387 0.5443661303354604 0\\n0.35380300500463724 0.25162647922392545 0\\n0.6183456203725826 0.6149244060571842 0\\n0.3535975681819603 0.42442916672527176 0\\n0.5473464777327791 0.33570023443733354 0\\n0.5317807243581119 0.5580318691396656 0\\n0.3121683454240982 0.5773812188871277 0\\n0.5451024416058111 0.323815106228485 0\\n0.5213963664029442 0.4036219560682751 0\\n0.48272344192383343 0.6034689183844681 0\\n0.6456449638098354 0.3212092700884317 0\\n0.7846765900588496 0.5428968693419772 0\\n0.3999010439269576 0.5800086195453337 0\\n0.33960597801776315 0.5088188702950146 0\\n0.340518281001711 0.23823447387366048 0\\n0.6078074469528075 0.2842451454746161 0\\n0.6607068626062191 0.723471089446295 0\\n0.39091677149913595 0.30792861687486506 0\\n0.33474294504315116 0.41339620108362385 0\\n0.8082497266569326 0.43418450106330975 0\\n0.23952890535196691 0.5670105540438541 0\\n0.4042159323032041 0.3879351547824089 0\\n0.24072792387752895 0.6007059437806004 0\\n0.49958551878743673 0.5279721790828041 0\\n0.39474849070445184 0.41828026143555863 0\\n0.40020919456099524 0.4316697823111222 0\\n0.6070264349646128 0.7674696231127318 0\\n0.31502373150432394 0.41229346391079935 0\\n0.3619009289991707 0.5094765800329513 0\\n0.3702154039546913 0.4785993267913141 0\\n0.33656518619044096 0.4628965520033953 0\\n0.5096637671070883 0.6718873092260186 0\\n0.5121071711394137 0.4734342742800117 0\\n0.4017715303862115 0.6911561311360835 0\\n0.524762045104445 0.6104974235918463 0\\n0.3196318459555122 0.49520662959751993 0\\n0.4963975582904888 0.256346413053132 0\\n0.6165979442647048 0.32525341114278583 0\\n0.4519679951926052 0.5762321414017798 0\\n0.2619924758075499 0.43323162234675205 0\\n0.5233740831035093 0.4987415248563235 0\\n0.6635338438362427 0.5242833719072204 0\\n0.41134376902454106 0.5561512288696662 0\\n0.2290234963640514 0.5281753539097074 0\\n0.3783346575469848 0.464429526978569 0\\n0.5760326377965584 0.7331093290623751 0\\n0.6623858702947101 0.4776272755971903 0\\n0.5822551666590048 0.7372723009314582 0\\n0.6498099691191005 0.36308482495689665 0\\n0.4895837465055414 0.4626370128657613 0\\n0.5976386131030156 0.7427679881429603 0\\n0.5142618140825629 0.32271335232409326 0\\n0.3908473845308089 0.6912815296130099 0\\n0.33222615836924374 0.3987541211073984 0\\n0.2642903779958865 0.5278570976760697 0\\n0.36593993304028816 0.5878615405965368 0\\n0.5783917766162816 0.43320376384297776 0\\n0.6647801861080054 0.7680183707402277 0\\n0.6016819758938255 0.6765975551074465 0\\n0.41982168708499 0.5982599396108977 0\\n0.5498684110535639 0.3958940024849078 0\\n0.6005593791162308 0.3881096629416132 0\\n0.5062450155701644 0.25961219797199275 0\\n0.4921382444696455 0.7111236151114841 0\\n0.5289573274872326 0.5361054353705196 0\\n0.5120227084163196 0.6285364992507503 0\\n0.488175314921641 0.38716820209052094 0\\n0.44203891549875185 0.32029718272936847 0\\n0.6770061875113998 0.6022582316747018 0\\n0.7454477626398182 0.6205334062013041 0\\n0.3490690117322808 0.3698880107643031 0\\n0.7755010664965593 0.64900444631843 0\\n0.4475798772619788 0.43469829083993705 0\\n0.7278228040171991 0.33911437657707366 0\\n0.6598696664273662 0.5630675280674049 0\\n0.666105573858625 0.6508058330240868 0\\n0.3181769340393954 0.6653555028027468 0\\n0.516104499014336 0.2998399389871137 0\">, @options=Hamster::Hash[:with => \"points\"]>], @cmd=\"splot \">"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_7dfd7c()\n",
    "  dataset = circle_dataset()  \n",
    "  train_data = dataset.clone\n",
    "  train_data[\"data\"] = dataset[\"data\"][0,900]\n",
    "\n",
    "  test_data = dataset.clone\n",
    "  test_data[\"data\"] = dataset[\"data\"][900,100]\n",
    "  \n",
    "  svm_learner = SupportVectorMachineLearner.new complexity: 1.0, kernel: GaussianKernel.new(2.0)\n",
    "  svm_learner.train train_data\n",
    "  \n",
    "  gaussian_model = svm_learner.model\n",
    "  puts \"Gaussian model has #{gaussian_model[\"data\"].size} support vectors\"\n",
    "  plot_decision_boundary dataset[\"data\"], svm_learner\n",
    "end\n",
    "test_7dfd7c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5314ed8fde53238d6702c0897cd6a007",
     "grade": false,
     "grade_id": "cell-7fb60e2467cce480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3.4 (15 Points)\n",
    "\n",
    "Compare the un-kernelized version, linear kernel, and Gaussian kernels on this dataset. Provide answers to the following questions as an array below:\n",
    "\n",
    "1. Which model of the ones we tried had the best performance as measured by AUC for the Circle Dataset\n",
    " * (A) Linear Kernel\n",
    " * (B) Gaussian Kernel\n",
    " * (C) Polynomial Kernel\n",
    " * (D) They were the same\n",
    "1. If there are $N$ examples in the training set, what is the size of the Gram (or kernel) matrix during training?\n",
    " * (A) $O(N)$\n",
    " * (B) $O(N \\log N)$\n",
    " * (C) $O(N^2)$\n",
    " * (D) $O(1)$\n",
    "1. For a linear classifier, if there are $N$ examples with $d$ features in the training set, what is the time complexity of calling ```predict``` for one example?\n",
    " * (A) $O(Nd)$\n",
    " * (B) $O(d)$\n",
    " * (C) $O(N^2)$\n",
    " * (D) $O(N \\log d)$\n",
    "1. For kernelized classifier, if there are $N$ examples with $d$ features in the training set, what is the time complexity of calling ```predict``` for one example?\n",
    " * (A) $O(Nd)$\n",
    " * (B) $O(N^2)$\n",
    " * (C) $O(N \\log d)$\n",
    " * (D) $O(d)$\n",
    "1. Why do you think the Gaussian kernel would have better performance for the Circle dataset?\n",
    " * (A) Gaussian kernel did not perform better than anything else.\n",
    " * (B) The circle dataset is non-linear and the Gaussian kernel can learn non-linear boundaries.\n",
    " * (C) Hinge loss causes the linear kernel to overfit, but the Gaussian kernel is very smooth and therefore less prone to overfitting.\n",
    " * (D) The data follows a Gaussian distribution therefore Gaussian kernels should fit the best. \n",
    " \n",
    " \n",
    "If your answers were all A, A, A, A, A, then write the following (case sensitive):\n",
    "\n",
    "```ruby\n",
    "def answer_fc3f7a()\n",
    "  %w(A A A A A A)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57c18baea814201307962f2b487cd872",
     "grade": false,
     "grade_id": "cell-fc3f7a6c0c6f61dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":answer_fc3f7a"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_fc3f7a()\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  %w(B C B A B)\n",
    "\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fdcf9574045f25c3d3b56945b61856b",
     "grade": true,
     "grade_id": "cell-06cf43144bc1d855",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_06cf43()\n",
    "  t51_answers = answer_fc3f7a()\n",
    "\n",
    "  assert_not_nil t51_answers, \"1\"\n",
    "  assert_true(t51_answers.is_a?(Array))\n",
    "  assert_equal(5, t51_answers.size)\n",
    "  assert_true(t51_answers.any? {|a| a.size == 1 and a =~ /[A-Z]/})\n",
    "\n",
    "end\n",
    "test_06cf43()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
