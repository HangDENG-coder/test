{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9: Neural Networks\n",
    "\n",
    "In this assignment we will implement the [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm for a neural network using a computational graph. \n",
    "\n",
    "Each node in the network has a reference to is ancestor (not successor nodes), as follows:\n",
    "![Network configuration](assignment-9.png)\n",
    "\n",
    "We will be implementing the nodes defined above, including their ```forward``` and ```backward``` passes. In the forward pass, the node reads its input and calculates an ```activation``` which it then retains for the next layer. In the backward pass, the partial derivative of output with respect to a variable is calculated in the ```pderv``` function, recursively through the graph. Nodes are connected by setting the respecitve back pointers in the ```connect``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d414981f57894c3982f2e2e6c2b86d",
     "grade": false,
     "grade_id": "cell-7697f330b839f334",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "require './assignment_lib.rb'\n",
    "\n",
    "rng = Distribution::Normal.rng(0,1, 293891)\n",
    "\n",
    "def connect src, dst\n",
    "  dst.back_net[src.name] = src\n",
    "end\n",
    "\n",
    "concentric_circle_data = concentric_dataset()\n",
    "xor_data = xor_dataset(rng)\n",
    "two_means_data = generate_synthetic_data(rng)\n",
    "nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec9afb5d8b58a71c0aa220431038c8ab",
     "grade": false,
     "grade_id": "cell-43b66dbe7751aec9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Input\n",
    "\n",
    "Implement the ```Input``` node which contains a single column of a vector. The value of the variable held by an ```Input``` object is set by calling the ```activation``` accessor directly. In previous assignments we specify a row in a dataset as a hash of features, such as the following:\n",
    "\n",
    "```ruby\n",
    "example = {\"features\" => {\"x1\" => 1.0, \"x2\" => 3.0}}\n",
    "```\n",
    "\n",
    "Now, the ```Input``` node holds the value of a single column. Each node has a name which represents the variable is knows about. The above statement is equivalent to initializing two ```Input``` nodes once as follows:\n",
    "\n",
    "```ruby\n",
    "x1 = Input.new \"x1\"\n",
    "x2 = Input.new \"x2\"\n",
    "```\n",
    "\n",
    "Each subsequent example sets the activation value directly, as follows:\n",
    "\n",
    "```ruby\n",
    "x1.activation = 1.0\n",
    "x2.activation = 3.0\n",
    "```\n",
    "\n",
    "## Question 1.1 (5 points)\n",
    "\n",
    "Implement the ```forward``` method which calculates the activation and stores it in a member variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f85dc148a195cd4d50d983d5a7cb0e00",
     "grade": false,
     "grade_id": "cell-803a358c9669c67b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Input\n",
    "  attr_accessor :activation\n",
    "  attr_reader :back_net\n",
    "  attr_reader :name\n",
    "  \n",
    "  def initialize name    \n",
    "    @name = name\n",
    "    @back_net = nil\n",
    "  end\n",
    "\n",
    "  def forward\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end\n",
    "nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd6e687d025ab4f8fd5cee143d0af19a",
     "grade": true,
     "grade_id": "cell-c69afda4ea3e718a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_input_forward\n",
    "  i = Input.new \"x1\"\n",
    "  assert_not_nil i\n",
    "  assert_nil i.activation\n",
    "\n",
    "  i.activation = 1.0\n",
    "  assert_equal 1.0, i.activation\n",
    "\n",
    "  assert_equal 1.0, i.forward\n",
    "\n",
    "  i.activation = 2.0\n",
    "  assert_equal 2.0, i.forward\n",
    "end\n",
    "test_input_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 (5 points)\n",
    "\n",
    "Implement the ```pderv``` method which calculates partial derivative of the input with respect to ```name```. That is, this method calculates:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{\\partial x}{\\partial \\text{name}}\n",
    "\\end{align}$\n",
    "\n",
    "where $x$ is an ```Input``` object.\n",
    "\n",
    "There are two possible derivative values here, when the ```name``` is the variable represented by the input node and when it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c912d3bbdcad3397dcf1b7b6da02d4a",
     "grade": false,
     "grade_id": "cell-bfdcb2751f1f30fe",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Input\n",
    "  def pderv name\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48b916140590811a53c7e9a1be1c9011",
     "grade": true,
     "grade_id": "cell-f15d4fd1c925703c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_input_pderv\n",
    "  i = Input.new \"x\"\n",
    "  i.activation = 7.1\n",
    "  \n",
    "  assert_equal 1.0, i.pderv(\"x\")\n",
    "  assert_equal 0.0, i.pderv(\"another_variable\")\n",
    "end\n",
    "test_input_pderv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f899813f17171b66ba0c4f94bc2d37cd",
     "grade": false,
     "grade_id": "cell-e6d8ccd6f149d289",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2.1 (5 points)\n",
    "\n",
    "Implement the $L_2$ loss function assuming that there is an ```incoming``` node whose ```activation``` is known. Feedback is provided by explicitly setting ```feedback```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b3b2def4f77bbba0db3b35f98a6b5cc",
     "grade": false,
     "grade_id": "cell-48e6dd6a3ab48899",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class L2Loss\n",
    "  attr_reader :activation\n",
    "  attr_reader :back_net\n",
    "  attr_reader :name\n",
    "  attr_accessor :feedback\n",
    "\n",
    "  def initialize\n",
    "    @back_net = Hash.new    \n",
    "  end\n",
    "  \n",
    "  def incoming\n",
    "    @back_net.values.first\n",
    "  end\n",
    "  \n",
    "  def forward\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec7ca6315bf3db996506affa7fe1db5",
     "grade": true,
     "grade_id": "cell-b1909157544e7855",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_l2loss_forward\n",
    "  l2loss = L2Loss.new\n",
    "  i = Input.new \"x1\"\n",
    "  connect i, l2loss\n",
    "  \n",
    "  i.activation = 7.5\n",
    "  assert_in_delta 7.5, l2loss.forward, 1e-4\n",
    "  assert_in_delta 7.5, l2loss.activation, 1e-4\n",
    "end\n",
    "\n",
    "test_l2loss_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55fb85928a8bd973b5bdbcc981fb42da",
     "grade": false,
     "grade_id": "cell-47cd914e1144527c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2.2 (5 points)\n",
    "\n",
    "Implement the $L_2$ loss fuction based on the ```activation``` and ```feedback```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d42621b9dfdb068b4b45fa47c5c316",
     "grade": false,
     "grade_id": "cell-510cfe38c3e486f7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class L2Loss\n",
    "  def loss\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b40c21e69b4eb568768786cb509ecf",
     "grade": true,
     "grade_id": "cell-e82d2291f4fc0a73",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_l2loss_loss\n",
    "  l2loss = L2Loss.new\n",
    "  i = Input.new \"x1\"\n",
    "  connect i, l2loss\n",
    "  \n",
    "  i.activation = 7.5\n",
    "  l2loss.forward\n",
    "  l2loss.feedback = 1.0\n",
    "  assert_in_delta 21.125, l2loss.loss, 1e-4\n",
    "end\n",
    "test_l2loss_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "597115e0c12f1be2c0d06eeb25c1b6e8",
     "grade": false,
     "grade_id": "cell-b9c63047770a321e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2.3 (5 points)\n",
    "\n",
    "Implement the partial derivative function $L_2$ loss fuction based on the ```activation``` and ```feedback```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b556014ffde320dbd46194d84b1e176",
     "grade": false,
     "grade_id": "cell-4032e548c5f47d7e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class L2Loss  \n",
    "  def pderv fname    \n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0969d237c725b437237cf4c075c1743",
     "grade": true,
     "grade_id": "cell-41229b31f1d209b4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_l2loss_pderv\n",
    "  l2loss = L2Loss.new\n",
    "  i = Input.new \"x1\"\n",
    "  connect i, l2loss\n",
    "  \n",
    "  i.activation = 7.5\n",
    "  l2loss.forward\n",
    "  l2loss.feedback = 1.0\n",
    "  assert_in_delta 6.5, l2loss.pderv(\"x1\"), 1e-4\n",
    "  assert_in_delta 0.0, l2loss.pderv(\"some_other_variable\"), 1e-4\n",
    "end\n",
    "test_l2loss_pderv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21de8d3dc4028a11e521b0eae51750f7",
     "grade": false,
     "grade_id": "cell-4d32a79c2712e608",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2.4 (5 points)\n",
    "\n",
    "Implement log loss, where the activation is assumed to take the sigmoid of the input. The partial derivative uses the same form as in [Assignment 4](assignment-4/assignment-4.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7141ceab2dc49f785eadfaee3b4b2566",
     "grade": false,
     "grade_id": "cell-6ce43dd3ed69c280",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogLoss\n",
    "  attr_reader :activation\n",
    "  attr_reader :back_net\n",
    "  attr_reader :name\n",
    "  attr_accessor :feedback\n",
    "\n",
    "  def initialize\n",
    "    @back_net = Hash.new\n",
    "  end\n",
    "  \n",
    "  def incoming\n",
    "    @back_net.values.first\n",
    "  end\n",
    "\n",
    "  def forward    \n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def pderv fname    \n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def loss\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ec24cadd651c5a1829effc45e808920",
     "grade": true,
     "grade_id": "cell-e246957f8bf03e35",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_logloss\n",
    "  logloss = LogLoss.new\n",
    "  i = Input.new \"x1\"\n",
    "  connect i, logloss\n",
    "  \n",
    "  i.activation = 0.77\n",
    "  assert_in_delta 0.6835208937, logloss.forward, 1e-4\n",
    "  \n",
    "  logloss.feedback = 1\n",
    "  assert_in_delta -0.3164791063, logloss.pderv(\"x1\"), 1e-4  \n",
    "  assert_in_delta 0.3804980545, logloss.loss, 1e-4\n",
    "end\n",
    "test_logloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d440be7d9a6c516c185e69bfae6d16c9",
     "grade": false,
     "grade_id": "cell-79b2cd7267597caf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3.1 (5 points)\n",
    "\n",
    "Implement the Sigmoid activation function. Given the incoming value $x$, the ```forward``` function calculates ```@activation``` as the sigmoid of x, or:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1+ e^{-x}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "In the ```pderv``` function, calculate the partial derivative of the activation function and apply the derivative to the ```back_net``` variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "901ef72e260df6683bf23687eef69ec1",
     "grade": false,
     "grade_id": "cell-e4f0ca293dc53332",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid\n",
    "  attr_reader :activation\n",
    "  attr_reader :name\n",
    "  attr_reader :back_net\n",
    "  \n",
    "  def initialize name = \"_\"\n",
    "    @name = name\n",
    "    @back_net = Hash.new\n",
    "  end\n",
    "  \n",
    "  def incoming\n",
    "    @back_net.values.first\n",
    "  end\n",
    "  \n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea3bf9912c2e18e1411f6a6cd706a942",
     "grade": true,
     "grade_id": "cell-92522070f935aa59",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_sigmoid\n",
    "  i = Input.new \"x1\"\n",
    "  i.activation = 0.0\n",
    "\n",
    "  sig = Sigmoid.new \"s\"\n",
    "  connect i, sig\n",
    "\n",
    "  s1 = sig.forward\n",
    "  assert_in_delta 0.5, s1, 1e-4, \"sig(0.5)\"\n",
    "  p1 = sig.pderv \"x1\"\n",
    "  assert_in_delta 0.25, p1, 1e-4, \"pderv(0.5)\"\n",
    "\n",
    "  \n",
    "  i.activation = 0.775\n",
    "  s2 = sig.forward\n",
    "  assert_in_delta 0.6846015003, s2, 1e-4, \"sig(0.775)\"\n",
    "  p2 = sig.pderv \"x1\"\n",
    "  assert_in_delta 0.2159222861, p2, 1e-4, \"pderv(0.775)\"\n",
    "  \n",
    "  p3 = sig.pderv \"some_other_variable\"\n",
    "  assert_in_delta 0.0, p3, 1e-4, \"pderv(other)\"\n",
    "end\n",
    "\n",
    "test_sigmoid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81c470a79a4304ad09ab8094e1d43cff",
     "grade": false,
     "grade_id": "cell-764aacb2780ef337",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3.2 (5 points)\n",
    "\n",
    "Implement the ReLU activation function. Given the incoming value $x$, the ```forward``` function calculates ```@activation``` as a function of x, as follows:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{ReLU}(x) = \\max \\left\\{ 0, x \\right\\}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "In the ```pderv``` function, calculate the partial derivative of the activation function and apply the derivative to the ```back_net``` variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e06400f5e4a718b4ed75497d8cf2e2ef",
     "grade": false,
     "grade_id": "cell-832dc16bcc2eb5c9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ReLU\n",
    "  attr_reader :activation\n",
    "  attr_reader :name\n",
    "  attr_reader :back_net\n",
    "  \n",
    "  def initialize name\n",
    "    @name = name\n",
    "    @back_net = Hash.new\n",
    "  end\n",
    "  \n",
    "  def incoming\n",
    "    @back_net.values.first\n",
    "  end\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e187963de4278bed1c68c3d39096fd9e",
     "grade": true,
     "grade_id": "cell-e1145bcacf02e2c0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_relu\n",
    "  i = Input.new \"x1\"\n",
    "  i.activation = 0.5\n",
    "\n",
    "  relu = ReLU.new \"s\"\n",
    "  connect i, relu\n",
    "\n",
    "  s1 = relu.forward\n",
    "  assert_in_delta 0.5, s1, 1e-4, \"relu(0.5)\"\n",
    "  p1 = relu.pderv \"x1\"\n",
    "  assert_in_delta 1.0, p1, 1e-4, \"pderv(0.5)\"\n",
    "\n",
    "  \n",
    "  i.activation = -0.775\n",
    "  s2 = relu.forward\n",
    "  assert_in_delta 0.0, s2, 1e-4, \"relu(-0.775)\"\n",
    "  p2 = relu.pderv \"x1\"\n",
    "  assert_in_delta 0.0, p2, 1e-4, \"pderv(-0.775)\"\n",
    "  \n",
    "  p3 = relu.pderv \"some_other_variable\"\n",
    "  assert_in_delta 0.0, p3, 1e-4, \"pderv(other)\"\n",
    "end\n",
    "\n",
    "test_relu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3145e5e334af02725153432f7f5cca46",
     "grade": false,
     "grade_id": "cell-7f19704d9ebc3b9c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4.1 (5 points)\n",
    "\n",
    "The ```LinearUnit``` calculates the inner product between the input activations and an internal weight vector. The weight vector is stored as a reference that is common across the whole network. Therefore, each feature name needs to be unique. \n",
    "\n",
    "The diagram below shows the network configuration for Logstic Regression. Multiple ```Input``` units are connected to the ```LinearUnit```, which connected to the ```Sigmoid``` activation function. The activation function is then connected to the ```L2Loss``` output node. \n",
    "\n",
    "![Network configuration](assignment-9-Page-2.png)\n",
    "\n",
    "In the ```forward``` function, each input node in the ```back_net``` has an activation. Assume there is a pre-defined weight for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ede29c1f03b0c89eb77af28acb89b81",
     "grade": false,
     "grade_id": "cell-88b5fa776d4a2a31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LinearUnit\n",
    "  attr_reader :activation\n",
    "  attr_reader :back_net\n",
    "  attr_reader :name\n",
    "  \n",
    "  def initialize name, weights\n",
    "    @name = name\n",
    "    @weights = weights\n",
    "    @back_net = Hash.new    \n",
    "  end\n",
    "\n",
    "  def n fname\n",
    "    [@name, fname].join(\".\")\n",
    "  end\n",
    "  \n",
    "  def forward\n",
    "    @activation = @weights[n(\"bias\")]\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "    return @activation\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b44c9d497a9ff21c3190c73213a47941",
     "grade": true,
     "grade_id": "cell-aaf6342ced97a4df",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_linear_forward\n",
    "  weights = Hash.new {|h,k| h[k] = 0.7}\n",
    "  linear = LinearUnit.new(\"f\", weights)\n",
    "  assert_not_nil linear\n",
    "  assert_true(linear.back_net.empty?)\n",
    "  \n",
    "  i = Input.new \"x1\"\n",
    "  i.activation = 0.2\n",
    "  \n",
    "  connect i, linear\n",
    "  assert_same i, linear.back_net[\"x1\"]\n",
    "  \n",
    "  assert_in_delta 0.2 * 0.7 + 0.7, linear.forward, 1e-4\n",
    "  assert_in_delta 0.7, weights[\"f.x1\"], 1e-4\n",
    "  assert_in_delta 0.7, weights[\"f.bias\"], 1e-4\n",
    "  assert_equal 2, weights.size\n",
    "\n",
    "end\n",
    "\n",
    "test_linear_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d961ef076803c6565c11d1ae2d2b248",
     "grade": false,
     "grade_id": "cell-df019dd1a4204973",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4.2 (5 points)\n",
    "\n",
    "The partial derivative function for the ```LinearUnit``` has two cases. In the ```pderv_weights``` below, calculate the partial derivative for weight variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8b53e17b82e4407cde5dc095bfbff1b",
     "grade": false,
     "grade_id": "cell-c6756771988ae544",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LinearUnit\n",
    "  def pderv_weights name\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def pderv name\n",
    "    pderv_weights name\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c39d33082058f73ab2fe89594e5628ee",
     "grade": true,
     "grade_id": "cell-852aad19e2dea1d6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_linear_pderv_weights\n",
    "  weights = Hash.new {|h,k| h[k] = 0.7}\n",
    "  linear = LinearUnit.new(\"f\", weights)\n",
    "  assert_not_nil linear\n",
    "\n",
    "  i = Input.new \"x1\"\n",
    "  i.activation = 0.2  \n",
    "  connect i, linear\n",
    "\n",
    "  i.forward\n",
    "  act = linear.forward\n",
    "\n",
    "  assert_in_delta 0.2, linear.pderv_weights(\"f.x1\"), 1e-4\n",
    "  assert_in_delta 1.0, linear.pderv_weights(\"f.bias\"), 1e-4\n",
    "end\n",
    "test_linear_pderv_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d05b43052d5e85d4742b60b9dbc5340",
     "grade": false,
     "grade_id": "cell-2be4cfcad8d6c44f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 4.3 (5 points)\n",
    "\n",
    "Considering the dataset below, copy **your** ```StochasticGradientDescent``` and train a logistic regression model. Make two changes to your trainer:\n",
    "\n",
    "1. Keep learning rate is fixed, i.e., do not try to reduce the learning rate.\n",
    "1. Divide by the size of the mini-batch in the SGD not in gradient calculation\n",
    "\n",
    "The ```LogisticRegression``` function should implement these steps just like every other model we used:\n",
    "\n",
    "1. ```forward```: Calls the forward function for the input, linear unit, activation, and output nodes.\n",
    "1. ```func```: Calculate the total $L_2$ loss under a sigmoid activation function for all examples in the dataset.\n",
    "1. ```grad```: Calculate the total gradient vector for all parameters in the function. im\n",
    "\n",
    "\n",
    "Because the implementation above takes one row at a time, it will call the forward operation again in ```func``` and ```grad```.\n",
    "\n",
    "\n",
    "Paste **your** implementation of the AUCMetric class here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a388fbf02439f3ac9e37f8fa99b62011",
     "grade": false,
     "grade_id": "cell-4e4dbd804060839e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "module Metric\n",
    "  def apply scores\n",
    "  end\n",
    "end\n",
    "\n",
    "plot_dataset(two_means_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bb69aa072777605fceb5bce10bb66b8",
     "grade": false,
     "grade_id": "cell-05718ff13e51d5f7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AUCMetric \n",
    "  include Metric\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "class StochasticGradientDescent\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975b4739f000ead9e2346235f6bff12d",
     "grade": false,
     "grade_id": "cell-0c8cf1aca128ce0d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression\n",
    "  def initialize weights\n",
    "    @x1 = Input.new \"x1\"\n",
    "    @x2 = Input.new \"x2\"\n",
    "    @inner = LinearUnit.new \"wx\", weights\n",
    "    @sig = Sigmoid.new \"sig\"\n",
    "    @out = L2Loss.new\n",
    "    \n",
    "    connect @x1, @inner\n",
    "    connect @x2, @inner\n",
    "    connect @inner, @sig\n",
    "    connect @sig, @out\n",
    "  end\n",
    "  \n",
    "  def predict examples\n",
    "    examples.collect {|row| forward row}\n",
    "  end\n",
    "  \n",
    "  def forward row\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def func dataset, weights\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def grad data, weights\n",
    "    g = Hash.new {|h,k| h[k] = 0.0}\n",
    "\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "    g\n",
    "  end\n",
    "  \n",
    "  def adjust weights\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cff348b8d0903a52593b6adeeb71d564",
     "grade": true,
     "grade_id": "cell-f3d313d85586d5dd",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression data\n",
    "  cumulative_loss = 0.0\n",
    "  i = 0\n",
    "  weights = Hash.new {|h,k| h[k] = 0.0}\n",
    "  obj = LogisticRegression.new weights\n",
    "  sgd = StochasticGradientDescent.new obj, weights, 0.02\n",
    "  5.times do |epoch|\n",
    "    data.each_slice(20) do |batch|    \n",
    "      sgd.update batch\n",
    "      cumulative_loss += obj.func(batch, sgd.weights)\n",
    "      i += 1\n",
    "      puts cumulative_loss / i if i % 100 == 0\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  puts weights  \n",
    "  predictions = obj.predict data\n",
    "  scores = predictions.collect.with_index {|score, i| [score, data[i][\"label\"] > 0 ? 1 : 0]}\n",
    "  auc = AUCMetric.new.apply scores\n",
    "  puts \"AUC: #{auc}\"\n",
    "  \n",
    "  assert_true(0.5 >= (cumulative_loss / i), \"Loss < 0.5\")\n",
    "  assert_true(auc > 0.9, \"AUC > 0.9\")\n",
    "  \n",
    "  return obj\n",
    "end\n",
    "\n",
    "trained_lr_model = test_logistic_regression(two_means_data)\n",
    "nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75499e5c560f2a61dfc7d2ac13daa7bb",
     "grade": false,
     "grade_id": "cell-5c5d14ade1f52db7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Observe\n",
    "The following plot of the decision boundary should show a higher values for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "224fc3cad1f0dda0f3832b24167074df",
     "grade": false,
     "grade_id": "cell-5069205e4a7a0448",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(two_means_data, trained_lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df02145246c050f71a132db7157f819e",
     "grade": false,
     "grade_id": "cell-5decfda4731c6f92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5.1 (5 points)\n",
    "\n",
    "Neural networks connect multiple linear units together. Testing that this works. No additional code needed here, just another test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "395565af342b3f091116c6a699372dbc",
     "grade": true,
     "grade_id": "cell-ad31f8bd759ae2e8",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_hidden_forward\n",
    "  weights = {\n",
    "      \"h0.x1\" => 1.0, \"h0.bias\" => 0.0,\n",
    "      \"h1.x1\" => 2.0, \"h1.bias\" => 0.0,\n",
    "      \"f0.h0\" => 3.0, \"f0.h1\" => 4.0, \"f0.bias\" => 0.0\n",
    "    }\n",
    "  h0 = LinearUnit.new(\"h0\", weights)\n",
    "  h1 = LinearUnit.new(\"h1\", weights)\n",
    "  f0 = LinearUnit.new(\"f0\", weights)\n",
    "  \n",
    "  i = Input.new \"x1\"  \n",
    "  i.activation = 0.2\n",
    "  \n",
    "  connect i, h0\n",
    "  connect i, h1\n",
    "  \n",
    "  connect h0, f0\n",
    "  connect h1, f0\n",
    "\n",
    "  i.forward\n",
    "  h0.forward\n",
    "  h1.forward\n",
    "  f0.forward\n",
    "\n",
    "  assert_in_delta 0.2, h0.activation, 1e-4, \"h0\"\n",
    "  assert_in_delta 0.4, h1.activation, 1e-4, \"h1\"\n",
    "  assert_in_delta 0.2 * 3 + 0.4 * 4, f0.activation, 1e-4, \"f0\"\n",
    "  \n",
    "end\n",
    "test_hidden_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "277481208aef6f07cc806cbd0fd382a7",
     "grade": false,
     "grade_id": "cell-10f616e248b9f9b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5.2 (5 points)\n",
    "\n",
    "Implement the backpropagation function for the linear unit when the variable requested is _not_ one of the weights. This function is what makes the backpropagation algorithm work. We will also overwrite the ```pderv``` function here to use the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ad948436a55fd2b9390d88f5761725e",
     "grade": false,
     "grade_id": "cell-daf3ec42522a615b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class LinearUnit\n",
    "  def pderv_back name\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  \n",
    "  def pderv name\n",
    "    if name.start_with?(@name + \".\") and @weights.has_key? name\n",
    "      pderv_weights name\n",
    "    else\n",
    "      pderv_back name\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74d026197c639af87e897b109957d6a7",
     "grade": true,
     "grade_id": "cell-1fffbe32495bef9c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_hidden_backward\n",
    "  weights = {\n",
    "      \"h0.x1\" => 1.5, \"h0.bias\" => 0.0,\n",
    "      \"h1.h0\" => 2.0, \"h1.bias\" => 0.0,\n",
    "      \"h2.h0\" => 3.0, \"h2.bias\" => 0.0,    \n",
    "      \"h3.h1\" => 4.0, \"h3.h2\" => 5.0, \"h3.bias\" => 0.0    \n",
    "  }\n",
    "  \n",
    "  h0 = LinearUnit.new(\"h0\", weights)\n",
    "  h1 = LinearUnit.new(\"h1\", weights)\n",
    "  h2 = LinearUnit.new(\"h2\", weights)\n",
    "  h3 = LinearUnit.new(\"h3\", weights)\n",
    "  \n",
    "  i = Input.new \"x1\"  \n",
    "  i.activation = 0.2\n",
    "  \n",
    "  connect i, h0\n",
    "  \n",
    "  connect h0, h1\n",
    "  connect h0, h2\n",
    "  connect h1, h3\n",
    "  connect h2, h3\n",
    "\n",
    "  [i, h0, h1, h2, h3].each {|n| n.forward}\n",
    "\n",
    "  assert_in_delta 0.2, h0.pderv(\"h0.x1\"), 1e-4, \"h0\"\n",
    "  assert_in_delta 0.3, h1.pderv(\"h1.h0\"), 1e-4, \"h1\"\n",
    "  assert_in_delta 0.3, h2.pderv(\"h2.h0\"), 1e-4, \"h2\"\n",
    "  assert_in_delta 4.0 * (2 * 0.2) + 5 * (3 * 0.2), h3.pderv(\"h0.x1\"), 1e-4, \"h3\"\n",
    "  \n",
    "end\n",
    "test_hidden_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad0d4cfed4e644d4b612ff219dff65a7",
     "grade": true,
     "grade_id": "cell-91192be992563435",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_91192b\n",
    "  w1 = {\"v.x1\" => 6.21285855261965, \"v.x2\" => 5.89754417523916, \"v.bias\" => -28.0932043104677}\n",
    "  w2 = {\"t.x1\" => -6.47151832422693, \"t.x2\" => -6.25964370262662, \"t.bias\" => 21.3658877452187}\n",
    "  w3 = {\"u.z1\" => 5.79180358051235, \"u.z2\" => 5.7240375033176, \"u.bias\" => -2.83249422973696}\n",
    "  \n",
    "  x1 = Input.new \"x1\"\n",
    "  x2 = Input.new \"x2\"\n",
    "\n",
    "  v = LinearUnit.new(\"v\", w1)\n",
    "  z1 = Sigmoid.new \"z1\"\n",
    "  connect x1, v\n",
    "  connect x2, v\n",
    "  connect v, z1\n",
    "\n",
    "  t = LinearUnit.new(\"t\", w2)\n",
    "  z2 = Sigmoid.new \"z2\"\n",
    "  connect x1, t\n",
    "  connect x2, t\n",
    "  connect t, z2\n",
    "\n",
    "  u = LinearUnit.new(\"u\", w3)\n",
    "  yhat = LogLoss.new \n",
    "  \n",
    "  connect z1, u\n",
    "  connect z2, u\n",
    "  connect u, yhat\n",
    "  \n",
    "  x1.activation = 1.5\n",
    "  x2.activation = 2.0\n",
    "  \n",
    "  x1.forward; x2.forward\n",
    "  puts \"v(x) = %.4f\" % v.forward\n",
    "  puts \"z1(v) = %.4f\" % z1.forward\n",
    "  puts \"t(x) = %.4f\" % t.forward\n",
    "  puts \"z2(t) = %.4f\" % z2.forward\n",
    "  puts \"u(z) = %.4f\" % u.forward\n",
    "  puts \"yhat(u) = %.4f\" % yhat.forward\n",
    "  assert_in_delta 0.2449, yhat.activation, 1e-3\n",
    "  \n",
    "  puts \"Calculate d[j]/dw11\"\n",
    "  yhat.feedback = 1\n",
    "  puts \"d[j]/dw11 = %.4f\" % yhat.pderv(\"v.x1\")\n",
    "  assert_in_delta -0.0061, yhat.pderv(\"v.x1\"), 1e-3\n",
    "  puts \"d[z1]/dw11 = %.4f\" % z1.pderv(\"v.x1\")\n",
    "  assert_in_delta 0.0014, z1.pderv(\"v.x1\"), 1e-3\n",
    "  puts \"d[v]/dw11 = %.4f\"% v.pderv(\"v.x1\")  \n",
    "  assert_in_delta 1.5000, v.pderv(\"v.x1\"), 1e-3\n",
    "  \n",
    "  puts \"Calculate d[j]/dw21\"\n",
    "  puts \"d[j]/dw21 = %.4f\" % yhat.pderv(\"t.x1\")\n",
    "  assert_in_delta -1.3542, yhat.pderv(\"t.x1\"), 1e-3\n",
    "  puts \"d[z2]/dw21 = %.4f\" % z2.pderv(\"t.x1\")\n",
    "  assert_in_delta 0.3133, z2.pderv(\"t.x1\"), 1e-3\n",
    "  puts \"d[t]/dw21 = %.4f\"% t.pderv(\"t.x1\")  \n",
    "  assert_in_delta 1.5000, t.pderv(\"t.x1\"), 1e-3\n",
    "\n",
    "  puts \"Calculate d[j]/dw31\"\n",
    "  puts \"d[j]/dw31 = %.4f\" % yhat.pderv(\"u.z1\")\n",
    "  assert_in_delta -0.0007, yhat.pderv(\"u.z1\"), 1e-3\n",
    "  puts \"d[u]/dw31 = %.4f\"% u.pderv(\"u.z1\")  \n",
    "  assert_in_delta 0.0009, u.pderv(\"u.z1\"), 1e-3\n",
    "\n",
    "  puts \"Calculate d[j]/dwQ = 0\"\n",
    "  assert_in_delta 0.0, yhat.pderv(\"Q\"), 1e-3\n",
    "end\n",
    "test_91192b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df351d40c2a125043ed01a306b98cd26",
     "grade": false,
     "grade_id": "cell-10302be7a56919bb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 5.3 (10 points)\n",
    "\n",
    "Implement the ```NeuralNetwork``` class following the pattern for the ```LogisticRegression``` above, where there are three main operations: ```forward```, ```func```, and ```grad```. \n",
    "\n",
    "Assume that ```layers``` contains all nodes properly connected. Each layer is an array of nodes connected appropriately. Even if there is only one node in the layer, it needs to be in an array.\n",
    "\n",
    "We will verify that a 1-node neural network produces exactly the same result as the LogisticRegression model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86e1ad041adf1df4817c5ce914df0ac5",
     "grade": false,
     "grade_id": "cell-9d40194ae5b08293",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork\n",
    "  attr_reader :batch_loss\n",
    "  def initialize layers\n",
    "    @inputs = Hash.new\n",
    "    @layers = layers\n",
    "    @out = @layers[-1][0]\n",
    "    @batch_loss = 0.0\n",
    "  end\n",
    "\n",
    "  def forward example\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "    \n",
    "  def func examples, weights\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def grad examples, weights\n",
    "    update_inputs examples\n",
    "    g = Hash.new {|h,k| h[k] = 0.0}\n",
    "    # BEGIN YOUR CODE\n",
    "    raise NotImplementedError.new()\n",
    "    #END YOUR CODE\n",
    "    g\n",
    "  end\n",
    "  \n",
    "  def predict data\n",
    "    data.map do |row|\n",
    "      forward row\n",
    "    end \n",
    "  end\n",
    "  \n",
    "  def update_inputs examples\n",
    "    examples.flat_map {|r| r[\"features\"].keys}\n",
    "      .uniq\n",
    "      .reject {|k| @inputs.has_key? k}\n",
    "      .each do |k|\n",
    "        @inputs[k] = Input.new k\n",
    "        @layers.first.each {|f| connect @inputs[k], f}\n",
    "      end\n",
    "    @inputs\n",
    "  end\n",
    "  \n",
    "  def adjust weights\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d5ad75f07e7a5ec678822bb3e62f069",
     "grade": true,
     "grade_id": "cell-33bd3b385e6b9c48",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_neural_network_1_layer examples\n",
    "  cumulative_loss = 0.0\n",
    "  i = 0\n",
    "  weights = Hash.new {|h,k| h[k] = 0.0}\n",
    "  layers = [\n",
    "    [LinearUnit.new(\"wx\", weights)],\n",
    "    [Sigmoid.new(\"sig\")],\n",
    "    [L2Loss.new]\n",
    "  ]\n",
    "    \n",
    "  connect layers[0][0], layers[1][0]\n",
    "  connect layers[1][0], layers[2][0]\n",
    "\n",
    "  obj = NeuralNetwork.new layers\n",
    "  sgd = StochasticGradientDescent.new obj, weights, 0.02\n",
    "  5.times do |epoch|\n",
    "    examples.each_slice(20) do |batch|    \n",
    "      sgd.update batch\n",
    "      cumulative_loss += obj.func(batch, sgd.weights)\n",
    "      i += 1\n",
    "      puts cumulative_loss / i if i % 100 == 0\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  puts weights  \n",
    "  predictions = obj.predict examples\n",
    "  scores = predictions.collect.with_index {|score, i| [score, examples[i][\"label\"] > 0 ? 1 : 0]}\n",
    "  auc = AUCMetric.new.apply scores\n",
    "  puts \"AUC: #{auc}\"\n",
    "  \n",
    "  assert_true(0.5 >= (cumulative_loss / i), \"Loss < 0.5\")\n",
    "  assert_true(auc > 0.9, \"AUC > 0.9\")\n",
    "  \n",
    "  return obj\n",
    "end\n",
    "\n",
    "trained_lr_model2 = test_neural_network_1_layer(two_means_data)\n",
    "nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7e1a75cd4fa66030ff3077b586e4519",
     "grade": false,
     "grade_id": "cell-d16e057ca1d2d65c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 6.1 (10 points)\n",
    "Using a 2-layer neural network achieve an $\\log$ training loss of less than 0.3 on the XOR dataset. You will initialize a ```problem``` Hash with a weights initializer and the layers. You are free to choose the activation and any layers. We will standardize on the ```LogLoss``` for this dataset, which you will need to add. Don't forget to connect your nodes. You are also free to use any initializer you want.\n",
    "\n",
    "### Practice\n",
    "You may find the [TensorFlow Neural Network Playgroud](https://playground.tensorflow.org) helpful. It runs in the browser and has all the datasets we are using here.\n",
    "\n",
    "* [XOR](https://playground.tensorflow.org/#dataset=xor)\n",
    "* [Concentric Cirles](https://playground.tensorflow.org/#dataset=circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2b7dd50430013376254831bf0ffeddb",
     "grade": false,
     "grade_id": "cell-cdf45cd10fef3236",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_dataset(xor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bd24cd503a6b9240425b7d7291af6ed",
     "grade": false,
     "grade_id": "cell-a4e911311c17091e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_problem_xor\n",
    "  rng = Distribution::Normal.rng(0,0.2,293891)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return problem_xor\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07d9a95cb7ea861aec1f0cb27db0e298",
     "grade": true,
     "grade_id": "cell-f40f324e078fe152",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_neural_network_xor problem, examples\n",
    "  cumulative_loss = 0.0\n",
    "  i = 0\n",
    "\n",
    "  obj = NeuralNetwork.new problem[\"layers\"]\n",
    "  sgd = StochasticGradientDescent.new obj, problem[\"weights\"], problem[\"learning_rate\"]\n",
    "  500.times do |epoch|\n",
    "    examples.each_slice(10) do |batch|    \n",
    "      sgd.update batch\n",
    "      cumulative_loss += obj.func(batch, sgd.weights)\n",
    "      i += 1\n",
    "      puts [i, cumulative_loss / i].join(\"\\t\") if i % 100 == 0\n",
    "      break if cumulative_loss / i < 0.3\n",
    "    end\n",
    "    break if cumulative_loss / i < 0.3\n",
    "  end\n",
    "  \n",
    "  predictions = obj.predict examples\n",
    "  scores = predictions.collect.with_index {|score, i| [score, examples[i][\"label\"] > 0 ? 1 : 0]}\n",
    "  auc = AUCMetric.new.apply scores\n",
    "  puts \"AUC: #{auc}\"\n",
    " \n",
    "  assert_true(cumulative_loss / i < 0.3)\n",
    "  assert_true(auc > 0.9)\n",
    "  return obj\n",
    "end\n",
    "\n",
    "trained_xor_model = test_neural_network_xor(create_problem_xor(), xor_data)\n",
    "nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "206d4651015ee7d496621ac20bf8e108",
     "grade": false,
     "grade_id": "cell-c112f5a5167d613b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary xor_data, trained_xor_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "748b7ead37ae1aaffa2dec0bb1198097",
     "grade": false,
     "grade_id": "cell-f86b50a145241d2f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 6.1 (5 points)\n",
    "Using a 3-layer neural network achieve an $\\log$ training loss of less than 0.2 on the concentric circles dataset. You will initialize a ```problem``` Hash with a weights initializer and the layers. You are free to choose the activation and any layers. We will standardize on the ```LogLoss``` for this dataset, which you will need to add. Don't forget to connect your nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0778d4ed4a7c3a079058419c698c4c53",
     "grade": false,
     "grade_id": "cell-d7bb06ec10ca79b8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_dataset concentric_circle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5823959fc5124676767ceb4bb2743e7",
     "grade": false,
     "grade_id": "cell-f960c71875436c12",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_problem_concentric\n",
    "  rng = Distribution::Normal.rng(0,0.2,293891)\n",
    "  # BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    "  return problem_concentric\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7995c107689c9b898371405c3f28af0c",
     "grade": true,
     "grade_id": "cell-23a5f41e6aaab0d9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_neural_network_concentric problem, examples\n",
    "  cumulative_loss = 0.0\n",
    "  i = 0\n",
    "\n",
    "  obj = NeuralNetwork.new problem[\"layers\"]\n",
    "  sgd = StochasticGradientDescent.new obj, problem[\"weights\"], problem[\"learning_rate\"]\n",
    "  500.times do |epoch|\n",
    "    examples.each_slice(10) do |batch|    \n",
    "      sgd.update batch\n",
    "      cumulative_loss += obj.func(batch, sgd.weights)\n",
    "      i += 1\n",
    "      puts [i, cumulative_loss / i].join(\"\\t\") if i % 100 == 0\n",
    "      break if cumulative_loss / i < 0.4\n",
    "    end\n",
    "    break if cumulative_loss / i < 0.4\n",
    "  end\n",
    "  \n",
    "  predictions = obj.predict examples\n",
    "  scores = predictions.collect.with_index {|score, i| [score, examples[i][\"label\"] > 0 ? 1 : 0]}\n",
    "  auc = AUCMetric.new.apply scores\n",
    "  puts \"AUC: #{auc}\"\n",
    " \n",
    "  assert_true(cumulative_loss / i < 0.4)\n",
    "  assert_true(auc > 0.9)\n",
    "\n",
    "  \n",
    "  return obj\n",
    "end\n",
    "\n",
    "trained_concentric_model = test_neural_network_concentric(create_problem_concentric(), concentric_circle_data)\n",
    "nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db07b467c1544f341ee785494487c169",
     "grade": false,
     "grade_id": "cell-991e6ede47e9e771",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(concentric_circle_data, trained_concentric_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
