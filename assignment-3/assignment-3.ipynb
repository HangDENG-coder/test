{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Gradient Descent and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (5 points)\n",
    "\n",
    "Let's implement a test function for the gradient descent optimizer, a 3D simple parabola. All gradient-optimized trainers are implemented as a objective function. The follow the same basic pattern:\n",
    "\n",
    "```ruby\n",
    "class MyGradientLearnableModel\n",
    "  def func x, w\n",
    "    #Returns the value of the objective function, \n",
    "    #  summing across all examples in x\n",
    "  end\n",
    "  def grad x, w\n",
    "    #Returns a hash of derivative values for each variable in w,\n",
    "    # gradient is summed across all examples in x\n",
    "    dw = {\"0\" => w[\"0\"], \"1\" => w[\"1\"]}\n",
    "  end\n",
    "  def adjust w\n",
    "    # Applies any problem-specific alterations to w\n",
    "  end\n",
    "end\n",
    "```\n",
    "\n",
    "Now, let's implement a Parabola objective function which does not depend on the data at all. It is defined as follows:\n",
    "\n",
    "### $L(w) = \\frac{1}{2}\\left( \\left(w_{0} - 1\\right)^2 + \\left(w_{1} - 2\\right)^2 \\right)$\n",
    "\n",
    "Implement the ```func``` method for the loss function, $L(w)$. \n",
    "\n",
    "**Note**: The data, i.e., ```x``` is not actually used in this objective function. Other objective functions may use ```x```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54115f66272f6d5339cda445dcca261d",
     "grade": false,
     "grade_id": "cell-f52748ce9cfc537f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParabolaObjective\n",
    "  def func x, w\n",
    "    # BEGIN YOUR CODE\n",
    "    if w.length == 2\n",
    "          return 0.5*((w[w.keys[0]]-1.0)**2+(w[w.keys[1]]-2.0)**2)\n",
    "    elsif w.length == 1\n",
    "          return 0.5*((w[w.keys[0]]-1.0)**2)\n",
    "    end\n",
    "    #END YOUR CODE\n",
    "  end \n",
    "  def adjust w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4ac4ef8ff6b09de9799e7aada958998",
     "grade": true,
     "grade_id": "cell-91665d329bcaad34",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "t1 = ParabolaObjective.new\n",
    "assert_in_delta(0.0, t1.func([], {\"0\" => 1.0, \"1\" => 2.0}), 1e-3)\n",
    "assert_in_delta(0.5, t1.func([], {\"0\" => 1.0, \"1\" => 1.0}), 1e-3)\n",
    "assert_in_delta(0.5, t1.func([], {\"0\" => 1.0, \"1\" => 3.0}), 1e-3)\n",
    "assert_in_delta(2.5, t1.func([], {\"0\" => 3.0, \"1\" => 1.0}), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 (5 Points)\n",
    "\n",
    "Implement the gradient function for $L(w)$. It evaluates the gradient for the value of $x$ for each dimension of $w$. In this simple case, $L(w)$ does not depend on $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "336415156704e9b2ea16ae3940b2e032",
     "grade": false,
     "grade_id": "cell-b5c9699ed1ed88fa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ParabolaObjective\n",
    "  def grad x, w\n",
    "    # BEGIN YOUR CODE\n",
    "        \n",
    "    dw = Hash.new()\n",
    "    name = w.keys\n",
    "    if w.length == 2\n",
    "        dw = {name[0] => w[name[0]]-1.0, name[1] => w[name[1]]-2.0}\n",
    "    elsif w.length == 1\n",
    "        dw = {name[0] => w[name[0]]-1.0}\n",
    "    end\n",
    "    return dw\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9b03b5652d757511bb12c6642715b2d",
     "grade": true,
     "grade_id": "cell-aaa3cd8bce5fe79b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "t2 = ParabolaObjective.new\n",
    "w2_1 = t2.grad([], {\"0\" => 3.0, \"1\" => 7.0})\n",
    "assert_in_delta(2.0, w2_1[\"0\"], 1e-3)\n",
    "assert_in_delta(5.0, w2_1[\"1\"], 1e-3)\n",
    "\n",
    "w2_2 = t2.grad([], {\"0\" => -3.0, \"1\" => -7.0})\n",
    "assert_in_delta(-4.0, w2_2[\"0\"], 1e-3)\n",
    "assert_in_delta(-9.0, w2_2[\"1\"], 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (1 Point)\n",
    "\n",
    "\n",
    "Implement gradient descent for any objective function class. Your function must provide a callback which we will use to monitor its performance and possibly to halt execution. A simple example illustrating the callback is as follows:\n",
    "\n",
    "```ruby\n",
    "def gradient_descent_example dataset, w, obj, learning_rate, tol, max_iter, &block\n",
    "    iter = 1\n",
    "    until converged(last_loss, current_loss) do\n",
    "        w_last = w\n",
    "        loss = calc_loss(w)\n",
    "        w = update(w)\n",
    "        w = adjust(w)\n",
    "        iter += 1\n",
    "        break unless yield w, iter, loss\n",
    "    end\n",
    "    \n",
    "    return w\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "There are three main parts to the algorithm above:\n",
    "1. Detecting the convergence\n",
    "1. The norm can be calculated as the dot product of two vectors: $||w|| = w \\cdot w$\n",
    "1. Once the loss and gradient functions are calculated, we will update the values of each weight\n",
    "\n",
    "### Implement dot product\n",
    "In this first part, implement the dot product. The dot product below should be for sparse vectors. Use ```has_key?``` to skip entries in vector ```w``` not present in ```x```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd831dd58d1fca904a54df797cbaf64b",
     "grade": false,
     "grade_id": "cell-7af25ba96bfb8ab2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":dot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implement the error function given a weight vector, w\n",
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "   x_name = x.keys\n",
    "    w_name = w.keys\n",
    "\n",
    "    i = 0\n",
    "    dot = 0.0\n",
    "    if x_name.length == 0 \n",
    "      dot =  0.0\n",
    "    else \n",
    "        while i<x_name.length do\n",
    "\n",
    "          if w_name.include?x_name[i]\n",
    "            index = w_name.index { |x| [x_name[i]].include?(x) }\n",
    "            if x[x_name[i]].is_a? Numeric  and w[w_name[index]].is_a? Numeric\n",
    "              dot += x[x_name[i]]*w[w_name[index]]\n",
    "            else \n",
    "              dot +=0.0\n",
    "            end\n",
    "          else \n",
    "            dot += 0.0\n",
    "          end\n",
    "          i+=1\n",
    "        end  \n",
    "    end\n",
    "  return dot\n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "729b2f04141c6e728d5a50c168864202",
     "grade": true,
     "grade_id": "cell-6e56a41a0ad960bf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0}), 1e-6\n",
    "assert_in_delta 6.0, dot({\"a\" => 2.0}, {\"a\" => 3.0, \"b\" => 4.0}), 1e-6\n",
    "assert_equal 0.0, dot({}, {})\n",
    "assert_equal 0.0, dot({\"a\" => 1.0}, {\"b\" => 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 (1 Point)\n",
    "Implement the L2 norm for a vector, i.e., $\\left \\lVert x \\right \\rVert$, when represented by a hash. Hint: use ```dot``` and don't forget the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87e05e64c2e89f801ccf85a4b394f421",
     "grade": false,
     "grade_id": "cell-4b9adf1cdcc42b60",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":norm"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return Math.sqrt(dot w, w)\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c875d2607666516fcc6e381176393f8",
     "grade": true,
     "grade_id": "cell-6732c7d27e4f664a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 2.0, norm({\"a\" => 1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 2.0, norm({\"a\" => -1.41421, \"b\" => 1.41421}), 1e-2\n",
    "assert_in_delta 0.0, norm({}), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 (3 points)\n",
    "Implement a function that updates a weight vector, ```w```, given a gradient vector, ```dw```, and learning rate, ```lr```.  Do not change the original weight vector. Hint: use ```clone```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a759dd69710e353e41a8435530c516cd",
     "grade": false,
     "grade_id": "cell-7353e3fd009c70fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update_weights"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_weights(w, dw, lr)\n",
    "  # BEGIN YOUR CODE\n",
    "  i = 0\n",
    "  name = w.keys\n",
    "  new_weights = Hash.new\n",
    "  while i< w.length do\n",
    "    new_weights.store(name[i],w[name[i]] - lr * dw[name[i]])\n",
    "    i+=1\n",
    "  end\n",
    "  return new_weights\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35238b8fa6c3aab46d461bb48957817e",
     "grade": true,
     "grade_id": "cell-7890d2e9cc768dbc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "assert_in_delta 1.5, update_weights({\"a\" => 1.0}, {\"a\" => -0.25}, 2.0)[\"a\"], 1e-2\n",
    "assert_in_delta 2.5, update_weights({\"a\" => 1.0, \"b\" => 3.0}, {\"a\" => -0.25, \"b\" => 0.25}, 2.0)[\"b\"], 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (15 Points)\n",
    "\n",
    "Now, put all these functions together to implement gradient descent. This function takes the ```dataset``` and calls the function and gradient on all the examples. Hint: Increment ```iter``` before calling ```yield```.\n",
    "\n",
    "To detect the convergence, we will continue iterating until one of these two conditions occurs:\n",
    "\n",
    "1. Absolute difference between the objective function before and after parameter update is less than ```tol```.\n",
    "1. We have run more than ```max_iter``` iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd9bf72182964e413bb3974d81e358fe",
     "grade": false,
     "grade_id": "cell-54768dfb34dacd23",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":gradient_descent"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent dataset, w, obj, learning_rate, tol, max_iter, &block\n",
    "  iter = 0\n",
    "  examples = dataset[\"data\"]\n",
    "  # BEGIN YOUR CODE\n",
    "    last_loss = 0.0\n",
    "    loss = 100*tol\n",
    "  \n",
    "    until ((last_loss - loss).abs < tol) do \n",
    "\n",
    "        last_loss = obj.func(examples, w)\n",
    "        dw = obj.grad(examples, w)\n",
    "        w = update_weights(w, dw, learning_rate)\n",
    "        loss = obj.func(examples, w)\n",
    "      \n",
    "        iter +=1\n",
    "        break unless yield w, iter, loss\n",
    "        break if iter > max_iter\n",
    "    end\n",
    "\n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec458221cdd59bb7bf14ab35906c4d4b",
     "grade": true,
     "grade_id": "cell-e54d5e8595d96ad4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t4 = ParabolaObjective.new\n",
    "t4_w_init = {\"0\" => 3.0, \"1\" => 7.0}\n",
    "t4_w_goal = {\"0\" => 1.0, \"1\" => 2.0}\n",
    "t4_dataset = {\"data\" => []}\n",
    "\n",
    "t4_loss = 1.0\n",
    "t4_w = nil\n",
    "gradient_descent t4_dataset, t4_w_init, t4, 0.1, 0.001, 100 do |w, iter, loss|\n",
    "  t4_loss = loss\n",
    "  t4_w = w\n",
    "end\n",
    "\n",
    "assert_in_delta 0.01, t4_loss, 1e-2\n",
    "assert_in_delta 1.0, t4_w[\"0\"], 1e-1\n",
    "assert_in_delta 2.0, t4_w[\"1\"], 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8815332ed062dfb1962b2cb26c92e51f",
     "grade": true,
     "grade_id": "cell-208aaf7c7bf7b770",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t4 = ParabolaObjective.new\n",
    "t4_w_init = {\"0\" => 3.0, \"1\" => 7.0}\n",
    "t4_data = {\"data\" => []}\n",
    "\n",
    "t4_total_loss = 0.0\n",
    "t4_iterations = []\n",
    "t4_losses = []\n",
    "gradient_descent t4_data, t4_w_init, t4, 0.1, 0.001, 100 do |w, iter, loss|\n",
    "  t4_total_loss += loss\n",
    "  t4_iterations << iter\n",
    "  assert_true(iter > 0, \"Make sure to increment 'iter' before calling 'yield'\")\n",
    "  t4_losses << t4_total_loss / iter\n",
    "end\n",
    "\n",
    "assert_true(t4_iterations.size > 30)\n",
    "assert_true(t4_losses[-1] < 3)\n",
    "assert_true(t4_losses[-1] > 0)\n",
    "assert_true(t4_iterations[-1] > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='vis-777bfb41-ea00-4d65-87b3-e22505432ce2'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"0b75a13d-9154-46bd-9c4d-274cc554fe2f\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,39],\"yrange\":[1.584592728710167,11.745]}}],\"data\":{\"0b75a13d-9154-46bd-9c4d-274cc554fe2f\":[{\"x\":1,\"y\":11.745},{\"x\":2,\"y\":10.629224999999998},{\"x\":3,\"y\":9.654781499999999},{\"x\":4,\"y\":8.801529761249999},{\"x\":5,\"y\":8.052391285289998},{\"x\":6,\"y\":7.392864117570748},{\"x\":7,\"y\":6.810617087341976},{\"x\":8,\"y\":6.295149860653625},{\"x\":9,\"y\":5.83750789967061},{\"x\":10,\"y\":5.430043258859874},{\"x\":11,\"y\":5.06621367243318},{\"x\":12,\"y\":4.740413651781636},{\"x\":13,\"y\":4.4478323611782695},{\"x\":14,\"y\":4.184333911657656},{\"x\":15,\"y\":3.9463564372131876},{\"x\":16,\"y\":3.730826919508764},{\"x\":17,\"y\":3.5350892280490345},{\"x\":18,\"y\":3.356843259457511},{\"x\":19,\"y\":3.194093406467922},{\"x\":20,\"y\":3.045104876277066},{\"x\":21,\"y\":2.9083666188423076},{\"x\":22,\"y\":2.7825598266594387},{\"x\":23,\"y\":2.6665311352639653},{\"x\":24,\"y\":2.559269793748653},{\"x\":25,\"y\":2.4598881916189526},{\"x\":26,\"y\":2.3676052261647613},{\"x\":27,\"y\":2.2817320764085136},{\"x\":28,\"y\":2.201660018251936},{\"x\":29,\"y\":2.1268499728949624},{\"x\":30,\"y\":2.0568235287767553},{\"x\":31,\"y\":1.9911552177185536},{\"x\":32,\"y\":1.9294658599035275},{\"x\":33,\"y\":1.8714168208696798},{\"x\":34,\"y\":1.8167050477013689},{\"x\":35,\"y\":1.7650587718198771},{\"x\":36,\"y\":1.716233782808153},{\"x\":37,\"y\":1.6700101920725878},{\"x\":38,\"y\":1.6261896172740908},{\"x\":39,\"y\":1.584592728710167}]},\"extension\":[]}\n",
       "        var id_name = '#vis-777bfb41-ea00-4d65-87b3-e22505432ce2';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x000056414ff191b8 @properties={:diagrams=>[#<Nyaplot::Diagram:0x000056414fcc08d0 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"0b75a13d-9154-46bd-9c4d-274cc554fe2f\"}, @xrange=[1, 39], @yrange=[1.584592728710167, 11.745]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 39], :yrange=>[1.584592728710167, 11.745]}}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t4_iterations, y: t4_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 (5 Points)\n",
    "\n",
    "Let's learn the parameter of a Bernoulli distribution using the method of maximum likelihood. Consider the following dataset in which we are tossing a biased coin with probability $\\mu$ of returning a success (1). There is an analytical solution to find this parameter $\\mu$ given a dataset of successes and trials. Compute this analytical solution. \n",
    "\n",
    "Here the ```label``` field is either 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset format\n",
    "This is the format for the coin dataset. The format below will be used throughout most of the assignments. A dataset contains some extra details like the classes and names of features. The ```data``` entry is an array of examples containing ```features``` and a label. Notice that in this particular dataset, there aren't any \"features\", just a ```bias``` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classes\": {\n",
      "  },\n",
      "  \"features\": [\n",
      "    \"x\"\n",
      "  ],\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"features\": {\n",
      "        \"bias\": 1.0\n",
      "      },\n",
      "      \"label\": 1.0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "puts JSON.pretty_generate(coin_dataset(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "908d76a78e7e78ba9a7ab1f3beed242e",
     "grade": false,
     "grade_id": "cell-5fa56022cb1d93b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q31_binomial_parameter"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin_data = coin_dataset(1000)\n",
    "\n",
    "def q31_binomial_parameter(coin_data)\n",
    "  # BEGIN YOUR CODE\n",
    "    num = coin_data[\"data\"].length\n",
    "    i =  0\n",
    "    sum = 0\n",
    "    while i < num do\n",
    "     sum +=coin_data[\"data\"][i][\"label\"]\n",
    "      i+=1.0\n",
    "    end\n",
    "    return sum/num\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c9b7b7e94293f333e7c9ac657d48493",
     "grade": true,
     "grade_id": "cell-00e4a8fc6cd8141e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t31_coin_data = coin_dataset(10000)\n",
    "assert_in_delta 0.77, q31_binomial_parameter(t31_coin_data), 5e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 (5 Points)\n",
    "\n",
    "Now, let's use the maximum likelihood function and gradient descent to find the same parameter value. Define the objective function for a binomial distribution for multiple samples. Remember that the ```label``` is the target value and every example has only one feature, ```bias```. Learn the weight for the ```bias``` feature should converge to $w_{bias} = \\mu$.\n",
    "\n",
    "In this first step, calculate the log likelihood function for the binomial distribution of $n$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "537973344d496a0626033100a3d0060f",
     "grade": false,
     "grade_id": "cell-92354ef065795ab6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":func"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinomialModel\n",
    "  def func examples, w\n",
    "    # BEGIN YOUR CODE\n",
    "    \n",
    "    i = 0\n",
    "    sum = 0.0\n",
    "\n",
    "    while i<examples.length do\n",
    "      \n",
    "      sum += examples[i][\"label\"]*Math.log(w[\"bias\"])  +  (1-examples[i][\"label\"])*Math.log(1-w[\"bias\"])\n",
    "      i+=1\n",
    "    end\n",
    "    return -sum\n",
    "\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7eb211a6b8ebedb4c3219fe59cf8172",
     "grade": true,
     "grade_id": "cell-68cf9b7f4c38b214",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_model = BinomialModel.new\n",
    "t32_t1 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 0.0}\n",
    "t32_t2 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 1.0}\n",
    "t32_dataset = {\n",
    "  \"data\" => [t32_t1, t32_t2]\n",
    "}\n",
    "\n",
    "assert_in_delta 1.469677, t32_model.func([t32_t1], {\"bias\" => 0.77}), 1e-3\n",
    "assert_in_delta 0.261365, t32_model.func([t32_t2], {\"bias\" => 0.77}), 1e-3\n",
    "assert_in_delta 1.731041, t32_model.func(t32_dataset[\"data\"], {\"bias\" => 0.77}), 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 (5 Points)\n",
    "\n",
    "Calculate the gradient of the binomial log likelihood function over $n$ examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d49d810b64d6a0a1fd9c006508bed0d",
     "grade": false,
     "grade_id": "cell-af9a1550d76082ae",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinomialModel\n",
    "  def grad examples, w\n",
    "    # BEGIN YOUR CODE\n",
    "    g = Hash.new\n",
    "    i = 0\n",
    "    sum = 0.0\n",
    "    adjust w\n",
    "    mu = w[\"bias\"]\n",
    "    while i< examples.length do\n",
    "      x = examples[i][\"label\"]\n",
    "      sum+=(x/mu) -(1.0-x)/(1.0-mu)\n",
    "\n",
    "      i+=1\n",
    "    end\n",
    "    g.store(\"bias\",-sum) \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return g\n",
    "  end\n",
    "  \n",
    "  ## Adjusts the parameter to be within the allowable range\n",
    "  def adjust w\n",
    "    w[\"bias\"] = [[0.001, w[\"bias\"]].max, 0.999].min\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc9f101bb22d328e59943e09873508f6",
     "grade": true,
     "grade_id": "cell-d87514aa1ef00351",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t33_model = BinomialModel.new\n",
    "t33_t1 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 0.0}\n",
    "t33_t2 = {\"features\" => {\"bias\" => 1.0}, \"label\" => 1.0}\n",
    "\n",
    "t33_dataset = {\"data\" => [t33_t1, t33_t2]}\n",
    "\n",
    "assert_in_delta 4.347826, t32_model.grad([t33_t1], {\"bias\" => 0.77})[\"bias\"], 1e-3\n",
    "assert_in_delta -1.29870, t32_model.grad([t33_t2], {\"bias\" => 0.77})[\"bias\"], 1e-3\n",
    "assert_in_delta 3.049124, t32_model.grad(t33_dataset[\"data\"], {\"bias\" => 0.77})[\"bias\"], 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.4 (5 Points)\n",
    "\n",
    "Putting the objective function to work, use gradient descent to find the parameter for the binomial distribution. You get to set the learning rate. Return the learning rate you have obtained which works well. You may have to try a few until you get it right.\n",
    "\n",
    "Note that, while capable of returning the same value, this method reads the data many more times than the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the learning rate\n",
    "Here, set this function to return one number. For example, if you decide that the learning rate needs to be 1.234, implement the following. Note: 1.234 might not be the best choice.\n",
    "\n",
    "```ruby\n",
    "def q34_learning_rate\n",
    "  1.234\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9afa2f41321b339e15c344f2cc4070e5",
     "grade": false,
     "grade_id": "cell-ca57b6ffd6468fe0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q34_learning_rate"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q34_learning_rate\n",
    "  # BEGIN YOUR CODE\n",
    "  0.0000009\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "350ee4c1ef4303ce3a95adc0bd015575",
     "grade": true,
     "grade_id": "cell-03c7000ad0157d31",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"bias\"=>0.4583321657276431}\t\n",
      "11\t{\"bias\"=>0.5562088315481644}\t6889.483693674285\n",
      "21\t{\"bias\"=>0.6257456506585226}\t6496.091312352947\n",
      "31\t{\"bias\"=>0.675782285084037}\t6230.227059151687\n",
      "41\t{\"bias\"=>0.711068673747158}\t6045.616080606115\n",
      "51\t{\"bias\"=>0.7351084101302675}\t5914.802548525477\n",
      "61\t{\"bias\"=>0.7508784732892031}\t5820.058694867535\n",
      "71\t{\"bias\"=>0.7608799892513304}\t5749.676198105762\n",
      "81\t{\"bias\"=>0.7670582287438735}\t5695.955256398518\n",
      "91\t{\"bias\"=>0.7708043637932329}\t5653.859982940111\n",
      "101\t{\"bias\"=>0.7730480696542399}\t5620.081396628424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-116e5e0c-b136-4ae1-94d2-5e74386820e4'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"c43250d9-5f24-4209-a9de-f37991e1dc96\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,101],\"yrange\":[5617.0688518248935,7427.90054237226]}}],\"data\":{\"c43250d9-5f24-4209-a9de-f37991e1dc96\":[{\"x\":1,\"y\":7427.90054237226},{\"x\":2,\"y\":7355.548511007428},{\"x\":3,\"y\":7286.899171523106},{\"x\":4,\"y\":7221.663706105282},{\"x\":5,\"y\":7159.587599798151},{\"x\":6,\"y\":7100.44517890138},{\"x\":7,\"y\":7044.035214024058},{\"x\":8,\"y\":6990.177344526609},{\"x\":9,\"y\":6938.709144284548},{\"x\":10,\"y\":6889.483693674285},{\"x\":11,\"y\":6842.3675551343595},{\"x\":12,\"y\":6797.239073431282},{\"x\":13,\"y\":6753.986939386282},{\"x\":14,\"y\":6712.508969043671},{\"x\":15,\"y\":6672.711060299402},{\"x\":16,\"y\":6634.506296691206},{\"x\":17,\"y\":6597.814173999391},{\"x\":18,\"y\":6562.5599299387495},{\"x\":19,\"y\":6528.673960871017},{\"x\":20,\"y\":6496.091312352947},{\"x\":21,\"y\":6464.751232636706},{\"x\":22,\"y\":6434.596780092065},{\"x\":23,\"y\":6405.574477010238},{\"x\":24,\"y\":6377.634003467264},{\"x\":25,\"y\":6350.727925913329},{\"x\":26,\"y\":6324.811455971988},{\"x\":27,\"y\":6299.842235605242},{\"x\":28,\"y\":6275.780145357782},{\"x\":29,\"y\":6252.587132859132},{\"x\":30,\"y\":6230.227059151687},{\"x\":31,\"y\":6208.665560738688},{\"x\":32,\"y\":6187.869925523568},{\"x\":33,\"y\":6167.808981045085},{\"x\":34,\"y\":6148.452993612058},{\"x\":35,\"y\":6129.773577112736},{\"x\":36,\"y\":6111.743610418785},{\"x\":37,\"y\":6094.337162431718},{\"x\":38,\"y\":6077.529423927823},{\"x\":39,\"y\":6061.296645453451},{\"x\":40,\"y\":6045.616080606115},{\"x\":41,\"y\":6030.465934109799},{\"x\":42,\"y\":6015.825314157568},{\"x\":43,\"y\":6001.674188552269},{\"x\":44,\"y\":5987.993344226958},{\"x\":45,\"y\":5974.764349772121},{\"x\":46,\"y\":5961.969520638238},{\"x\":47,\"y\":5949.591886718339},{\"x\":48,\"y\":5937.615162048817},{\"x\":49,\"y\":5926.023716396299},{\"x\":50,\"y\":5914.802548525477},{\"x\":51,\"y\":5903.937260967007},{\"x\":52,\"y\":5893.414036126701},{\"x\":53,\"y\":5883.219613596611},{\"x\":54,\"y\":5873.341268547},{\"x\":55,\"y\":5863.766791093272},{\"x\":56,\"y\":5854.484466546774},{\"x\":57,\"y\":5845.48305647099},{\"x\":58,\"y\":5836.751780475332},{\"x\":59,\"y\":5828.280298689261},{\"x\":60,\"y\":5820.058694867535},{\"x\":61,\"y\":5812.077460085258},{\"x\":62,\"y\":5804.32747698778},{\"x\":63,\"y\":5796.80000456594},{\"x\":64,\"y\":5789.486663431973},{\"x\":65,\"y\":5782.379421575262},{\"x\":66,\"y\":5775.4705805803915},{\"x\":67,\"y\":5768.752762292798},{\"x\":68,\"y\":5762.218895919138},{\"x\":69,\"y\":5755.862205551524},{\"x\":70,\"y\":5749.676198105762},{\"x\":71,\"y\":5743.654651665027},{\"x\":72,\"y\":5737.791604220842},{\"x\":73,\"y\":5732.081342803768},{\"x\":74,\"y\":5726.518392996453},{\"x\":75,\"y\":5721.097508821968},{\"x\":76,\"y\":5715.813663000034},{\"x\":77,\"y\":5710.662037564078},{\"x\":78,\"y\":5705.638014831422},{\"x\":79,\"y\":5700.73716871928},{\"x\":80,\"y\":5695.955256398518},{\"x\":81,\"y\":5691.288210277198},{\"x\":82,\"y\":5686.73213030573},{\"x\":83,\"y\":5682.283276595062},{\"x\":84,\"y\":5677.938062339508},{\"x\":85,\"y\":5673.693047035041},{\"x\":86,\"y\":5669.544929984686},{\"x\":87,\"y\":5665.490544081578},{\"x\":88,\"y\":5661.5268498609985},{\"x\":89,\"y\":5657.650929812029},{\"x\":90,\"y\":5653.859982940111},{\"x\":91,\"y\":5650.151319571181},{\"x\":92,\"y\":5646.522356388779},{\"x\":93,\"y\":5642.970611695048},{\"x\":94,\"y\":5639.493700887033},{\"x\":95,\"y\":5636.089332139708},{\"x\":96,\"y\":5632.755302287381},{\"x\":97,\"y\":5629.489492895108},{\"x\":98,\"y\":5626.289866512307},{\"x\":99,\"y\":5623.154463100599},{\"x\":100,\"y\":5620.081396628424},{\"x\":101,\"y\":5617.0688518248935}]},\"extension\":[]}\n",
       "        var id_name = '#vis-116e5e0c-b136-4ae1-94d2-5e74386820e4';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x000056415146a2d8 @properties={:diagrams=>[#<Nyaplot::Diagram:0x0000564151445190 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"c43250d9-5f24-4209-a9de-f37991e1dc96\"}, @xrange=[1, 101], @yrange=[5617.0688518248935, 7427.90054237226]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 101], :yrange=>[5617.0688518248935, 7427.90054237226]}}>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t34 = BinomialModel.new\n",
    "t34_w_init = {\"bias\" => rand}\n",
    "t34_data = coin_dataset(10000)\n",
    "\n",
    "t34_learning_rate = q34_learning_rate()\n",
    "\n",
    "t34_total_loss = 0.0\n",
    "t34_iterations = []\n",
    "t34_losses = []\n",
    "t34_w = t34_w_init\n",
    "gradient_descent t34_data, t34_w_init, t34, t34_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t34_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t34_total_loss += loss\n",
    "  t34_iterations << iter\n",
    "  assert_true(iter > 0, \"Make sure to increment iter before calling yield\")\n",
    "  t34_losses << t34_total_loss / iter.to_f\n",
    "  t34_w = w\n",
    "end\n",
    "\n",
    "\n",
    "assert_true(t34_losses[-1] < 8000)\n",
    "assert_true(t34_losses[-1] > 0)\n",
    "assert_true(t34_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t34_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t34_iterations, y: t34_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (10 Points)\n",
    "\n",
    "The maximum likelihood method above reads the data multiple times and can benefit from prior knowledge in the form of a prior distribution for the parameter, $\\mu$. Using the Beta distribution as the conjugate prior, implement the likelihood function and its gradient. Now we are learning three parameters altogether: $w_{bias} = \\mu$, $\\alpha$, $\\beta$.\n",
    "\n",
    "First, let's compute the closed form estimator for $\\mu$ with a fixed $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fe610db1d60504b6baacd8c486dcfde",
     "grade": false,
     "grade_id": "cell-97d4bb1bc5d5a0f3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q41_closed_form_beta_binomial"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q41_closed_form_beta_binomial(coin_data, alpha, beta)\n",
    "  # BEGIN YOUR CODE\n",
    "    i=0\n",
    "    sum =0.0\n",
    "    while i < coin_data[\"data\"].length do\n",
    "      sum += coin_data[\"data\"][i][\"label\"]\n",
    "      i+=1\n",
    "    end\n",
    "    return (sum+alpha)/(alpha+beta+coin_data[\"data\"].length)\n",
    "\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af6ab15034ba413fb34cf9c99f3f715f",
     "grade": true,
     "grade_id": "cell-e089341132a19f54",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t41_coin_data = coin_dataset(10000)\n",
    "assert_in_delta 0.77, q41_closed_form_beta_binomial(t41_coin_data, 7743, 10000 - 7743), 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2 (10 points)\n",
    "\n",
    "Implement the negative log likelihood function for the beta + binomial values. Checkout this [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Maximum_likelihood) definition of the likelihood function. Remember we are interested in the negative log likelihood.\n",
    "\n",
    "A special function is needed ```GSL::Sf::lnbeta(a, b)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae3b18deff351186867bca1e042c1992",
     "grade": false,
     "grade_id": "cell-0c423e99f1c034e0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BetaBinomialModel\n",
    "  def func dataset, w\n",
    "    # BEGIN YOUR CODE\n",
    "      adjust w\n",
    "      alpha = w[\"alpha\"]\n",
    "      beta = w[\"beta\"]\n",
    "      if w[0].is_a? Numeric\n",
    "        bias = w[0]\n",
    "        w[\"bias\"]=bias\n",
    "      elsif w[\"bias\"].is_a? Numeric\n",
    "        bias = w[\"bias\"]\n",
    "        w[\"bias\"]=bias\n",
    "      end\n",
    "      adjust w\n",
    "      y = GSL::Sf::lnbeta(alpha, beta)\n",
    "      i = 0 \n",
    "      sum =0\n",
    "      num = dataset.length\n",
    "      while i<num do\n",
    " \n",
    "        sum += (alpha-1.0)* Math.log(bias) + (beta-1.0)* Math.log(1.0-bias)\n",
    "\n",
    "        i+=1\n",
    "      end\n",
    "     t32_model =  BinomialModel.new\n",
    "     sum = sum-y*num\n",
    "      return -sum+t32_model.func(dataset, {\"bias\" => bias})\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  def adjust w\n",
    "    w[\"bias\"] = [[0.001, w[\"bias\"]].max, 0.999].min\n",
    "    w[\"beta\"] = [0.0001, w[\"beta\"]].max\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18a9751dfa872fc62f990d54afd87b64",
     "grade": true,
     "grade_id": "cell-c3da75938a85ae1b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t42 = BetaBinomialModel.new\n",
    "srand 777 #seed random number generator\n",
    "t42_data = coin_dataset(1000)[\"data\"]\n",
    "t42_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "t42_w[\"alpha\"] = 7.0\n",
    "t42_w[\"beta\"] = 3.0\n",
    "\n",
    "assert_in_delta 10373.126026759332, t42.func(t42_data, t42_w), 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (10 points)\n",
    "\n",
    "Implement the negative log likelihood gradient for all the parameters. Checkout this [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Maximum_likelihood) definition of the likelihood function. Remember we are interested in the negative log likelihood. The gradient for the ```bias``` requires all examples in the dataset. However, the gradient for ```alpha``` and ```beta``` does not require a pass over the dataset.\n",
    "\n",
    "A special function is needed ```GSL::Sf::psi(a + b)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17bd13df38d687cdb957a67a4a762db0",
     "grade": false,
     "grade_id": "cell-9d4f8012ea73cfa8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BetaBinomialModel\n",
    "  def grad dataset, w\n",
    "    # BEGIN YOUR CODE\n",
    "    adjust w\n",
    "    alpha = w[\"alpha\"]\n",
    "    beta = w[\"beta\"]\n",
    "    if w[0].is_a? Numeric\n",
    "        bias = w[0]\n",
    "        w[\"bias\"]=bias\n",
    "    elsif w[\"bias\"].is_a? Numeric\n",
    "        bias = w[\"bias\"]\n",
    "        w[\"bias\"]=bias\n",
    "    end\n",
    "    \n",
    "    g = Hash.new\n",
    "    g.store(\"alpha\",-(Math.log(bias)-(-GSL::Sf::psi(alpha+beta)+GSL::Sf::psi(alpha))) )\n",
    "    g.store(\"beta\",-(Math.log(1-bias)-(-GSL::Sf::psi(alpha+beta)+GSL::Sf::psi(beta))))\n",
    "    model  = BinomialModel.new\n",
    "    g.store(\"bias\",model.grad(dataset, {\"bias\" => bias})[\"bias\"] )\n",
    "    return g\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25a6d3730d8f7b4dc750dbcb27184354",
     "grade": true,
     "grade_id": "cell-2752de436c18d1fd",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "\n",
    "t43 = BetaBinomialModel.new\n",
    "srand 777 #seed random number generator\n",
    "t43_data = coin_dataset(1000)[\"data\"]\n",
    "t43_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "t43_w[\"alpha\"] = 7.0\n",
    "t43_w[\"beta\"] = 3.0\n",
    "\n",
    "t43_grad = t43.grad(t43_data, t43_w)\n",
    "\n",
    "assert_in_delta -7902.2222222221935, t43_grad[\"bias\"], 1e2\n",
    "assert_in_delta 1.9236168390257913, t43_grad[\"alpha\"], 1e-1\n",
    "assert_in_delta -1.2236077383104214, t43_grad[\"beta\"], 1e-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4 (20 points)\n",
    "\n",
    "Run the gradient descent by selecting the initial weights and learning rate. Try a few values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "706ade57f59434796197e6bd977d6a95",
     "grade": false,
     "grade_id": "cell-963091d78ac84fe4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":q44_weights_and_learning_rate"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q44_weights_and_learning_rate\n",
    "\n",
    "  w={\"alpha\"=>5,\"beta\"=>4,\"bias\"=>0.1}\n",
    "  lr = 0.00001\n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return [w, lr]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "445d5c9fb43b646e7d49fd0774b9c231",
     "grade": true,
     "grade_id": "cell-2bf436344900b4d6",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t{\"alpha\"=>4.999983319387165, \"beta\"=>4.000007791632939, \"bias\"=>0.8530000000000271}\t\n",
      "11\t{\"alpha\"=>5.000022878583458, \"beta\"=>3.999940564007174, \"bias\"=>0.7777056784023381}\t5232.15650987812\n",
      "21\t{\"alpha\"=>5.000061188204944, \"beta\"=>3.999878645124152, \"bias\"=>0.7777000010067855}\t4675.042731068756\n",
      "31\t{\"alpha\"=>5.000099496574959, \"beta\"=>3.9998167281625765, \"bias\"=>0.7777000000001785}\t4489.134105648042\n",
      "41\t{\"alpha\"=>5.000137803819699, \"beta\"=>3.9997548126810654, \"bias\"=>0.7777000000000002}\t4396.047261470562\n",
      "51\t{\"alpha\"=>5.000176109939219, \"beta\"=>3.999692898679531, \"bias\"=>0.7777000000000002}\t4340.089138087342\n",
      "61\t{\"alpha\"=>5.0002144149335574, \"beta\"=>3.999630986157967, \"bias\"=>0.7777000000000002}\t4302.695379592603\n",
      "71\t{\"alpha\"=>5.0002527188027495, \"beta\"=>3.9995690751163657, \"bias\"=>0.7777000000000002}\t4275.909833453372\n",
      "81\t{\"alpha\"=>5.00029102154683, \"beta\"=>3.9995071655547174, \"bias\"=>0.7777000000000002}\t4255.754423406623\n",
      "91\t{\"alpha\"=>5.000329323165835, \"beta\"=>3.9994452574730164, \"bias\"=>0.7777000000000002}\t4240.019107080136\n",
      "101\t{\"alpha\"=>5.000367623659801, \"beta\"=>3.9993833508712524, \"bias\"=>0.7777000000000002}\t4227.377859052448\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-a260a0a0-b9da-4684-88eb-a1d7d50cc8cf'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"4afdf6f1-eda5-42e7-9a10-19f205313c4b\"}],\"options\":{\"x_label\":\"Iteration\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[1,101],\"yrange\":[4226.248525446593,13030.491935858397]}}],\"data\":{\"4afdf6f1-eda5-42e7-9a10-19f205313c4b\":[{\"x\":1,\"y\":13030.491935858397},{\"x\":2,\"y\":9255.534696414212},{\"x\":3,\"y\":7713.533420365818},{\"x\":4,\"y\":6866.360813427125},{\"x\":5,\"y\":6333.873666291062},{\"x\":6,\"y\":5970.586110579689},{\"x\":7,\"y\":5708.122959713649},{\"x\":8,\"y\":5510.180067373043},{\"x\":9,\"y\":5355.811273804711},{\"x\":10,\"y\":5232.15650987812},{\"x\":11,\"y\":5130.920445470448},{\"x\":12,\"y\":5046.529779668822},{\"x\":13,\"y\":4975.109319458645},{\"x\":14,\"y\":4913.884513111528},{\"x\":15,\"y\":4860.8181098044115},{\"x\":16,\"y\":4814.381151963787},{\"x\":17,\"y\":4773.404032258415},{\"x\":18,\"y\":4736.9768950536345},{\"x\":19,\"y\":4704.381368949338},{\"x\":20,\"y\":4675.042731068756},{\"x\":21,\"y\":4648.495719214214},{\"x\":22,\"y\":4624.359660156616},{\"x\":23,\"y\":4602.320083025278},{\"x\":24,\"y\":4582.114928086321},{\"x\":25,\"y\":4563.524064886674},{\"x\":26,\"y\":4546.361229085841},{\"x\":27,\"y\":4530.467751017025},{\"x\":28,\"y\":4515.707628088045},{\"x\":29,\"y\":4501.96361667873},{\"x\":30,\"y\":4489.134105648042},{\"x\":31,\"y\":4477.130594969432},{\"x\":32,\"y\":4465.8756471180795},{\"x\":33,\"y\":4455.3012109370575},{\"x\":34,\"y\":4445.347241286638},{\"x\":35,\"y\":4435.960555328336},{\"x\":36,\"y\":4427.093879429179},{\"x\":37,\"y\":4418.705050618743},{\"x\":38,\"y\":4410.756344134209},{\"x\":39,\"y\":4403.213904418302},{\"x\":40,\"y\":4396.047261470562},{\"x\":41,\"y\":4389.228917973425},{\"x\":42,\"y\":4382.733995400322},{\"x\":43,\"y\":4376.539929507424},{\"x\":44,\"y\":4370.626207345545},{\"x\":45,\"y\":4364.974139338273},{\"x\":46,\"y\":4359.566661090241},{\"x\":47,\"y\":4354.388160495029},{\"x\":48,\"y\":4349.424326454666},{\"x\":49,\"y\":4344.662016124483},{\"x\":50,\"y\":4340.089138087342},{\"x\":51,\"y\":4335.694549272281},{\"x\":52,\"y\":4331.467963767914},{\"x\":53,\"y\":4327.399871958055},{\"x\":54,\"y\":4323.481468642256},{\"x\":55,\"y\":4319.704588998551},{\"x\":56,\"y\":4316.06165140655},{\"x\":57,\"y\":4312.545606289142},{\"x\":58,\"y\":4309.149890247185},{\"x\":59,\"y\":4305.868384857796},{\"x\":60,\"y\":4302.695379592603},{\"x\":61,\"y\":4299.62553838445},{\"x\":62,\"y\":4296.653869428595},{\"x\":63,\"y\":4293.775697859918},{\"x\":64,\"y\":4290.9866409927145},{\"x\":65,\"y\":4288.282585845183},{\"x\":66,\"y\":4285.659668706371},{\"x\":67,\"y\":4283.114256535352},{\"x\":68,\"y\":4280.642929999541},{\"x\":69,\"y\":4278.242467987898},{\"x\":70,\"y\":4275.909833453372},{\"x\":71,\"y\":4273.64216045005},{\"x\":72,\"y\":4271.436742251486},{\"x\":73,\"y\":4269.291020444397},{\"x\":74,\"y\":4267.202574908275},{\"x\":75,\"y\":4265.169114596229},{\"x\":76,\"y\":4263.188469042371},{\"x\":77,\"y\":4261.258580532307},{\"x\":78,\"y\":4259.377496875345},{\"x\":79,\"y\":4257.543364726101},{\"x\":80,\"y\":4255.754423406623},{\"x\":81,\"y\":4254.008999185387},{\"x\":82,\"y\":4252.305499977038},{\"x\":83,\"y\":4250.642410422862},{\"x\":84,\"y\":4249.0182873239955},{\"x\":85,\"y\":4247.43175539734},{\"x\":86,\"y\":4245.881503325596},{\"x\":87,\"y\":4244.3662800827915},{\"x\":88,\"y\":4242.884891507113},{\"x\":89,\"y\":4241.436197107235},{\"x\":90,\"y\":4240.019107080136},{\"x\":91,\"y\":4238.632579526479},{\"x\":92,\"y\":4237.275617846959},{\"x\":93,\"y\":4235.947268306361},{\"x\":94,\"y\":4234.646617754535},{\"x\":95,\"y\":4233.372791489002},{\"x\":96,\"y\":4232.124951253977},{\"x\":97,\"y\":4230.902293360308},{\"x\":98,\"y\":4229.704046922931},{\"x\":99,\"y\":4228.52947220323},{\"x\":100,\"y\":4227.377859052448},{\"x\":101,\"y\":4226.248525446593}]},\"extension\":[]}\n",
       "        var id_name = '#vis-a260a0a0-b9da-4684-88eb-a1d7d50cc8cf';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x00005641515185e0 @properties={:diagrams=>[#<Nyaplot::Diagram:0x00005641514cb6a0 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"4afdf6f1-eda5-42e7-9a10-19f205313c4b\"}, @xrange=[1, 101], @yrange=[4226.248525446593, 13030.491935858397]>], :options=>{:x_label=>\"Iteration\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[1, 101], :yrange=>[4226.248525446593, 13030.491935858397]}}>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t44 = BetaBinomialModel.new\n",
    "t44_data = coin_dataset(10000)\n",
    "\n",
    "t44_w_init, t44_learning_rate = q44_weights_and_learning_rate()\n",
    "\n",
    "t44_total_loss = 0.0\n",
    "t44_iterations = []\n",
    "t44_losses = []\n",
    "t44_w = t34_w_init\n",
    "gradient_descent t44_data, t44_w_init, t44, t44_learning_rate, 0.001, 100 do |w, iter, loss|\n",
    "  puts [iter, w, t44_losses[-1]].join(\"\\t\") if iter % 10 == 1\n",
    "  t44_total_loss += loss\n",
    "  t44_iterations << iter\n",
    "  t44_losses << t44_total_loss / iter.to_f\n",
    "  t44_w = w\n",
    "end\n",
    "\n",
    "\n",
    "assert_true(t44_losses[-1] < 8000)\n",
    "assert_true(t44_iterations[-1] > 30)\n",
    "assert_in_delta 0.77, t44_w[\"bias\"], 5e-2\n",
    "\n",
    "### Plot the cumulative loss per iteration\n",
    "Daru::DataFrame.new({x: t44_iterations, y: t44_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Iteration\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
