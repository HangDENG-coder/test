{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835d19048b3bed93b9e21ce1fff09b99",
     "grade": false,
     "grade_id": "cell-8b9e8debc2d6eb61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Final Project: Part 3.2\n",
    "\n",
    "Implement a classifier that achieves an auc, $a>0.6$. This demonstrates that you have discovered some moderately useful features and can control how your model performs. You will not receive any extra points for a model which performs better than 0.6 in this question. \n",
    "\n",
    "See details on how to create the class at the end of the notebook. \n",
    "\n",
    "Notes:\n",
    "1. See details at the end of this notebook.\n",
    "1. Do not use the target label as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5264f4a53a28f2a4bde78230196e8292",
     "grade": false,
     "grade_id": "cell-062c85c03b64b28b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './final_project_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "211851f70a4046f895bc4ee3aba5ca42",
     "grade": false,
     "grade_id": "cell-f738197b6f6ecad1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 0 (0 Points)\n",
    "\n",
    "Paste whatever of **your code** you want to use here. You should have completed them in \"Final Project Tests\" in this folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3132454d7a127cd48a509ed9a7eca3f9",
     "grade": false,
     "grade_id": "cell-c7e36dcff627d5f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#<SQLite3::Database:0x0000558558fb8b90 @tracefunc=nil, @authorizer=nil, @encoding=nil, @busy_handler=nil, @collations={}, @functions={}, @results_as_hash=true, @type_translation=nil, @type_translator=#<Proc:0x0000558559172120 /usr/local/lib/ruby/gems/2.7.0/gems/sqlite3-1.4.2/lib/sqlite3/database.rb:722 (lambda)>, @readonly=true>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "# require './assignment_lib'\n",
    "\n",
    "#Initializes the database used for this assignment\n",
    "dir = \"/home/dataset\"\n",
    "$dev_db = SQLite3::Database.new \"#{dir}/credit_risk_data_dev.db\", results_as_hash: true, readonly: true\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":mean"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean x\n",
    "  # BEGIN YOUR CODE\n",
    "  x = x.compact\n",
    "  x.inject(0.0){|sum,i| sum + i }/x.length()  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":stdev"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stdev x\n",
    "  # BEGIN YOUR CODE\n",
    "  x = x.compact\n",
    "  mu = mean x\n",
    "  su = x.inject(0.0){|sum,i| sum + (i-mu)**2 }/(x.length()-1)\n",
    "  Math.sqrt(su) \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":dot"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implement the error function given a weight vector, w\n",
    "def dot x, w\n",
    "  # BEGIN YOUR CODE\n",
    "    x_name = x.keys\n",
    "    w_name = w.keys\n",
    "\n",
    "    i = 0\n",
    "    dot = 0.0\n",
    "    if x_name.length == 0 \n",
    "      dot =  0.0\n",
    "    else \n",
    "        while i<x_name.length do\n",
    "\n",
    "          if w_name.include?x_name[i]\n",
    "            index = w_name.index { |x| [x_name[i]].include?(x) }\n",
    "            if x[x_name[i]].is_a? Numeric  and w[w_name[index]].is_a? Numeric\n",
    "              dot += x[x_name[i]]*w[w_name[index]]\n",
    "            else \n",
    "              dot +=0.0\n",
    "            end\n",
    "          else \n",
    "            dot += 0.0\n",
    "          end\n",
    "          i+=1\n",
    "        end  \n",
    "    end\n",
    "  return dot\n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":norm"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm w\n",
    "  # BEGIN YOUR CODE\n",
    "  return Math.sqrt(dot w, w)\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":cross_validate"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # BEGIN YOUR CODE\n",
    "def cross_validate dataset, folds, &block\n",
    "  examples = dataset[\"data\"]\n",
    "  # BEGIN YOUR CODE\n",
    "  batch = examples.length/folds\n",
    "  folds.times do |fold|\n",
    "\n",
    "\n",
    "      test_data = dataset.clone\n",
    "      test_data[\"data\"] = test_data[\"data\"][fold*batch,batch]  ##CV training examples\n",
    "\n",
    "      train_data = dataset.clone\n",
    "      train_data[\"data\"] = train_data[\"data\"] - test_data[\"data\"]  ##CV testing examples\n",
    "\n",
    "    ## Call the callback like this:\n",
    "    yield train_data, test_data, fold\n",
    "  end\n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "\n",
    "# #END YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ZScoreTransformer\n",
    "  attr_reader :means, :stdevs\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new\n",
    "    @stdevs = Hash.new\n",
    "    @feature_names = feature_names    \n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "       \n",
    "    examples = dataset[\"data\"] \n",
    "\n",
    "    j = 0\n",
    "    data = Array.new(@feature_names.length) { Array.new(examples.length) }\n",
    "    while j < @feature_names.length do\n",
    "          i = 0\n",
    "          while i < examples.length do\n",
    "            data[j][i]=(examples[i][\"features\"][@feature_names[j]])\n",
    "\n",
    "            i +=1\n",
    "          end\n",
    "          \n",
    "          m = mean data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "          s = stdev data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "\n",
    "          @means[@feature_names[j]] = m\n",
    "          @stdevs[@feature_names[j]] = s\n",
    "\n",
    "          j +=1\n",
    "    end\n",
    "    return @means,@stdevs\n",
    "    # END YOUR CODE\n",
    "  end\n",
    "  \n",
    "    def apply example_batch\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    i = 0\n",
    "    feature =  (self.means).keys\n",
    "\n",
    "\n",
    "    while i< feature.length do    \n",
    "\n",
    "        if (self.means[feature[i]]).is_a? Numeric and   (self.stdevs[feature[i]]).is_a?Numeric and    (self.stdevs[feature[i]]) !=0\n",
    "          j = 0\n",
    "          while j<example_batch.length do\n",
    "              if example_batch[j][\"features\"][feature[i]].is_a?Numeric\n",
    "                example_batch[j][\"features\"][feature[i]] = (example_batch[j][\"features\"][feature[i]] - self.means[feature[i]])/self.stdevs[feature[i]]\n",
    "              end\n",
    "                j+=1\n",
    "          end\n",
    "        end\n",
    "        i +=1\n",
    "    end\n",
    "\n",
    "\n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MeanImputation\n",
    "  attr_reader :means\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new\n",
    "    @feature_names = feature_names\n",
    "  end\n",
    "  \n",
    "  def train dataset    \n",
    "    # BEGIN YOUR CODE\n",
    "    examples = dataset[\"data\"] \n",
    "\n",
    "    j = 0\n",
    "    data = Array.new(@feature_names.length) { Array.new(examples.length) }\n",
    "    while j < @feature_names.length do\n",
    "          i = 0\n",
    "          while i < examples.length do\n",
    "            data[j][i]=(examples[i][\"features\"][@feature_names[j]])\n",
    "\n",
    "            i +=1\n",
    "          end\n",
    "          \n",
    "          m = mean data[j].reject { |e| e.nil? or e.is_a? String}\n",
    "\n",
    "\n",
    "          @means[@feature_names[j]] = m\n",
    "\n",
    "\n",
    "          j +=1\n",
    "          \n",
    "    \n",
    "    end\n",
    "\n",
    "    return @means\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    example = Hash.new()\n",
    "    \n",
    "    while i< example_batch.length do    \n",
    "        \n",
    "        j = 0\n",
    "        name  = example_batch[i][\"features\"].keys\n",
    "        while j < @feature_names.length do   \n",
    "\n",
    "          if name.include?@feature_names[j]\n",
    "             if example_batch[i][\"features\"][@feature_names[j]].is_a?Numeric\n",
    "               example_batch[i][\"features\"][@feature_names[j]] = example_batch[i][\"features\"][@feature_names[j]]\n",
    "             else\n",
    "               example_batch[i][\"features\"][@feature_names[j]] = self.means[@feature_names[j]]\n",
    "             end\n",
    "          else \n",
    "             example_batch[i][\"features\"].store(@feature_names[j],self.means[@feature_names[j]])\n",
    "\n",
    "          end\n",
    "        j+=1\n",
    "        end\n",
    "\n",
    "        i +=1\n",
    "    end\n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AgeRangeAsVector\n",
    "\n",
    "  def initialize feature_name\n",
    "    @feature_name = feature_name\n",
    "  end\n",
    "  def train dataset; end\n",
    "  def apply(example_batch)\n",
    "    min_age = 0\n",
    "    max_age = 100\n",
    "    pattern = \"age_range_%d\"\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "     \n",
    "    while i< example_batch.length do        \n",
    "          if example_batch[i][\"features\"].keys.include? @feature_name\n",
    "                bin = -example_batch[i][\"features\"][@feature_name]/(365*5)  *5\n",
    "\n",
    "                example_batch[i][\"features\"].delete(@feature_name)\n",
    "\n",
    "                binned_age = min_age\n",
    "\n",
    "                while binned_age < max_age+1 do\n",
    "\n",
    "                      new_feature_name = pattern% binned_age\n",
    "\n",
    "                      if bin > binned_age or bin < (binned_age-5)    \n",
    "                        if binned_age == max_age\n",
    "                          example_batch[i][\"features\"].store(new_feature_name,1)\n",
    "                        elsif binned_age == max_age\n",
    "                          example_batch[i][\"features\"].store(new_feature_name,1)\n",
    "                        else\n",
    "                        example_batch[i][\"features\"].store(new_feature_name,nil)\n",
    "                        end\n",
    "                      else \n",
    "                        example_batch[i][\"features\"].store(new_feature_name,1)\n",
    "                      end\n",
    "\n",
    "                      binned_age+=5\n",
    "                end\n",
    "          else\n",
    "          end\n",
    "    i+=1\n",
    "    end \n",
    "\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TargetAveraging\n",
    "  attr_reader :means\n",
    "  \n",
    "  def initialize feature_names\n",
    "    @means = Hash.new {|h,k| h[k] = Hash.new}\n",
    "    @feature_names = feature_names\n",
    "    @pattern = \"avg_%s\"\n",
    "  end\n",
    "  \n",
    "  def train dataset    \n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    while i< @feature_names.length do\n",
    "\n",
    "          a = dataset.clone()[\"data\"].select{|e|  !e[\"features\"][@feature_names[i]].nil?}\n",
    "          if a.empty?\n",
    "          else\n",
    "                b = a.uniq! {|e|  e[\"features\"][@feature_names[i]] }\n",
    "                name = b.map { |e|  e[\"features\"][@feature_names[i]]  }           ######find unique value\n",
    "\n",
    "                num =Array.new(name.length,0)\n",
    "                lab_num = Array.new(name.length,0)\n",
    "                j = 0\n",
    "                while j< name.length do\n",
    "                      a = dataset.clone()[\"data\"].select{|e|  !e[\"features\"][@feature_names[i]].nil?}\n",
    "                      k = 0\n",
    "                      while k< dataset[\"data\"].length do\n",
    "                            if dataset[\"data\"][k][\"features\"][@feature_names[i]] == name[j]\n",
    "                                  num[j]+=1\n",
    "                                  lab_num[j] += dataset[\"data\"][k][\"label\"]\n",
    "                            end\n",
    "                            k+=1\n",
    "                      end\n",
    "\n",
    "                      @means[@feature_names[i]].store(name[j],lab_num[j].to_f/num[j])\n",
    "                      j+=1\n",
    "\n",
    "                end\n",
    "          end\n",
    "          i+=1\n",
    "    end\n",
    "  \n",
    "    return @means\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "    def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "    @pattern = \"avg_%s\"\n",
    "    j = 0\n",
    "    mean = self.means\n",
    "    fea = mean.keys\n",
    "\n",
    "    i = 0\n",
    "    while i< fea.length do\n",
    "      name = mean[fea[i]].keys\n",
    "      j= 0\n",
    "      \n",
    "      while j<name.length do\n",
    "        \n",
    "        k = 0        \n",
    "        while k < example_batch.length do\n",
    "\n",
    "          if !(example_batch[k][\"features\"][fea[i]]).nil?\n",
    "            value = example_batch[k][\"features\"][fea[i]]\n",
    "            example_batch[k][\"features\"].delete(fea[i])\n",
    "\n",
    "            example_batch[k][\"features\"].store(@pattern%fea[i],mean[fea[i]][value])\n",
    "          end\n",
    "          k+=1\n",
    "        end\n",
    "        \n",
    "        j+=1\n",
    "      end\n",
    "      i+=1\n",
    "    end\n",
    "\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OneHotEncoding\n",
    "  def initialize feature_names\n",
    "    @feature_names = feature_names\n",
    "    @pattern = \"%s=%s\"\n",
    "  end\n",
    "  \n",
    "  def train dataset; end\n",
    "  \n",
    "  def apply(example_batch)\n",
    "    \n",
    "    # BEGIN YOUR CODE\n",
    "    \n",
    "    j = 0\n",
    "    while j<example_batch.length do\n",
    "      \n",
    "          name = example_batch[j][\"features\"].keys\n",
    "          i = 0\n",
    "          while i< @feature_names.length do\n",
    "              if name.include?@feature_names[i] and !(example_batch[j][\"features\"][@feature_names[i]]).nil?\n",
    "\n",
    "                name_new = @pattern % [@feature_names[i], example_batch[j][\"features\"][@feature_names[i]]]\n",
    "                example_batch[j][\"features\"].delete(@feature_names[i])\n",
    "                example_batch[j][\"features\"].store(name_new,1.0)\n",
    "              end\n",
    "              i+=1\n",
    "          end\n",
    "          j+=1\n",
    "    end\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    \n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogTransform\n",
    "  def initialize feature_names\n",
    "    @feature_names = feature_names\n",
    "    @pattern = \"log_%s\"\n",
    "  end\n",
    "  \n",
    "  def train dataset; end\n",
    "  \n",
    "  def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    while i < @feature_names.length do\n",
    "      j = 0\n",
    "     \n",
    "      while j<example_batch.length do\n",
    "   \n",
    "        if (example_batch[j][\"features\"].keys).include? @feature_names[i] and  example_batch[j][\"features\"][@feature_names[i]] > 0\n",
    "                data = example_batch[j][\"features\"][@feature_names[i]]\n",
    "                example_batch[j][\"features\"].delete(@feature_names[i])\n",
    "                example_batch[j][\"features\"].store(@pattern % @feature_names[i], Math.log(data))\n",
    "        end\n",
    "        j+=1\n",
    "      end\n",
    "      \n",
    "      \n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class L2Normalize\n",
    "  def train dataset; end\n",
    "  def apply(example_batch)\n",
    "    # BEGIN YOUR CODE\n",
    "      j = 0\n",
    "      while j < example_batch.length do\n",
    "\n",
    "          name = example_batch[j][\"features\"].keys\n",
    "          i = 0\n",
    "          normal_factor = norm example_batch[j][\"features\"]\n",
    "\n",
    "          while i<name.length do\n",
    "            if example_batch[j][\"features\"][name[i]].is_a? Numeric and normal_factor!=0\n",
    "                example_batch[j][\"features\"][name[i]] = example_batch[j][\"features\"][name[i]]/normal_factor\n",
    "            else\n",
    "                example_batch[j][\"features\"][name[i]] = example_batch[j][\"features\"][name[i]]\n",
    "            end\n",
    "            i+=1\n",
    "          end\n",
    "\n",
    "\n",
    "          j+=1\n",
    "      end    \n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return example_batch\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureTransformPipeline\n",
    "  def initialize *transformers\n",
    "    @transformers = transformers\n",
    "  end\n",
    "  \n",
    "  def train dataset\n",
    "    # BEGIN YOUR CODE \n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i< @transformers.length do\n",
    "        @transformers[i].train dataset \n",
    "\n",
    "        j = 0\n",
    "        while j<dataset[\"data\"].length do\n",
    "  \n",
    "          c = dataset[\"data\"][j]\n",
    "          dataset[\"data\"][i] = (@transformers[i].apply  [c])[0]\n",
    "\n",
    "          j+=1\n",
    "        end\n",
    "      i+=1\n",
    "    end\n",
    "       \n",
    "    return dataset\n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def apply example_batch \n",
    "    return @transformers.inject(example_batch) do |u, transform|\n",
    "      u = transform.apply example_batch\n",
    "\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "class StochasticGradientDescent\n",
    "  attr_reader :weights\n",
    "  attr_reader :objective\n",
    "  def initialize obj, w_0, lr \n",
    "    @objective = obj\n",
    "    @weights = w_0\n",
    "    @n = 1.0\n",
    "    @lr = lr\n",
    "  end\n",
    "  def update x\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    lr  = @lr.to_f / Math.sqrt(@n)\n",
    "    @n +=1\n",
    "\n",
    "    if x.empty?\n",
    "    else\n",
    "      feature = x[0][\"features\"].keys\n",
    "    \n",
    "    end\n",
    "  \n",
    "    if (@weights.keys)==[0] and  x.length>0\n",
    "      i=0\n",
    "      while i<feature.length do\n",
    "        @weights.store(feature[i],@weights[0])\n",
    "        i+=1\n",
    "      end\n",
    "      \n",
    "    end\n",
    "    \n",
    "    @weights.delete(0)\n",
    "    \n",
    "    i = 0\n",
    "    gradient = @objective.grad x,@weights\n",
    "\n",
    "    while i < @weights.length do\n",
    "      if @weights[@weights.keys[i]].is_a?Numeric and gradient[@weights.keys[i]].is_a?Numeric\n",
    "        @weights[@weights.keys[i]]  = @weights[@weights.keys[i]] - lr*gradient[@weights.keys[i]]        \n",
    "      end\n",
    "      i+=1\n",
    "    end    \n",
    "\n",
    "   return @objective.adjust @weights\n",
    "\n",
    "  \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegressionModelL2\n",
    "  def initialize reg_param\n",
    "    @reg_param = reg_param\n",
    "  end\n",
    "\n",
    "  def predict row, w\n",
    "    x = row[\"features\"]    \n",
    "    return 1.0 / (1 + Math.exp(-dot(w, x)))\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].nan? or w[k].infinite?}\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].abs > 1e5 }\n",
    "  end\n",
    "  \n",
    "  def func data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    adjust w\n",
    "    i = 0\n",
    "    sum  = 0.0\n",
    "\n",
    "    while i< data.length do\n",
    "      y=(data[i][\"label\"])\n",
    "      yhat=dot(w, data[i][\"features\"])\n",
    "      sum += Math.log(Math.exp(-y*yhat)+1.0)\n",
    "      i+=1\n",
    "    end\n",
    "\n",
    "    \n",
    "    return sum/data.length + 0.5 * @reg_param *(norm(w)**2)\n",
    "  end \n",
    "   def grad data, w\n",
    "    # BEGIN YOUR CODE\n",
    "    \n",
    "    g = Hash.new()\n",
    "    key_list =[]\n",
    "    data.each do |e|\n",
    "        key_list.push((e[\"features\"].keys))\n",
    "    end\n",
    "    name = key_list.flatten.uniq{|x| x  }\n",
    "\n",
    "    sum  = Array.new(name.length)\n",
    "    \n",
    "\n",
    "    adjust w\n",
    "    \n",
    "    if (w.keys).empty?\n",
    "      i = 0\n",
    "      while i<name.length do\n",
    "        w.store(name[i],w[0])\n",
    "        i+=1\n",
    "      end\n",
    "      w.delete(0)\n",
    "    end\n",
    "    \n",
    "    norm  = norm w\n",
    "\n",
    "    i = 0\n",
    "    while i< w.length do\n",
    "      \n",
    "      sum = 0.0\n",
    "      j = 0\n",
    "      while j<data.length\n",
    "        yhat =dot  data[j][\"features\"], w\n",
    "        ylabel = data[j][\"label\"]\n",
    "        if data[j][\"features\"][name[i]].is_a? Numeric\n",
    "          sum += (1.0/(1.0+Math.exp(ylabel*yhat)))*(-ylabel *data[j][\"features\"][name[i]] )\n",
    "        end\n",
    "        j+=1\n",
    "      end\n",
    "      sum = (sum/data.length) + @reg_param * w[name[i]]\n",
    "      g.store(name[i],sum)\n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return g\n",
    "  end\n",
    "  def adjust w\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].nan? or w[k].infinite?}\n",
    "    w.each_key {|k| w[k] = 0.0 if w[k].abs > 1e5 }\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":num_negatives"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_negatives scores\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  return scores.length - num_positives(scores)\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":num_positives"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_positives scores\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "  i = 0\n",
    "  sum = 0\n",
    "  while i< scores.length do\n",
    "    if scores[i][1]>0\n",
    "      sum += 1\n",
    "    end\n",
    "    i+=1\n",
    "  end\n",
    "  return sum\n",
    "  \n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":apply"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class AUCMetric \n",
    "  include Metric  \n",
    "\n",
    "  def roc_curve(scores)\n",
    "  fp_rates = [0.0]\n",
    "  tp_rates = [0.0]\n",
    "  auc = 0.0\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "\n",
    "  t_list = scores.sort_by{|e| [-e[0]]}\n",
    " \n",
    "  i = 0\n",
    "  total_pos = num_positives scores\n",
    "  total_neg = num_negatives scores\n",
    "  p = 0.0\n",
    "  n = 0.0\n",
    "\n",
    "  t_list.each do |e|\n",
    "\n",
    "    if e[1] == 1\n",
    "      p +=1.0   \n",
    "      fp_rates.append(fp_rates[-1])      \n",
    "      tp_rates.append(p/total_pos)\n",
    "    else\n",
    "      n +=1.0 \n",
    "      fp_rates.append(n/total_neg)      \n",
    "      tp_rates.append(tp_rates[-1])\n",
    "    end\n",
    "\n",
    "  end\n",
    "\n",
    "  i = 1\n",
    "  while i < (tp_rates.length) do\n",
    "      auc+= 0.5*(fp_rates[i] -fp_rates[i-1])*(tp_rates[i] + tp_rates[i-1]  )\n",
    "    i+=1\n",
    "  end\n",
    "  \n",
    "\n",
    "  #END YOUR CODE\n",
    "    return [fp_rates, tp_rates, auc]\n",
    "  end\n",
    "\n",
    "\n",
    "\n",
    "#   def calc_auc_only scores\n",
    "#     fp, tp, auc = roc_curve scores\n",
    "#     return auc\n",
    "#   end\n",
    "  def apply scores\n",
    "    fp, tp, auc = roc_curve scores\n",
    "\n",
    "    return auc\n",
    "  end\n",
    "\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":evaluate"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN YOUR CODE\n",
    "class LogisticRegressionLearner\n",
    "  attr_reader :parameters\n",
    "  attr_reader :weights  \n",
    "  include Learner  \n",
    "\n",
    "  def initialize regularization: 0.0, learning_rate: 0.01, batch_size: 20, epochs: 1\n",
    "    @parameters = {\"regularization\" => regularization, \n",
    "      \"learning_rate\" => learning_rate, \n",
    "      \"epochs\" => epochs, \"batch_size\" => batch_size}\n",
    "  end\n",
    "\n",
    "  def train dataset\n",
    "\n",
    "\n",
    "    reg = @parameters[\"regularization\"]\n",
    "    batch_size = @parameters[\"batch_size\"]\n",
    "    num_epoch = @parameters[\"epochs\"]\n",
    "    lr = @parameters[\"learning_rate\"]\n",
    "\n",
    "    w = @weights\n",
    "\n",
    "\n",
    "    if w.nil?\n",
    "      w = Hash.new {|h,k| h[k] =0.0}\n",
    "    end\n",
    "\n",
    "   losses = []\n",
    "\n",
    "   num_epoch.times do\n",
    "\n",
    "        obj = LogisticRegressionModelL2.new(reg)\n",
    "        l = obj.func(dataset[\"data\"][0,batch_size],w)\n",
    "  \n",
    "        losses.append(l)\n",
    "        sgd = StochasticGradientDescent.new(obj, w, lr)\n",
    "        sgd.update(dataset[\"data\"][0,batch_size])\n",
    "\n",
    "        w = sgd.weights\n",
    "\n",
    "  \n",
    "  end\n",
    "  @weights = w\n",
    "  \n",
    "  return w\n",
    "   \n",
    "  end\n",
    "\n",
    "\n",
    "  def predict example \n",
    "    \n",
    "    x = example[\"features\"]    \n",
    "   \n",
    "   return dot(@weights, x)\n",
    "  end\n",
    "\n",
    "\n",
    "  def evaluate dataset\n",
    "   \n",
    "    examples = dataset[\"data\"]\n",
    "    \n",
    "    i = 0\n",
    "    score = Array.new(examples.length) { Array.new(2) }\n",
    "    while i<examples.length do      \n",
    "      x = examples[i]   \n",
    "      y = self.predict(x)   \n",
    "      score[i][0] = y\n",
    "      score[i][1] = x[\"label\"]\n",
    "      i+=1\n",
    "    end\n",
    "\n",
    "  return score\n",
    "  end\n",
    "\n",
    "\n",
    "end\n",
    "#END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add as many extra cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f481898d40e1e947e476b7e92e72772a",
     "grade": false,
     "grade_id": "cell-aebcc157f8dd3dd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 0.1\n",
    "Use the cell below to create your ```ClassifierTwo``` which creates the training set. Assume that the evaluation dataset may come from a different evaluation database so don't just return the training set again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , flag_own_car \n",
    "# , flag_own_realty\n",
    "# , organization_type\n",
    "# where ext_source_1 <> ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f82c296a0da9e2fb61fd88d3bc5254c2",
     "grade": false,
     "grade_id": "cell-6df809cc3353c1e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":create_learners"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassifierTwo\n",
    "  include FinalProjectClassifier  \n",
    "  \n",
    "  # BEGIN YOUR CODE\n",
    "  include FinalProjectClassifier\n",
    "#   include Learner\n",
    "  \n",
    "  \n",
    "  # BEGIN YOUR CODE\n",
    "  # Run SQL on the database and any transforms needed for the training database\n",
    "  def create_training_dataset training_db\n",
    " sql =  \"select target\n",
    ", sk_id_curr\n",
    ", ext_source_1\n",
    ", ext_source_2\n",
    ", ext_source_3\n",
    "from application_train\n",
    "where ext_source_1 <> ''\n",
    "order by sk_id_curr\"\n",
    " \n",
    "    \n",
    "#  sql =  \"select target\n",
    "# , sk_id_curr\n",
    "# , amt_income_total\n",
    "# , amt_credit\n",
    "# , commonarea_avg\n",
    "# , code_gender\n",
    "# , name_family_status\n",
    "# , name_housing_type\n",
    "# , name_income_type\n",
    "# , name_education_type \n",
    "# , organization_type\n",
    "# , amt_annuity\n",
    "# , days_employed\n",
    "# , own_car_age\n",
    "# , ext_source_1\n",
    "# , ext_source_2\n",
    "# , ext_source_3\n",
    "# from application_train\n",
    "# where sk_id_curr <> ''\n",
    "# order by sk_id_curr\" \n",
    "    \n",
    "#     dataset2 = create_dataset training_db, sql  \n",
    "\n",
    "#     dataset = Hash.new\n",
    "#     dataset[\"features\"] = dataset2[\"features\"]\n",
    "#     dataset[\"data\"] = dataset2[\"data\"][0,1000]\n",
    "\n",
    "    \n",
    "    dataset = create_dataset training_db, sql  \n",
    "   \n",
    "    \n",
    "##transform 1    \n",
    "    transform =  FeatureTransformPipeline.new(   \n",
    " \n",
    "#     #Treat amt_income_total and amt_credit as log normal\n",
    "#     LogTransform.new(%w(amt_income_total amt_credit)),\n",
    "#     ZScoreTransformer.new(%w(log_amt_income_total log_amt_credit)),\n",
    "#     MeanImputation.new(%w(log_amt_income_total log_amt_credit)), \n",
    "      \n",
    "#     AgeRangeAsVector.new,     \n",
    "#     OneHotEncoding.new(%w(name_family_status code_gender)),\n",
    "      \n",
    " \n",
    "#     TargetAveraging.new(%w(name_income_type flag_own_car flag_own_realty\n",
    "#       name_family_status organization_type name_housing_type name_education_type)),      \n",
    "#     L2Normalize.new\n",
    "    ZScoreTransformer.new(%w( ext_source_1 ext_source_2 ext_source_3)),   \n",
    "      \n",
    "#     LogTransform.new(%w(amt_income_total amt_credit amt_annuity )),\n",
    "#     ZScoreTransformer.new(%w(log_amt_income_total log_amt_credit amt_annuity \n",
    "# days_employed own_car_age  ext_source_1 ext_source_2 ext_source_3)),  \n",
    "#     MeanImputation.new(%w(log_amt_income_total log_amt_credit days_employed own_car_age)), \n",
    "      \n",
    "\n",
    "# #     OneHotEncoding.new(%w(name_family_status code_gender)),\n",
    "      \n",
    " \n",
    "#     TargetAveraging.new(%w(name_income_type name_family_status organization_type name_contract_type\n",
    "#     name_housing_type name_education_type code_gender )),   \n",
    "      \n",
    "\n",
    "\n",
    "#     L2Normalize.new \n",
    "  )\n",
    "##transform 2\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    transform.train dataset\n",
    "    dataset[\"data\"].map {|x| x[\"bias\"] = 1.0}\n",
    "\n",
    "    \n",
    "    return dataset\n",
    "  end\n",
    "\n",
    "  # Run SQL on the database and any transforms needed for the evaluation database\n",
    "  def create_evaluation_dataset evaluation_db\n",
    "\n",
    "    return create_training_dataset evaluation_db\n",
    "  end\n",
    "\n",
    "  #Return an array of learners\n",
    "  \n",
    "  def initialize min_auc: 0.48, max_auc: 0.6, folds: 5\n",
    "    @parameters = {\"min_auc\" => min_auc, \"max_auc\"=> max_auc, \"folds\" => folds}\n",
    "  end\n",
    "  \n",
    "  def create_learners dataset\n",
    "\n",
    "    linear = LogisticRegressionLearner.new(regularization: 0.7, learning_rate: 0.7, batch_size: 128, epochs: 1)\n",
    "    return [linear]\n",
    "  end\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set:\n",
      "[\"ext_source_1\", \"ext_source_2\", \"ext_source_3\"]\n",
      "{\"label\"=>0, \"id\"=>456251, \"features\"=>{\"ext_source_1\"=>-1.6614269210261252, \"ext_source_2\"=>0.8465539163255636}, \"bias\"=>1.0}\n",
      "\n",
      "Evaluation Set:\n",
      "[\"ext_source_1\", \"ext_source_2\", \"ext_source_3\"]\n",
      "{\"label\"=>0, \"id\"=>456251, \"features\"=>{\"ext_source_1\"=>-1.6614269210261252, \"ext_source_2\"=>0.8465539163255636}, \"bias\"=>1.0}\n",
      "\n",
      "Model trained on dev set:\n",
      "LogisticRegressionLearner\n",
      "\n",
      "Testing on\n",
      "{\"label\"=>0, \"id\"=>136052, \"features\"=>{\"ext_source_1\"=>-1.3242580128759445, \"ext_source_2\"=>-1.2482188185534955, \"ext_source_3\"=>0.9422491642496317}, \"bias\"=>1.0}\n",
      "0.007919222306940508\n"
     ]
    }
   ],
   "source": [
    "def test_75516f(test_data)\n",
    "  test_basics test_data\n",
    "end\n",
    "\n",
    "test_data_75516f = {classifier: ClassifierTwo.new, min_auc: 0.6, max_auc: 1.0, folds: 5}\n",
    "test_75516f(test_data_75516f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5abff09831ced81d0e3e9c062c228092",
     "grade": false,
     "grade_id": "cell-e2fff7a0f040db92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.1 (5 points)\n",
    "\n",
    "Validate that the classifier works by trying to create a small dataset and training the model on it. The model will be retrained later. This just verify that the interface is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3d72662c4c2524c63caf8464485a1d7",
     "grade": true,
     "grade_id": "cell-1801005798d2006d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set:\n",
      "[\"ext_source_1\", \"ext_source_2\", \"ext_source_3\"]\n",
      "{\"label\"=>0, \"id\"=>456251, \"features\"=>{\"ext_source_1\"=>-1.6614269210261252, \"ext_source_2\"=>0.8465539163255636}, \"bias\"=>1.0}\n",
      "\n",
      "Evaluation Set:\n",
      "[\"ext_source_1\", \"ext_source_2\", \"ext_source_3\"]\n",
      "{\"label\"=>0, \"id\"=>456251, \"features\"=>{\"ext_source_1\"=>-1.6614269210261252, \"ext_source_2\"=>0.8465539163255636}, \"bias\"=>1.0}\n",
      "\n",
      "Model trained on dev set:\n",
      "LogisticRegressionLearner\n",
      "\n",
      "Testing on\n",
      "{\"label\"=>0, \"id\"=>219957, \"features\"=>{\"ext_source_1\"=>0.8873429707865538, \"ext_source_2\"=>1.3311606222810841, \"ext_source_3\"=>0.9502662065655427}, \"bias\"=>1.0}\n",
      "-0.04646368646651861\n"
     ]
    }
   ],
   "source": [
    "def test_75516f(test_data)\n",
    "  test_basics test_data\n",
    "end\n",
    "\n",
    "test_data_75516f = {classifier: ClassifierTwo.new, min_auc: 0.6, max_auc: 1.0, folds: 5}\n",
    "test_75516f(test_data_75516f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c11260f0de56b248101434323be46a5d",
     "grade": false,
     "grade_id": "cell-ac3509d4d294f7b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.2 (15 points)\n",
    "\n",
    "Measures the 5-fold cross-validation performance of your classifier. This may take a while, so be cognizant of the fact that your classifier may run out of memory--the server is a shared environment.\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. This cell will timeout after 3 hours, after which you will see an ```Timeout::Error execution expired``` error. No exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":parameter_search"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### metrics.rb\n",
    "\n",
    "#### KEEP THIS AT THE TOP OF YOUR FILE ####\n",
    "def plot_roc_curve fp, tp, auc\n",
    "  plot = Daru::DataFrame.new({x: fp, y: tp}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "    plot.x_label \"False Positive Rate\"\n",
    "    plot.y_label \"True Positive Rate\"\n",
    "    diagram.title(\"AUC: %.4f\" % auc)\n",
    "    plot.legend(true)\n",
    "  end\n",
    "end  \n",
    "\n",
    "def cross_validation_model_performance dataset, folds, learners, metric    \n",
    "  learners.map do |learner|\n",
    "    tr_metrics = []\n",
    "    te_metrics = []\n",
    "    puts \"#{folds}-fold CV: #{learner.class.name}, parameters: #{learner.parameters}\"\n",
    "    cross_validate dataset, folds do |train_dataset, test_dataset|\n",
    "      learner.train train_dataset\n",
    "      train_scores = learner.evaluate train_dataset\n",
    "      test_scores = learner.evaluate test_dataset      \n",
    "      tr_metrics << metric.apply(train_scores)\n",
    "      te_metrics << metric.apply(test_scores)\n",
    "    end\n",
    "      \n",
    "    #Train on full training set\n",
    "    learner.train dataset\n",
    "    learner_name = learner.name\n",
    "    puts mean(te_metrics)\n",
    "    {\n",
    "      \"learner\" => learner_name, \"trained_model\" => learner, \"parameters\" => learner.parameters, \"folds\" => folds,\n",
    "      \"mean_train_metric\" => mean(tr_metrics), \"stdev_train_metric\" => stdev(tr_metrics),\n",
    "      \"mean_test_metric\" => mean(te_metrics), \"stdev_test_metric\" => stdev(te_metrics),\n",
    "    }\n",
    "  end\n",
    "end\n",
    "\n",
    "def best_performance_by_learner stats  \n",
    "  stats.group_by {|s| s[\"learner\"]}.map do |g_s|\n",
    "    learner, learner_stats = g_s\n",
    "    best_parameters = learner_stats.max_by {|l| l[\"mean_test_metric\"]}    \n",
    "    [learner, best_parameters]\n",
    "  end.to_h\n",
    "end\n",
    "\n",
    "def parameter_search learners, dataset, folds = 5\n",
    "  metric = AUCMetric.new  \n",
    "  stats = cross_validation_model_performance dataset, folds, learners, metric\n",
    "  best_by_learner = best_performance_by_learner stats  \n",
    "    summary = Hash.new\n",
    "    best_by_learner.each_key do |k|\n",
    "        summary[k] = best_by_learner[k].clone\n",
    "        summary[k].delete \"trained_model\"\n",
    "    end\n",
    "  puts JSON.pretty_generate(summary)\n",
    "\n",
    "  assert_equal learners.size, stats.size\n",
    "  assert_true(stats.all? {|s| a = s[\"mean_train_metric\"]; a >= 0.0 and a <= 1.0}, \"0 <= Train AUC <= 1\")\n",
    "  assert_true(stats.all? {|s| a = s[\"mean_test_metric\"]; a >= 0.0 and a <= 1.0}, \"0 <= Train AUC <= 1\")\n",
    "  \n",
    "  stats.map! {|s| t = s.clone; t.delete \"trained_model\"; t}\n",
    "  df = Daru::DataFrame.new(stats) \n",
    "    \n",
    "  return [df, best_by_learner]\n",
    "end\n",
    "\n",
    "### ADD YOUR CODE AFTER THIS LINE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaea9a774c7688f91c409ebe659e4383",
     "grade": true,
     "grade_id": "cell-9d63ad1cdc5be174",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset\n",
      "Creating learners\n",
      "Running 5-fold cross validation\n",
      "5-fold CV: LogisticRegressionLearner, parameters: {\"regularization\"=>0.7, \"learning_rate\"=>0.7, \"epochs\"=>1, \"batch_size\"=>128}\n"
     ]
    }
   ],
   "source": [
    "def test_9e9453(test_data)\n",
    "  run_cross_validation_performance test_data\n",
    "  test_data[:cross_validation_results]\n",
    "end\n",
    "test_9e9453(test_data_75516f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9b31135f549f8436522d5c4e11ecb92",
     "grade": true,
     "grade_id": "cell-0cd2f28ec1e90fdd",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_758950(test_data)\n",
    "  test_cross_validation_performance test_data\n",
    "end\n",
    "test_758950(test_data_75516f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160099fc989cf59d3fca7b5a6fc544fe",
     "grade": false,
     "grade_id": "cell-3fce57788668a629",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.3 (20 points)\n",
    "\n",
    "Plots the ROC curve on the dev dataset, which is a separate database. Checks that the AUC is within target. Note that this assumes the model has already been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b76cae794c2f82016f5b7de0fbaaf97b",
     "grade": true,
     "grade_id": "cell-4e39182e6d5ebcac",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluation dataset\n",
      "Evaluating classifier\n",
      "[[456251, 0.0], [100007, 0.0], [100015, 0.0], [100025, 0.0], [100068, 0.0]]\n",
      "Validating predictions against labels from database\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "There is no prediction for 100002. Make sure you are not removing any records.",
     "output_type": "error",
     "traceback": [
      "\u001b[31mArgumentError\u001b[0m: There is no prediction for 100002. Make sure you are not removing any records.",
      "/home/hangdeng/final-project-3.2/final_project_lib.rb:92:in `block in get_labels_for'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/sqlite3-1.4.2/lib/sqlite3/database.rb:199:in `block (2 levels) in execute'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/sqlite3-1.4.2/lib/sqlite3/resultset.rb:134:in `each'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/sqlite3-1.4.2/lib/sqlite3/database.rb:198:in `block in execute'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/sqlite3-1.4.2/lib/sqlite3/database.rb:151:in `prepare'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/sqlite3-1.4.2/lib/sqlite3/database.rb:193:in `execute'",
      "/home/hangdeng/final-project-3.2/final_project_lib.rb:89:in `get_labels_for'",
      "/home/hangdeng/final-project-3.2/final_project_lib.rb:200:in `test_evaluation_set_performance'",
      "(pry):1321:in `test_f67f50'",
      "(pry):1323:in `<main>'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:290:in `eval'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:290:in `evaluate_ruby'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:659:in `handle_line'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:261:in `block (2 levels) in eval'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:260:in `catch'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:260:in `block in eval'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:259:in `catch'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/pry-0.13.1/lib/pry/pry_instance.rb:259:in `eval'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/backend.rb:66:in `eval'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/backend.rb:12:in `eval'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/kernel.rb:90:in `execute_request'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/kernel.rb:49:in `dispatch'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/kernel.rb:38:in `run'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/command.rb:110:in `run_kernel'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/lib/iruby/command.rb:40:in `run'",
      "/usr/local/lib/ruby/gems/2.7.0/gems/iruby-0.4.0/bin/iruby:5:in `<top (required)>'",
      "/usr/local/bin/iruby:23:in `load'",
      "/usr/local/bin/iruby:23:in `<main>'"
     ]
    }
   ],
   "source": [
    "def test_f67f50(test_data)\n",
    "  test_data[:db] = dev_db()\n",
    "  test_data[:db_size] = 15334\n",
    "  test_data[:name] = \"dev_eval\"\n",
    "  test_evaluation_set_performance test_data\n",
    "end\n",
    "test_f67f50(test_data_75516f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "731144b81e8bee35f3cdc430407ed362",
     "grade": false,
     "grade_id": "cell-84e07b9103cac38c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.3 (60 Points)\n",
    "\n",
    "Tests your model on the test dataset. This is a hidden test, so you will not see this until after you submit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65649e60e20caab9c5ab05c6705d86a2",
     "grade": true,
     "grade_id": "cell-dfab784a227eec3c",
     "locked": true,
     "points": 60,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_2eca1f(test_data)\n",
    "  hidden_test_data = test_data.clone\n",
    "end\n",
    "test_2eca1f(test_data_75516f)\n",
    "test_data_75516f = nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "We will use all the models, feature extraction, and evaluation methods you learned in this course on a realistic dataset. In the final project, you will create better and better classifiers for the final project dataset. In each set of classifiers, create a dataset directly from the database. Assume that the database will be provided to you and **do not assume** that the database will be the same all the time.\n",
    "\n",
    "Note that the test set performance will be a **HIDDEN** test. You may find that your model does not perform as well on the hidden test as on the training set. To mitigate this, we are using cross-validation and a hold-out set which is about the same size as the testing set. \n",
    "\n",
    "Note the following requirements:\n",
    "1. Focus on data preparation and creating, normalizing, understanding new features.\n",
    "1. You may use R notebooks on this server for exploration but anything you obtain there must run in this notebook\n",
    "1. Use your own implementation of models, which you will have refactored as part of [Final Project Tests](./final_project_tests.ipynb)\n",
    "1. Avoid creating new models that we did not cover in the class. There is no credit for fancy models.\n",
    "1. Do not use the target label, or anything that is derived based on the training label as features. \n",
    "1. You may talk to other students about your solution, but do not share code. This is an individual assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this and all subsequent questions, create your own class following the ```FinalProjectClassifier``` interface below.\n",
    "\n",
    "Each example in the dataset you create will have an ```id``` field. This is how we will track performance. Please note that that you must provide a score for each example in the evaluation database provided to your classifier. So please take care to keep all examples in the evaluation set. You may sample the training set as much as you want. \n",
    "\n",
    "The scores will extracted from the database and your ```AUCMetric``` will be applied.\n",
    "\n",
    "Here is an example of a classifier:\n",
    "```ruby\n",
    "class ClassifierOne\n",
    "  include FinalProjectClassifier\n",
    "  \n",
    "  # Run SQL on the database and any transforms needed for the training database\n",
    "  def create_training_dataset training_db\n",
    "    sql = <<SQL\n",
    "select sk_id_curr, target\n",
    "from application_train  \n",
    "order by sk_id_curr\n",
    "SQL\n",
    "    dataset = create_dataset training_db, sql    \n",
    "    dataset[\"data\"].map {|x| x[\"bias\"] = 1.0}\n",
    "    return dataset\n",
    "  end\n",
    "  \n",
    "  # Run SQL on the database and any transforms needed for the evaluation database\n",
    "  def create_evaluation_dataset evaluation_db\n",
    "    return create_training_dataset evaluation_db\n",
    "  end\n",
    "  \n",
    "  #Return an array of learners\n",
    "  def create_learners dataset\n",
    "    linear = LogisticRegressionLearner.new(regularization: 0.01, learning_rate: 0.001, batch_size: 20, epochs: 1)\n",
    "    return [linear]\n",
    "  end\n",
    "\n",
    "end\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
