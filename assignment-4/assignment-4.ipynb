{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d09897b4a8b4e15f9e660234bd03c2c",
     "grade": false,
     "grade_id": "cell-2db1ec9fd61f5d6b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment 4: Linear Models (2)\n",
    "\n",
    "\n",
    "In this exercise, we will transform our gradient descent algorithm into stochastic gradent descent. We will then implement linear regression and logistic regression. Finally, we will run these models on a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ca0d151ba43bb4deb857d7f8c49f505",
     "grade": false,
     "grade_id": "cell-af1d85683fc29192",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if(window['d3'] === undefined ||\n",
       "   window['Nyaplot'] === undefined){\n",
       "    var path = {\"d3\":\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\",\"downloadable\":\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\"};\n",
       "\n",
       "\n",
       "\n",
       "    var shim = {\"d3\":{\"exports\":\"d3\"},\"downloadable\":{\"exports\":\"downloadable\"}};\n",
       "\n",
       "    require.config({paths: path, shim:shim});\n",
       "\n",
       "\n",
       "require(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\n",
       "\n",
       "\tvar script = d3.select(\"head\")\n",
       "\t    .append(\"script\")\n",
       "\t    .attr(\"src\", \"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\")\n",
       "\t    .attr(\"async\", true);\n",
       "\n",
       "\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\n",
       "\n",
       "\n",
       "\t    var event = document.createEvent(\"HTMLEvents\");\n",
       "\t    event.initEvent(\"load_nyaplot\",false,false);\n",
       "\t    window.dispatchEvent(event);\n",
       "\t    console.log('Finished loading Nyaplotjs');\n",
       "\n",
       "\t};\n",
       "\n",
       "\n",
       "});});\n",
       "}\n"
      ],
      "text/plain": [
       "\"if(window['d3'] === undefined ||\\n   window['Nyaplot'] === undefined){\\n    var path = {\\\"d3\\\":\\\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\\\",\\\"downloadable\\\":\\\"http://cdn.rawgit.com/domitry/d3-downloadable/master/d3-downloadable\\\"};\\n\\n\\n\\n    var shim = {\\\"d3\\\":{\\\"exports\\\":\\\"d3\\\"},\\\"downloadable\\\":{\\\"exports\\\":\\\"downloadable\\\"}};\\n\\n    require.config({paths: path, shim:shim});\\n\\n\\nrequire(['d3'], function(d3){window['d3']=d3;console.log('finished loading d3');require(['downloadable'], function(downloadable){window['downloadable']=downloadable;console.log('finished loading downloadable');\\n\\n\\tvar script = d3.select(\\\"head\\\")\\n\\t    .append(\\\"script\\\")\\n\\t    .attr(\\\"src\\\", \\\"http://cdn.rawgit.com/domitry/Nyaplotjs/master/release/nyaplot.js\\\")\\n\\t    .attr(\\\"async\\\", true);\\n\\n\\tscript[0][0].onload = script[0][0].onreadystatechange = function(){\\n\\n\\n\\t    var event = document.createEvent(\\\"HTMLEvents\\\");\\n\\t    event.initEvent(\\\"load_nyaplot\\\",false,false);\\n\\t    window.dispatchEvent(event);\\n\\t    console.log('Finished loading Nyaplotjs');\\n\\n\\t};\\n\\n\\n});});\\n}\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f069a274046e5b7da8b642701fac01d4",
     "grade": false,
     "grade_id": "cell-dab91214224425c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1.1 (10 points)\n",
    "\n",
    "Transform your batch gradient descent into a stochastic gradient descent algorithm. Using a class here allows the SGD algorithm to maintain state, such as learning rate and weights. Test your algorithm on the coin dataset and Binomial model you coded in Assignment 3. Plot the likelihood measured for each batch of 100 examples on one pass of the dataset.\n",
    "\n",
    "We will implement mini-batch SGD. Therefore you will not have to alter your Binomial Model. \n",
    "\n",
    "Stochasic gradient descent requires a learning rate that decreases with each iteration. Use the following learning rate:\n",
    "\n",
    "## $\\eta = \\frac{\\eta_{0}}{\\sqrt{t}}$\n",
    "\n",
    "where $\\eta_{0}$ is the initial learning rate and $t$ is the number of mini-batch iterations.\n",
    "\n",
    "The ```update``` function updates the member variable ```@weights``` and potentially other parameters based on the dataset. For SGD, now ```x``` is not the full dataset but a batch of examples. It needs to call the ```grad``` function in the objective, and there is no return value.\n",
    "\n",
    "\n",
    "Example lines with ```__note__``` indicate placeholders for code:\n",
    "\n",
    "```ruby\n",
    "def update x\n",
    "    @objective.grad(x, @weights)\n",
    "    learning_rate = __calculate_learning_rate__\n",
    "    __update_each_weight__ #based on the update formula and the learning rate\n",
    "    __don_t_forget_to_call_adjust__\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9660cfb6ed8d474caab10b4cd3ecccbb",
     "grade": false,
     "grade_id": "cell-8692d024ef034147",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":update"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StochasticGradientDescent\n",
    "  attr_reader :weights\n",
    "  attr_reader :objective\n",
    "  def initialize obj, w_0, lr = 0.01\n",
    "    @objective = obj\n",
    "    @weights = w_0\n",
    "    @n = 1.0\n",
    "    @lr = lr\n",
    "  end\n",
    "  def update x\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    lr  = @lr.to_f / Math.sqrt(@n)\n",
    "    @n +=1\n",
    "\n",
    "    i = 0\n",
    "    gradient = @objective.grad x,@weights\n",
    "    while i < @weights.length do\n",
    "      if @weights[@weights.keys[i]].is_a?Numeric and gradient[@weights.keys[i]].is_a?Numeric\n",
    "        @weights[@weights.keys[i]]  = @weights[@weights.keys[i]] - lr*gradient[@weights.keys[i]]\n",
    "      end\n",
    "      i+=1\n",
    "    end    \n",
    "    \n",
    "   @objective.adjust @weights\n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b40b7441e541b823fa2baa2fa02bec2",
     "grade": true,
     "grade_id": "cell-6db1274572f3953f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"0\"=>1.23, \"1\"=>4.56}\n"
     ]
    }
   ],
   "source": [
    "### TEST ###\n",
    "# Demonstrates that weights are stored in the gradient function. \n",
    "# This is included in the provided code, so just an example of how to use SGD\n",
    "class EmptyObjective\n",
    "  def func x, w; end\n",
    "  def grad x, w; end\n",
    "  def adjust w; end\n",
    "end\n",
    "\n",
    "t0_w = {\"0\" => 1.23, \"1\" => 4.56}\n",
    "t0_obj = EmptyObjective.new\n",
    "t0_sgd = StochasticGradientDescent.new t0_obj, t0_w, 0.25\n",
    "\n",
    "puts t0_sgd.weights\n",
    "assert_in_delta 1.23, t0_sgd.weights[\"0\"], 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "003125faec630a1213aaa440f0516e36",
     "grade": true,
     "grade_id": "cell-b6fbee77946aa56f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "{\"x0\"=>0.25, \"z1\"=>0.5}\n",
      "1.40625\n"
     ]
    }
   ],
   "source": [
    "### TEST ###\n",
    "# Perform a single update to the weights and see that the objective function value is different.\n",
    "# This is a no-data objective, so we are not providing any data, so our batch of examples is empty.\n",
    "class ParabolaObjective\n",
    "  def func x, w\n",
    "    0.5 * ((w[\"x0\"] - 1) ** 2.0 + (w[\"z1\"] - 2) ** 2.0)\n",
    "  end\n",
    "  def grad x, w\n",
    "    dw = {\"x0\" => (w[\"x0\"] - 1), \"z1\" => (w[\"z1\"] - 2)}\n",
    "  end\n",
    "  def adjust w\n",
    "  end\n",
    "end\n",
    "\n",
    "def test_1_1_0\n",
    "  w = {\"x0\" => 0.0, \"z1\" => 0.0}\n",
    "  learning_rate = 0.25\n",
    "  obj = ParabolaObjective.new\n",
    "  sgd = StochasticGradientDescent.new obj, w, learning_rate\n",
    "  batch_of_examples = []\n",
    "  \n",
    "  lik_0 = obj.func(batch_of_examples, sgd.weights)\n",
    "  puts lik_0\n",
    "  \n",
    "  #Calculate objective for initial weights\n",
    "  assert_in_delta 0.0, sgd.weights[\"x0\"], 1e-2\n",
    "  assert_in_delta 2.5, lik_0, 1e-2\n",
    "\n",
    "  #Update weights\n",
    "  sgd.update(batch_of_examples)\n",
    "  puts sgd.weights\n",
    "  \n",
    "  #Now, objective function is different for the newly updated weights\n",
    "  lik_1 = obj.func(batch_of_examples, sgd.weights)\n",
    "  puts lik_1\n",
    "  \n",
    "  assert_in_delta 0.25, sgd.weights[\"x0\"], 1e-2\n",
    "  assert_in_delta 1.40625, lik_1, 1e-2\n",
    "end\n",
    "\n",
    "test_1_1_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf5df1afcd43e32d851b33cde700e028",
     "grade": true,
     "grade_id": "cell-653e55c495e9df88",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "# Testing on a known objective. Repeat SGD for 1000 iterations and expect to converge to correct solution.\n",
    "\n",
    "def test_1_1_1\n",
    "  w = {\"x0\" => 0.0, \"z1\" => 0.0}\n",
    "  learning_rate = 0.25\n",
    "  obj = ParabolaObjective.new\n",
    "  sgd = StochasticGradientDescent.new obj, w, learning_rate\n",
    "  batch_of_examples = []\n",
    "\n",
    "  lik = 1.0\n",
    "  1000.times do \n",
    "    sgd.update(batch_of_examples)\n",
    "    lik = obj.func(batch_of_examples, sgd.weights)\n",
    "  end\n",
    "\n",
    "  assert_true(lik < 0.1, \"SGD converges with simple objective\")\n",
    "  assert_in_delta 1.0, sgd.weights[\"x0\"], 0.1, \"Weight 0 expected to be 1.0\"\n",
    "  assert_in_delta 2.0, sgd.weights[\"z1\"], 0.1, \"Weight 1 expected to be 2.0\"\n",
    "end\n",
    "\n",
    "test_1_1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Question 2.1 (10 points)\n",
    "\n",
    "Implement linear regression as an objective function for use with stochastic gradient descent. First, we will implement the predict function. For a weight vector, $w$ and a single ```Row``` with features, $x$, implement:\n",
    "\n",
    "### $f(w,x) = w^T x$\n",
    "\n",
    "\n",
    "Note that in the formula above ```x``` is the ```features``` key in the example. Check out the ```fetch``` and ```has_key?``` methods in ruby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61f6b893684b904f112979ac19e65caa",
     "grade": false,
     "grade_id": "cell-513d6dcc72afdb50",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":predict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModel  \n",
    "  def predict example, weights\n",
    "    # BEGIN YOUR CODE\n",
    "    sum = 0.0 \n",
    "    i = 0\n",
    "    name  = example[\"features\"].keys\n",
    "    while i< weights.length do\n",
    "      n = weights.keys[i] \n",
    "      if name.include?n \n",
    "        if example[\"features\"][n].is_a?Numeric and weights[n].is_a?Numeric\n",
    "          sum += example[\"features\"][n]*weights[n]\n",
    "        else\n",
    "          sum += 0.0\n",
    "        end\n",
    "      end\n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    " \n",
    "    return sum\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0650e7815fcdd54ab27a7264b34a2162",
     "grade": true,
     "grade_id": "cell-b49b45d9c251f254",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":test_2_1_0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "# Tests that function can predict when all keys are found both vectors\n",
    "def test_2_1_0\n",
    "  lr = LinearRegressionModel.new\n",
    "  example = {\"features\" => {\"a\" => 2.0}}\n",
    "  weights = {\"a\" => 3.0}\n",
    "  \n",
    "  assert_in_delta 6.0, lr.predict(example, weights), 1e-6\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84967909ae7f11f28adb1ac16fa7b76c",
     "grade": true,
     "grade_id": "cell-1ee98fa7b333df87",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handles empty weights\n",
      "Handles weights not in features\n",
      "Handles features not in weights\n"
     ]
    }
   ],
   "source": [
    "### TEST ###\n",
    "# Tests that function can predict even when weights are missing\n",
    "def test_2_1_1\n",
    "  lr = LinearRegressionModel.new\n",
    "  \n",
    "  puts \"Handles empty weights\"\n",
    "  empty_example = {\"features\" => {}}\n",
    "  empty_weights = {}  \n",
    "  assert_equal 0.0, lr.predict(empty_example, empty_weights)\n",
    "  \n",
    "  puts \"Handles weights not in features\"\n",
    "  weights_without_a = {\"a\" => 0.0, \"b\" => 1.0}\n",
    "  example_a = {\"features\" => {\"a\" => 1.0}}\n",
    "  assert_equal 0.0, lr.predict(example_a, weights_without_a)\n",
    "  \n",
    "  puts \"Handles features not in weights\"\n",
    "  example_bc = {\"features\" => {\"b\" => 7.2, \"c\" => 2.5}}\n",
    "  assert_in_delta 7.2, lr.predict(example_bc, weights_without_a), 1e-2\n",
    "end\n",
    "\n",
    "test_2_1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Question 2.2 (10 points)\n",
    "\n",
    "Continuing the implementation, implement the $L_2$ loss, which applies to a mini-batch of $n$ points. Use the ```predict``` function you implemented earlier.\n",
    "\n",
    "### $L(w,X) = \\frac{1}{n} \\sum_{i} \\frac{1}{2} \\left(f(w,x_i) - y_i\\right) ^ 2$\n",
    "\n",
    "where $x_i$ is the ```features``` of the example, $y_i$ is the ```label``` of the example, and $n$ is the number of examples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "883edad6711fc33ec9e4ceac05c76389",
     "grade": false,
     "grade_id": "cell-ef24b617c1a0ecd5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModel\n",
    "  def func examples, w\n",
    "    # BEGIN YOUR CODE\n",
    "    sum  = 0.0\n",
    "    i = 0\n",
    "    obj = LinearRegressionModel.new\n",
    "    adjust w\n",
    "    while i< examples.length do \n",
    "      if examples[i][\"label\"].is_a?Numeric\n",
    "        sum += (obj.predict(examples[i], w) -  examples[i][\"label\"]) **2\n",
    "      else\n",
    "        sum += 0.0\n",
    "      end\n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    return sum/(2.0*examples.length)\n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  ## Adjusts the parameter to be within the allowable range\n",
    "  def adjust w\n",
    "    \n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d9bc2226a8a9ea58847310b6acdee50",
     "grade": true,
     "grade_id": "cell-a76cf363d0f1c40e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"features\"=>{\"bias\"=>1.0}, \"label\"=>1.0}]\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "### TEST ###\n",
    "# Use the coin dataset to measure the loss for a single-example batch.\n",
    "def test_2_2_0\n",
    "  #Use a fixed seed so that the coin dataset returns the same value each time\n",
    "  dataset = coin_dataset(100)\n",
    "  examples = dataset[\"data\"]\n",
    "  assert_equal 100, examples.size\n",
    "  \n",
    "  # Batch of size 1 from a dataset of 100 examples\n",
    "  batch = examples[0,1]\n",
    "  puts batch  \n",
    "  assert_equal 1, batch.size\n",
    "  \n",
    "  #Start with mu parameter 0.1, denoted by bias\n",
    "  model = LinearRegressionModel.new\n",
    "  weights_mu_0_1 = {\"bias\" => 0.5}\n",
    "  \n",
    "  #Calculate the squared loss\n",
    "  f = model.func batch, weights_mu_0_1\n",
    "  puts f\n",
    "  assert_in_delta 0.125, f, 0.050, \"Expected loss within [250, 350]\"\n",
    "end\n",
    "\n",
    "test_2_2_0()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f9a78db708686f41c340df5a58587d",
     "grade": true,
     "grade_id": "cell-930a1ec55478e6d9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "# Use the coin dataset\n",
    "t22_data = coin_dataset(1000)\n",
    "\n",
    "t22_model = LinearRegressionModel.new\n",
    "t22_w = Hash.new\n",
    "t22_w[\"bias\"] = 0.1\n",
    "\n",
    "t22_f = t22_model.func t22_data[\"data\"], t22_w\n",
    "assert_in_delta 0.300, t22_f, 0.050, \"Expected loss within [250, 350]\"\n",
    "\n",
    "t22_w[\"bias\"] = 0.77\n",
    "t22_f = t22_model.func t22_data[\"data\"], t22_w\n",
    "assert_in_delta 0.090, t22_f, 0.050, \"Expected loss for a closer guess to be within [40, 140]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Question 2.3 (10 points)\n",
    "\n",
    "Continuing the implementation of linear regression, now implement the gradient function. This returns a gradient vector for the mini-batch of $n$ points.\n",
    "\n",
    "Note that the gradient for a batch is the average gradient for all examples in the batch, which is why there is an initializer for the ```gradient``` hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cc5090310deec786a48442fad7de871",
     "grade": false,
     "grade_id": "cell-d96f7fe6f72bf82c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModel\n",
    "  def grad examples, weights\n",
    "    gradient = Hash.new {|h,k| h[k] = 0.0}\n",
    "    # BEGIN YOUR CODE\n",
    "\n",
    "    sum = 0.0\n",
    "    i=0\n",
    "    model = LinearRegressionModel.new\n",
    "    while i<examples.length do\n",
    "      sum += model.predict(examples[i], weights)  - examples[i][\"label\"]\n",
    "      i+=1\n",
    "    end\n",
    "    gradient.store(\"bias\", sum/examples.length)\n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return gradient\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bfacce111cc13ccf977dab124aa4c05",
     "grade": true,
     "grade_id": "cell-0284b1d493da8dc4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\n",
      "[{\"features\"=>{\"bias\"=>1.0}, \"label\"=>1.0}, {\"features\"=>{\"bias\"=>1.0}, \"label\"=>0.0}, {\"features\"=>{\"bias\"=>1.0}, \"label\"=>1.0}, {\"features\"=>{\"bias\"=>1.0}, \"label\"=>1.0}, {\"features\"=>{\"bias\"=>1.0}, \"label\"=>0.0}]\n",
      "Gradient:\n",
      "{\"bias\"=>-0.5}\n"
     ]
    }
   ],
   "source": [
    "### TEST ###\n",
    "# Calculate the gradient update on a batch of 5 examples\n",
    "\n",
    "def test_2_3_0 \n",
    "  #Use a fixed seed so the gradient value comes out the same\n",
    "  srand 1295187568912756074856\n",
    "  dataset = coin_dataset(1000)\n",
    "  model = LinearRegressionModel.new\n",
    "  \n",
    "  #Batch of size 5 of 1000\n",
    "  examples = dataset[\"data\"]\n",
    "  batch = examples[0,5]\n",
    "  puts \"Batch:\"  \n",
    "  puts batch\n",
    "  assert_equal 5, batch.size\n",
    "  \n",
    "  #Initialize weights with mu value represented by bias\n",
    "  weights = Hash.new\n",
    "  weights[\"bias\"] = 0.1\n",
    "\n",
    "  gradient = model.grad batch, weights\n",
    "  puts \"Gradient:\"\n",
    "  puts gradient\n",
    "  assert_in_delta -0.5, gradient[\"bias\"], 0.2, \"Expected loss within [-0.49, -0.89]\"\n",
    "end\n",
    "\n",
    "test_2_3_0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ced679c62665c25f724cd2b6401d44f",
     "grade": true,
     "grade_id": "cell-6581a7319efd449e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "\n",
    "t23_data = coin_dataset(1000)\n",
    "t23_model = LinearRegressionModel.new\n",
    "t23_w = Hash.new\n",
    "t23_w[\"bias\"] = 0.1\n",
    "\n",
    "t23_g = t23_model.grad t23_data[\"data\"], t23_w\n",
    "assert_in_delta -0.69, t23_g[\"bias\"], 0.2, \"Expected loss within [-0.49, -0.89]\"\n",
    "\n",
    "t23_w[\"bias\"] = 0.77\n",
    "t23_g = t23_model.grad t23_data[\"data\"], t23_w\n",
    "assert_in_delta 0.0, t23_g[\"bias\"], 0.1, \"Expected loss for a better guess to be within [-0.1, 0.1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 (10 points)\n",
    "\n",
    "Putting the previous steps together, use your ```StochasticGradientDecent``` to run linear regression for 10 passes (epochs) over the Coin Dataset, each pass with a mini-batch of size 20. Tune the learning rate, ```lr```, so that the model converges well. Assume that ```obj``` is an instance of ```LinearRegressionModel```, and ```w``` is an initial weight vector.\n",
    "\n",
    "Track the number of batches in the ```iters``` array and the loss in the ```losses``` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc9391a560e2495790954502116709ca",
     "grade": false,
     "grade_id": "cell-c5159a9b8d15fac9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":train_coin_sgd"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_coin_sgd(obj, w, dataset)\n",
    "  i = 0\n",
    "  iters = []\n",
    "  losses = []\n",
    "  \n",
    "  #Define sgd = StochasticGradientDescent.new obj, w, lr\n",
    "  # You set the learning rate, lr\n",
    "  # BEGIN YOUR CODE\n",
    "\n",
    "    lr = 3.7\n",
    "    sgd= StochasticGradientDescent.new( obj, w, lr)\n",
    "    10.times do \n",
    "      iters.append(i)\n",
    "      losses.append(obj.func(dataset[\"data\"][0,20],w))\n",
    "      sgd.update(dataset[\"data\"][0,20])\n",
    "      w = sgd.weights\n",
    "\n",
    "      i+=1  \n",
    "    end\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return [sgd, iters, losses]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2897553eb061eeeebdbfeec1bf56c681",
     "grade": true,
     "grade_id": "cell-ad954908a6c9a9cb",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id='vis-a1295f6c-c61b-44d6-a675-842adc3a0c3f'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"3781e45e-1357-4265-a2a6-d212e2fc45be\"}],\"options\":{\"x_label\":\"Batches\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[0,9],\"yrange\":[0.34500000000000014,3.983397966303145]}}],\"data\":{\"3781e45e-1357-4265-a2a6-d212e2fc45be\":[{\"x\":0,\"y\":0.34500000000000014},{\"x\":1,\"y\":1.2295312500000006},{\"x\":2,\"y\":2.626356335822861},{\"x\":3,\"y\":3.7143579919320424},{\"x\":4,\"y\":3.983397966303145},{\"x\":5,\"y\":3.6870066611895447},{\"x\":6,\"y\":3.249125079925365},{\"x\":7,\"y\":2.862029602383414},{\"x\":8,\"y\":2.5520445436902297},{\"x\":9,\"y\":2.3032608992804393}]},\"extension\":[]}\n",
       "        var id_name = '#vis-a1295f6c-c61b-44d6-a675-842adc3a0c3f';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x000055ec45041d98 @properties={:diagrams=>[#<Nyaplot::Diagram:0x000055ec450249a0 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"3781e45e-1357-4265-a2a6-d212e2fc45be\"}, @xrange=[0, 9], @yrange=[0.34500000000000014, 3.983397966303145]>], :options=>{:x_label=>\"Batches\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[0, 9], :yrange=>[0.34500000000000014, 3.983397966303145]}}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t24_data = coin_dataset(1000)\n",
    "t24_model = LinearRegressionModel.new\n",
    "t24_w = Hash.new\n",
    "t24_w[\"bias\"] = 0.1\n",
    "\n",
    "t24_trainer, t24_iter, t24_losses = train_coin_sgd t24_model, t24_w, t24_data\n",
    "\n",
    "assert_true t24_w.has_key?(\"bias\")\n",
    "assert_in_delta 0.77, t24_w[\"bias\"], 0.1, \"Expected weight for 'bias'  [0.67, 0.87]\"\n",
    "t24_cum_loss = 0.0\n",
    "t24_losses.each_index {|i| t24_cum_loss += t24_losses[i]; t24_losses[i] = t24_cum_loss / (t24_iter[i] + 1)}\n",
    "Daru::DataFrame.new({x: t24_iter, y: t24_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Batches\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2418aa0f48abdbe6ff014a29532ccb6",
     "grade": false,
     "grade_id": "cell-b41019cfe2cced3f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 3.1 (10 points)\n",
    "\n",
    "Implement Logistic Regression, following much the same process as with linear regression. The prediction function returns a value:\n",
    "\n",
    "### $f(x,w) = \\frac {1}{1 + \\exp \\left( -w^T x \\right) } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "778b87558628d1fdb62e6ddb4d0bd42d",
     "grade": false,
     "grade_id": "cell-33bf6c407e0d7157",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":adjust"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegressionModel\n",
    "  def predict row, w\n",
    "    # BEGIN YOUR CODE\n",
    "    adjust w\n",
    "    sum = 0.0 \n",
    "    i = 0\n",
    "    name  = row[\"features\"].keys\n",
    "\n",
    "    while i< w.length do\n",
    "      n = w.keys[i] \n",
    "      if name.include?n \n",
    "        if row[\"features\"][n].is_a?Numeric and w[n].is_a?Numeric\n",
    "          sum += row[\"features\"][n]*w[n]\n",
    "        else\n",
    "          sum += 0.0\n",
    "        end\n",
    "      end\n",
    "      i+=1\n",
    "    end\n",
    "   \n",
    "       \n",
    "    return 1.0/(1.0+Math.exp(-sum))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "  def adjust w\n",
    "    w\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4047451ff4933f13f0817b7009ea650",
     "grade": true,
     "grade_id": "cell-88560ef08ee29cdf",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t31_model = LogisticRegressionModel.new\n",
    "def t31_f(a: 0.0, b: 0.0)\n",
    "  row = {\"features\" => {\"a\" => a, \"b\" => b}}\n",
    "end\n",
    "def t31_w(a: 0.0, b: 0.0)\n",
    "  w = {\"a\" => a, \"b\" => b}\n",
    "end\n",
    "assert_in_delta 0.5, t31_model.predict(t31_f(), t31_w()), 1e-6\n",
    "assert_in_delta 0.2689, t31_model.predict(t31_f(a:1), t31_w(a:-1)), 1e-3\n",
    "assert_in_delta 1.0, t31_model.predict(t31_f(a:1, b:1000), t31_w(a:-1, b: 0.1)), 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 (10 points)\n",
    "\n",
    "Implement log loss assuming that the y label is defined as: $y \\in \\left\\{-1, 1\\right\\}$. Remember that the mini-batch loss is an expectation of the $n$ examples in the mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c5c25ba5a6a0dfae321b9ffd939236f",
     "grade": false,
     "grade_id": "cell-15878f1a66886147",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":func"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegressionModel\n",
    "  def func examples, w\n",
    "    # BEGIN YOUR CODE   \n",
    "    \n",
    "    sum  = 0.0\n",
    "    i = 0 \n",
    "    obj = LinearRegressionModel.new\n",
    "    adjust w\n",
    "\n",
    "    while i< examples.length do \n",
    "      if examples[i][\"label\"].is_a?Numeric\n",
    "\n",
    "        y = examples[i][\"label\"]\n",
    "      \n",
    "          #https://stats.stackexchange.com/questions/250937/which-loss-function-is-correct-for-logistic-regression#\n",
    "        \n",
    "\n",
    "           data = obj.predict(examples[i], w)\n",
    "          data = 1.0/(1.0+Math.exp(y*data))\n",
    "           sum += Math.log(1.0+Math.exp(-y*data))\n",
    "\n",
    "      else\n",
    "        sum += 0.0\n",
    "      end\n",
    "      i+=1\n",
    "    end\n",
    "    \n",
    "    return sum/examples.length\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db1f969ddf5fd5244aae153ae2863612",
     "grade": true,
     "grade_id": "cell-9dc5beeddbd5c30b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_data = coin_dataset(1000)\n",
    "t32_model = LogisticRegressionModel.new\n",
    "t32_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "assert_in_delta 0.66, t32_model.func(t32_data[\"data\"], t32_w), 0.2, \"Expected LR.func in [460, 860]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 (10 points)\n",
    "\n",
    "Calculate the gradient of the mini-batch log loss for each parameter $w$. This time, assume that $y \\in \\left\\{0, 1\\right\\}$. Hint: This assumption should simplify the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea56ec3af8b3217452cc693318d3edff",
     "grade": false,
     "grade_id": "cell-35515a097456be8f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":grad"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegressionModel\n",
    "  def grad examples, w\n",
    "    # BEGIN YOUR CODE\n",
    "    i = 0\n",
    "    sum  = 0.0\n",
    "    \n",
    "    model= LogisticRegressionModel.new\n",
    "    while i<examples.length do\n",
    "      weights = Hash.new\n",
    "      if w[\"bias\"].is_a?Numeric\n",
    "        weights.store(\"bias\",w[\"bias\"])\n",
    "      else\n",
    "      weights.store(\"bias\",w[i])\n",
    "      end\n",
    "      sigma = model.predict(examples[i],weights)\n",
    "      y = examples[i][\"label\"]\n",
    "      sum += (-y+sigma)*examples[i][\"features\"][\"bias\"]\n",
    "      i+=1\n",
    "    end\n",
    "    w[\"bias\"] =  sum/examples.length\n",
    "    \n",
    "    return w\n",
    "    \n",
    "    #END YOUR CODE\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc43d49ef3013a4d2a8b6943fc30f895",
     "grade": true,
     "grade_id": "cell-a546f94a5ee061c0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t32_data = coin_dataset(1000)\n",
    "t32_model = LogisticRegressionModel.new\n",
    "t32_w = Hash.new {|h,k| h[k] = 0.1}\n",
    "assert_in_delta 0.66, t32_model.func(t32_data[\"data\"], t32_w), 0.2, \"Expected LR.func in [460, 860]\"\n",
    "t32_g = t32_model.grad t32_data[\"data\"], t32_w\n",
    "assert_in_delta -0.26, t32_g[\"bias\"], 0.1, \"Expected LR.grad in [-0.36, -0.16]\"\n",
    "\n",
    "t32_w = Hash.new {|h,k| h[k] = 0.778}\n",
    "t32_g = t32_model.grad t32_data[\"data\"], t32_w\n",
    "assert_in_delta -0.1, t32_g[\"bias\"], 0.1, \"Expected LR.grad for a closer value to be in [-0.2, -0.2]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (6 points)\n",
    "\n",
    "Let's train our new models on a familiar dataset, spambase. Let's run gradient descent for a few steps on this dataset. Observe that the learned weights after just gradient 2 steps are very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"features\"=>{\"word_freq_our\"=>0.27, \"word_freq_mail\"=>0.83, \"word_freq_you\"=>0.27, \"word_freq_your\"=>0.27, \"word_freq_font\"=>8.58, \"char_freq_[\"=>0.092, \"char_freq_$\"=>0.185, \"char_freq_#\"=>0.232, \"capital_run_length_average\"=>7.313, \"capital_run_length_longest\"=>99.0, \"capital_run_length_total\"=>607.0, \"bias\"=>1.0}, \"label\"=>1.0}, {\"features\"=>{\"word_freq_your\"=>0.9, \"word_freq_george\"=>0.9, \"word_freq_data\"=>0.9, \"char_freq_[\"=>0.14, \"capital_run_length_average\"=>3.472, \"capital_run_length_longest\"=>28.0, \"capital_run_length_total\"=>125.0, \"bias\"=>1.0}, \"label\"=>0.0}]\n",
      "{\"bias\"=>0.8995944084089785}\n"
     ]
    }
   ],
   "source": [
    "### Example ###\n",
    "#Preview 2 lines from the Spambase dataset\n",
    "spambase = read_sparse_data_from_csv \"spambase\"\n",
    "spambase[\"data\"].each {|r| r[\"features\"][\"bias\"] = 1.0}\n",
    "puts spambase[\"data\"][0,2]\n",
    "\n",
    "q41_model = LinearRegressionModel.new\n",
    "q41_w = Hash.new {|h,k| h[k] = 0.0}\n",
    "q41_w[\"bias\"] = 1\n",
    "q41_sgd = StochasticGradientDescent.new q41_model, q41_w, 0.1\n",
    "2.times do\n",
    "  q41_batch = spambase[\"data\"].sample(10)\n",
    "  q41_sgd.update q41_batch\n",
    "end\n",
    "puts q41_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1 (Continued) \n",
    "We can correct this by _normalizing_ the data. A popular normalization is the z-score. For each feature, except bias, and considering only the non-zero values create a new zspambase dataset, ```zspambase```. The dataset ```zspambase``` is identical to spambase except that its features have been normalized as follows:\n",
    "\n",
    "### $x_z = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "where $\\mu$ is the mean of the $x$ value and $\\sigma$ is the standard deviation. Note that you have already seen an implementation of ```mean``` and ```stdev```, so find it and add it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be552b33440500fbf331278317542948",
     "grade": false,
     "grade_id": "cell-ad476d8f1e9bb351",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":normalization"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add mean and stdev here\n",
    "\n",
    "# BEGIN YOUR CODE\n",
    "def normalization \n",
    "def mean x\n",
    "  # BEGIN YOUR CODE\n",
    "  x.inject(0.0){|sum,i| sum + i }/x.length()  \n",
    "  #END YOUR CODE\n",
    "end\n",
    "\n",
    "\n",
    "def stdev x\n",
    "  # BEGIN YOUR CODE\n",
    "  mu = mean x\n",
    "  su = x.inject(0.0){|sum,i| sum + (i-mu)**2 }/(x.length()-1)\n",
    "  Math.sqrt(su) \n",
    "end  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spambase = read_sparse_data_from_csv \"spambase\"  \n",
    "dataset = spambase[\"data\"]  \n",
    "feature = spambase[\"features\"]\n",
    "data =   Array.new(feature.length) { Array.new(dataset.length){0}}\n",
    "\n",
    "i = 0\n",
    "while i<  dataset.length do\n",
    "  j = 0  \n",
    "  while j<feature.length do\n",
    "    data[j][i]=dataset[i][\"features\"][feature[j]]\n",
    "    j+=1\n",
    "  end\n",
    "  \n",
    "  i+=1\n",
    "end\n",
    "\n",
    "means = Hash.new\n",
    "stdev = Hash.new\n",
    "\n",
    "j = 0  \n",
    "while j<feature.length do\n",
    "    m = mean data[j].reject { |e| e.to_s.empty? }\n",
    "    s = stdev data[j].reject { |e| e.to_s.empty? }\n",
    "    means.store(feature[j],m)\n",
    "    stdev.store(feature[j],s)\n",
    "    j+=1\n",
    "end  \n",
    "  \n",
    "return [means,stdev]\n",
    "\n",
    "end\n",
    "  #END YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "#END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bafc508212cf426a2ec9755991c87569",
     "grade": false,
     "grade_id": "cell-481076f8bac87071",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"features\"=>{\"word_freq_our\"=>-0.628106690674003, \"word_freq_mail\"=>-0.0163998685249916, \"word_freq_you\"=>-1.2509960473524198, \"word_freq_your\"=>-0.9962817981732773, \"word_freq_font\"=>0.8660048920660688, \"char_freq_[\"=>-0.19095268670254528, \"char_freq_$\"=>-0.162631899108401, \"char_freq_#\"=>-0.038069985077358884, \"capital_run_length_average\"=>0.06686163123580875, \"capital_run_length_longest\"=>0.24027347134946417, \"capital_run_length_total\"=>0.533869650358239, \"bias\"=>1.0}, \"label\"=>1.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_zspambase spambase\n",
    "  zspambase = spambase.clone\n",
    "  zspambase[\"data\"] = spambase[\"data\"].collect do |r|\n",
    "    u = r.clone\n",
    "    u[\"features\"] = r[\"features\"].clone\n",
    "    u\n",
    "  end\n",
    " \n",
    "  # BEGIN YOUR CODE\n",
    "  i = 0\n",
    "\n",
    "  feature = zspambase[\"features\"]\n",
    "  means = normalization[0]\n",
    "  stdev = normalization[1]\n",
    "  while i<  zspambase[\"data\"].length do\n",
    "    \n",
    "    j = 0  \n",
    "    name = zspambase[\"data\"][i][\"features\"].keys    \n",
    "    while j<feature.length do\n",
    "      if name.include?feature[j]\n",
    "        \n",
    "       \n",
    "         zspambase[\"data\"][i][\"features\"][feature[j]] = (zspambase[\"data\"][i][\"features\"][feature[j]]-means[feature[j]])/stdev[feature[j]]\n",
    "          \n",
    "      end\n",
    "      j+=1\n",
    "    end\n",
    "\n",
    "    i+=1\n",
    "  end  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return zspambase\n",
    "end\n",
    "\n",
    "zspambase = create_zspambase spambase\n",
    "zspambase[\"data\"].first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4d4f5f9d7a599ca30d4aaa83198e29c",
     "grade": true,
     "grade_id": "cell-f9f979fc45ab2a30",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "t41_zs = create_zspambase spambase\n",
    "\n",
    "assert_in_delta 0.27, spambase[\"data\"].first[\"features\"][\"word_freq_our\"], 1e-5\n",
    "assert_in_delta -0.628106690674003, zspambase[\"data\"].first[\"features\"][\"word_freq_our\"], 1e-5\n",
    "\n",
    "assert_in_delta 607.0, spambase[\"data\"].first[\"features\"][\"capital_run_length_total\"], 1e-5\n",
    "assert_in_delta 0.53386, zspambase[\"data\"].first[\"features\"][\"capital_run_length_total\"], 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2 (7 points)\n",
    "\n",
    "Train Linear Regression for the ```zspambase``` dataset. Tune the learning rate as needed to train in one epoch. Hint: Learning rate may need to be very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c4ddd54c16eb6b7d485cf6c77497441",
     "grade": false,
     "grade_id": "cell-534aff92ed1aadf8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":train_zspambase_sgd"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_zspambase_sgd(obj, w, dataset)\n",
    "  i = 0\n",
    "  iters = []\n",
    "  losses = []\n",
    "  \n",
    "  #Define sgd = StochasticGradientDescent.new obj, w, lr\n",
    "  # You set the learning rate, lr\n",
    "  # BEGIN YOUR CODE\n",
    "  lr =  0.6\n",
    "  sgd =  StochasticGradientDescent.new(obj, w, lr)\n",
    "  features=dataset[\"features\"]\n",
    "  i = 0\n",
    "\n",
    "   46.times do\n",
    "        iters.append(i)\n",
    "        l = obj.func(dataset[\"data\"][(i+1)*100,(i+2)*100],w)\n",
    "        losses.append(l)\n",
    "        sgd.update(dataset[\"data\"][(i+1)*100,(i+2)*100])\n",
    "        w = sgd.weights\n",
    "\n",
    "\n",
    "        i+=1\n",
    "    end\n",
    "\n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return [sgd, iters, losses]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7881aca4101647e3c85ee5ccb52c67b6",
     "grade": true,
     "grade_id": "cell-1b5de61217794c41",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"bias\"=>0.4573688595721421}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-14c62085-eee6-42c5-bd7e-fc01f63d023b'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"c8f4867b-b2ab-4659-98f4-8a245d236a96\"}],\"options\":{\"x_label\":\"Batches\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[0,45],\"yrange\":[0.12436477258712883,0.275]}}],\"data\":{\"c8f4867b-b2ab-4659-98f4-8a245d236a96\":[{\"x\":0,\"y\":0.275},{\"x\":1,\"y\":0.2154416666666666},{\"x\":2,\"y\":0.18772877346202518},{\"x\":3,\"y\":0.1721104766052656},{\"x\":4,\"y\":0.16214696232584164},{\"x\":5,\"y\":0.15531741227679505},{\"x\":6,\"y\":0.1502368762445929},{\"x\":7,\"y\":0.14635262334409027},{\"x\":8,\"y\":0.14321803141328351},{\"x\":9,\"y\":0.14078355775239443},{\"x\":10,\"y\":0.1387576716025543},{\"x\":11,\"y\":0.1370229611200279},{\"x\":12,\"y\":0.13552160759896167},{\"x\":13,\"y\":0.13429274826408177},{\"x\":14,\"y\":0.13326103528695332},{\"x\":15,\"y\":0.13241054133797633},{\"x\":16,\"y\":0.13164706304757148},{\"x\":17,\"y\":0.1309889716196714},{\"x\":18,\"y\":0.13041177695498565},{\"x\":19,\"y\":0.1298668363741194},{\"x\":20,\"y\":0.12934058473668866},{\"x\":21,\"y\":0.1288854022697108},{\"x\":22,\"y\":0.12848107175148907},{\"x\":23,\"y\":0.12811360612603787},{\"x\":24,\"y\":0.1277788535038575},{\"x\":25,\"y\":0.12748346537856442},{\"x\":26,\"y\":0.12720180794266345},{\"x\":27,\"y\":0.1269337602908274},{\"x\":28,\"y\":0.1266835598928988},{\"x\":29,\"y\":0.12644501758705237},{\"x\":30,\"y\":0.12622306682542342},{\"x\":31,\"y\":0.1260116757607896},{\"x\":32,\"y\":0.1258118097579489},{\"x\":33,\"y\":0.1256196842267045},{\"x\":34,\"y\":0.1254366188421702},{\"x\":35,\"y\":0.12526730278901574},{\"x\":36,\"y\":0.1250956604940455},{\"x\":37,\"y\":0.12492936143364342},{\"x\":38,\"y\":0.12476305723603325},{\"x\":39,\"y\":0.12461165060026283},{\"x\":40,\"y\":0.1245030068439858},{\"x\":41,\"y\":0.12443815415696931},{\"x\":42,\"y\":0.12442083270307555},{\"x\":43,\"y\":0.12438428279452997},{\"x\":44,\"y\":0.12436477258712883},{\"x\":45,\"y\":0.12551309113728354}]},\"extension\":[]}\n",
       "        var id_name = '#vis-14c62085-eee6-42c5-bd7e-fc01f63d023b';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x000055ec46dfe610 @properties={:diagrams=>[#<Nyaplot::Diagram:0x000055ec46deb7e0 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"c8f4867b-b2ab-4659-98f4-8a245d236a96\"}, @xrange=[0, 45], @yrange=[0.12436477258712883, 0.275]>], :options=>{:x_label=>\"Batches\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[0, 45], :yrange=>[0.12436477258712883, 0.275]}}>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t25_model = LinearRegressionModel.new\n",
    "t25_w = Hash.new {|h,k| h[k] = 0.0}\n",
    "t25_w[\"bias\"] = 1\n",
    "\n",
    "t25_trainer, t25_iter, t25_losses = train_zspambase_sgd t25_model, t25_w, zspambase\n",
    "puts t25_w\n",
    "\n",
    "t25_cum_loss = 0.0\n",
    "t25_losses.each_index {|i| t25_cum_loss += t25_losses[i]; t25_losses[i] = t25_cum_loss / (t25_iter[i] + 1)}\n",
    "assert_true (t25_losses.last < 0.15), \"Expected last loss value less than target\"\n",
    "Daru::DataFrame.new({x: t25_iter, y: t25_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Batches\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (7 points)\n",
    "\n",
    "Run logistic regression on the ```zspambase``` dataset, tuning the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41ad8150351136ed6e4631337bb20d57",
     "grade": false,
     "grade_id": "cell-7fd32c360f47ff13",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":train_zspambase_logistic_sgd"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_zspambase_logistic_sgd(obj, w, dataset)\n",
    "  i = 0\n",
    "  iters = []\n",
    "  losses = []\n",
    "  \n",
    "  #Define sgd = StochasticGradientDescent.new obj, w, lr\n",
    "  # You set the learning rate, lr\n",
    "  # BEGIN YOUR CODE\n",
    "  lr = 10\n",
    "  sgd =  StochasticGradientDescent.new(obj, w, lr)\n",
    "  features=dataset[\"features\"]\n",
    "  i = 0\n",
    "\n",
    "   20.times do\n",
    "        iters.append(i)\n",
    "        l = obj.func(dataset[\"data\"][(i+1)*230,(i+2)*230],w)\n",
    "        losses.append(l)\n",
    "        sgd.update(dataset[\"data\"][(i+1)*230,(i+2)*230])\n",
    "        w = sgd.weights\n",
    "\n",
    "\n",
    "        i+=1\n",
    "    end\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  #END YOUR CODE\n",
    "  return [sgd, iters, losses]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0efa9ebe30c126d1b59a9bbc3341688",
     "grade": true,
     "grade_id": "cell-a2ee6be6489349a5",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"bias\"=>0.6380960833510414}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id='vis-42f9bdfd-2ee0-4d91-bedf-8da25238050a'></div>\n",
       "<script>\n",
       "(function(){\n",
       "    var render = function(){\n",
       "        var model = {\"panes\":[{\"diagrams\":[{\"type\":\"line\",\"options\":{\"x\":\"x\",\"y\":\"y\"},\"data\":\"973256f5-715d-45d8-ad08-f41b762d0cec\"}],\"options\":{\"x_label\":\"Batches\",\"y_label\":\"Cumulative Loss\",\"zoom\":true,\"width\":700,\"xrange\":[0,19],\"yrange\":[0.5958822791297451,0.6424190602284295]}}],\"data\":{\"973256f5-715d-45d8-ad08-f41b762d0cec\":[{\"x\":0,\"y\":0.6424190602284295},{\"x\":1,\"y\":0.5958822791297451},{\"x\":2,\"y\":0.6214755897208114},{\"x\":3,\"y\":0.6056618487218527},{\"x\":4,\"y\":0.6146692697845734},{\"x\":5,\"y\":0.6072899976017468},{\"x\":6,\"y\":0.6100917764622078},{\"x\":7,\"y\":0.6067126496396091},{\"x\":8,\"y\":0.6073628585444437},{\"x\":9,\"y\":0.606059365053091},{\"x\":10,\"y\":0.6059562170218126},{\"x\":11,\"y\":0.6053837583485853},{\"x\":12,\"y\":0.6051634558329745},{\"x\":13,\"y\":0.6049337245801013},{\"x\":14,\"y\":0.60477020775986},{\"x\":15,\"y\":0.6046511557019063},{\"x\":16,\"y\":0.6046097261239847},{\"x\":17,\"y\":0.6043149337230047},{\"x\":18,\"y\":0.6038691527832514},{\"x\":19,\"y\":0.5970747048574316}]},\"extension\":[]}\n",
       "        var id_name = '#vis-42f9bdfd-2ee0-4d91-bedf-8da25238050a';\n",
       "        Nyaplot.core.parse(model, id_name);\n",
       "\n",
       "        require(['downloadable'], function(downloadable){\n",
       "          var svg = d3.select(id_name).select(\"svg\");\n",
       "\t  if(!svg.empty())\n",
       "\t    svg.call(downloadable().filename('fig'));\n",
       "\t});\n",
       "    };\n",
       "    if(window['Nyaplot']==undefined){\n",
       "        window.addEventListener('load_nyaplot', render, false);\n",
       "\treturn;\n",
       "    } else {\n",
       "       render();\n",
       "    }\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "#<Nyaplot::Plot:0x000055ec46de84c8 @properties={:diagrams=>[#<Nyaplot::Diagram:0x000055ec46de0020 @properties={:type=>:line, :options=>{:x=>:x, :y=>:y}, :data=>\"973256f5-715d-45d8-ad08-f41b762d0cec\"}, @xrange=[0, 19], @yrange=[0.5958822791297451, 0.6424190602284295]>], :options=>{:x_label=>\"Batches\", :y_label=>\"Cumulative Loss\", :zoom=>true, :width=>700, :xrange=>[0, 19], :yrange=>[0.5958822791297451, 0.6424190602284295]}}>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST ###\n",
    "t43_model = LogisticRegressionModel.new\n",
    "t43_w = Hash.new {|h,k| h[k] = 0.0}\n",
    "t43_w[\"bias\"] = 1\n",
    "\n",
    "t43_trainer, t43_iter, t43_losses = train_zspambase_logistic_sgd t43_model, t43_w, zspambase\n",
    "t43_cum_loss = 0.0\n",
    "t43_losses.each_index {|i| t43_cum_loss += t43_losses[i]; t43_losses[i] = t43_cum_loss / (t43_iter[i] + 1)}\n",
    "puts t43_w\n",
    "\n",
    "assert_true(t43_losses.last < 0.6, \"Expected last loss value < 0.6\")\n",
    "Daru::DataFrame.new({x: t43_iter, y: t43_losses}).plot(type: :line, x: :x, y: :y) do |plot, diagram|\n",
    "  plot.x_label \"Batches\"\n",
    "  plot.y_label \"Cumulative Loss\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.7.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
